{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ensure that the output results of extensive output results are not truncated.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Change the width of the Notebook to see the output on the screen'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Change the width of the Notebook to see the output on the screen'''\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Register the GitHub link or the file relative location'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Register the GitHub link or the file relative location'''\n",
    "#the Github link\n",
    "#repository_loc = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main\"\n",
    "#the local link\n",
    "repository_loc = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get the other folder locations'''\n",
    "belgian_GTFS_loc = repository_loc + '/gtfs_train_Belgium_1503/'\n",
    "dutch_GTFS_loc = repository_loc + '/gtfs_train_Netherlands_1503/'\n",
    "swiss_GTFS_loc = repository_loc + '/gtfs_train_Switzerland_1503/'\n",
    "\n",
    "stops_series_loc = repository_loc + '/country_stops_series/'\n",
    "stops_cleaned_loc = repository_loc + '/stops_cleaned/'\n",
    "df_for_edges_loc = repository_loc + '/df_for_edges/'\n",
    "routes_loc = repository_loc + '/routes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import all the DataFrames that are common for the three train networks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import all the DataFrames that are common for the three train networks'''\n",
    "\n",
    "def common_imports(datalink):\n",
    "    #To import the agency dataset that contains limited information about the railway agency.\n",
    "    agency = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "    #To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "    calendar_dates = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n",
    "    #To import the routes dataset that provides the id, the name and the type of vehicle used for all railway routes.\n",
    "    routes = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "    #To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the railway stations.\n",
    "    stops = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "    #To import the transfers dataset that gives the minimum transfer time to switch routes at each railway station.\n",
    "    transfers = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "    #To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the railway route.\n",
    "    #The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "    trips = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "    return agency, calendar_dates, routes, stops, transfers, trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Belgium, calendar_dates_Belgium, routes_Belgium, stops_Belgium, transfers_Belgium, trips_Belgium = common_imports(belgian_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the translations dataset that provides the French-, Dutch-, German- and English-language translations of the Belgian railway stations.\n",
    "translations_Belgium = pd.read_csv(belgian_GTFS_loc + \"translations.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Belgium = pd.read_csv(belgian_GTFS_loc + \"calendar.txt\", sep=\",\")\n",
    "#To import the stop_time_overrides dataset \n",
    "stop_time_overrides_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_time_overrides.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Netherlands, calendar_dates_Netherlands, routes_Netherlands, stops_Netherlands, transfers_not_cleaned_Netherlands, trips_Netherlands = common_imports(dutch_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Dutch NS railway feed.\n",
    "feed_info_Netherlands = pd.read_csv(dutch_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_range = [*range(2, 19)]\n",
    "stop_times_Netherlands = pd.read_csv(dutch_GTFS_loc + \"stop_times-1.csv\", sep=\",\")\n",
    "for index in stop_times_range:\n",
    "    stop_times_Netherlands = pd.concat([stop_times_Netherlands, pd.read_csv(dutch_GTFS_loc + \"stop_times-\" + str(index)+ \".csv\", sep=\",\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Switzerland, calendar_dates_Switzerland, routes_Switzerland, stops_Switzerland, transfers_not_cleaned_Switzerland, trips_Switzerland = common_imports(swiss_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Swiss SBB railway feed.\n",
    "feed_info_Switzerland = pd.read_csv(swiss_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Switzerland = pd.read_csv(swiss_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Switzerland = pd.read_csv(swiss_GTFS_loc + \"calendar.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the railway data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the calendar_dates DataFrame'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the calendar_dates DataFrame'''\n",
    "\n",
    "def clean_calendar_dates(calendar_dates):\n",
    "    #To filter the dates from the selected begin to the end date\n",
    "    begin_date = 20210314\n",
    "    end_date = 20210713\n",
    "    calendar_dates_cleaned = calendar_dates.copy()\n",
    "    calendar_dates_cleaned = calendar_dates_cleaned.drop(calendar_dates_cleaned[(calendar_dates_cleaned['date'] > end_date) | (calendar_dates_cleaned['date'] < begin_date)].index)\n",
    "    return calendar_dates_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'''\n",
    "\n",
    "def country_information(stops, country_name, stops_cleaned_loc, stops_series_loc):\n",
    "    #To initialize the Nominatim API to get the location from the input string \n",
    "    geolocator = Nominatim(user_agent=\"application\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "    #To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "    country_list = []\n",
    "    for index, row in stops.iterrows():\n",
    "        latitude = row['stop_lat']\n",
    "        longitude = row['stop_lon']\n",
    "        # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "        location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "        # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "        address = location.raw['address']\n",
    "        country = address.get('country', '')\n",
    "        country_list.append(country)\n",
    "\n",
    "    #To add the values of country_list as a new attribute country \n",
    "    stops.loc[:,'country'] = country_list\n",
    "\n",
    "    #To calculate the total number of Belgian stations in the stops dataset\n",
    "    country_stops = stops[stops['country'] == country_name]\n",
    "    country_stops_series = stops.loc[stops['country'] == country_name, 'stop_name']\n",
    "    \n",
    "    stops.to_csv(f'{stops_cleaned_loc}stops_cleaned_{country_name}.csv')\n",
    "    country_stops_series.to_csv(f'{stops_series_loc}stops_{country_name}_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remove the accents from a string'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Remove the accents from a string'''\n",
    "\n",
    "def remove_accents(text):\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Belgium df'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Belgium df'''\n",
    "allowed_route_type = {'IC', 'L', 'P', 'ICT', 'IZY'}\n",
    "routes_cleaned_Belgium = routes_Belgium[(routes_Belgium['route_short_name'].isin(allowed_route_type)) | (routes_Belgium['route_short_name'].str.startswith('S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Belgium = clean_calendar_dates(calendar_dates_Belgium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops_Belgium df.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops_Belgium df.''' \n",
    "#To eliminate the stop_ids in the stops dataset that contain an underscore or that start with a character 'S'. \n",
    "stops_cleaned_Belgium = stops_Belgium[(~stops_Belgium['stop_id'].str.contains('_')) & (~stops_Belgium['stop_id'].str.contains('S'))]\n",
    "\n",
    "#To modify the object datatype of the stop_id column to the NumPy int64 datatype\n",
    "stops_cleaned_Belgium.loc[:,'stop_id'] = stops_cleaned_Belgium.loc[:,'stop_id'].astype(np.int64)\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Belgium'\n",
    "#country_information(stops_cleaned_Belgium, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Belgium = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Belgium_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Netherlands DataFrame'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>agency_id</th>\n",
       "      <th>route_short_name</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>route_desc</th>\n",
       "      <th>route_type</th>\n",
       "      <th>route_color</th>\n",
       "      <th>route_text_color</th>\n",
       "      <th>route_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>145</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>17803</td>\n",
       "      <td>IFF:NS</td>\n",
       "      <td>Sprinter</td>\n",
       "      <td>Nachtnettrein Utrecht Centraal &lt;-&gt; Rotterdam C...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_id agency_id route_short_name  \\\n",
       "count       145       145              145   \n",
       "unique      145        11               15   \n",
       "top       17803    IFF:NS         Sprinter   \n",
       "freq          1        87               47   \n",
       "\n",
       "                                          route_long_name route_desc  \\\n",
       "count                                                 145        145   \n",
       "unique                                                144          1   \n",
       "top     Nachtnettrein Utrecht Centraal <-> Rotterdam C...        nan   \n",
       "freq                                                    2        145   \n",
       "\n",
       "       route_type route_color route_text_color route_url  \n",
       "count         145         145              145       145  \n",
       "unique          1           1                1         1  \n",
       "top             2         nan              nan       nan  \n",
       "freq          145         145              145       145  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Netherlands DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Netherlands = routes_Netherlands[routes_Netherlands['route_type'] == 2]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands.astype(str)\n",
    "routes_cleaned_Netherlands.describe(include=['object'])\n",
    "\n",
    "#To change the route_id object datatype to a NumPy int64 datatype\n",
    "routes_cleaned_Netherlands.loc[:,'route_id'] = routes_cleaned_Netherlands.loc[:,'route_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Netherlands = clean_calendar_dates(calendar_dates_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops DataFrame'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops DataFrame'''\n",
    "#To take from the stops_initial_Netherlands df all stop_ids that contain a 'stoparea:' to get the correct stop coordinates\n",
    "stops_cleaned_Netherlands = stops_Netherlands[stops_Netherlands['stop_id'].str.contains('stoparea:')]\n",
    "\n",
    "#To remove the accents from the accented characters and to convert the remaining characters to uppercase characters\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Netherlands'\n",
    "#country_information(stops_cleaned_Netherlands, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Netherlands = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Netherlands_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stop_times df'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-0c4af4e053df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstops_Netherlands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_accents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_times_cleaned_Netherlands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-77cb8bec6883>\u001b[0m in \u001b[0;36mremove_accents\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Clean the stop_times df'''\n",
    "stop_times_cleaned_Netherlands = stop_times_Netherlands.copy()\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_id'] = stop_times_cleaned_Netherlands.stop_id.apply(str)\n",
    "stop_times_cleaned_Netherlands = pd.merge(stop_times_cleaned_Netherlands, stops_Netherlands[['stop_id', 'stop_name']], on='stop_id')\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean the routes_Switzerland DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Switzerland = routes_Switzerland[routes_Switzerland['route_type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Switzerland = clean_calendar_dates(calendar_dates_Switzerland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean the stop_times_Switzerland DataFrame'''\n",
    "# To remove the superfluous characters of the stop_id (platform codes)\n",
    "stop_times_cleaned_Switzerland = stop_times_Switzerland.copy()\n",
    "stop_times_cleaned_Switzerland_column = stop_times_cleaned_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland_column\n",
    "\n",
    "# To make the stop_ids numerical \n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean the stops_Switzerland DataFrame'''\n",
    "#To remove the superfluous characters (platform codes)\n",
    "stops_cleaned_Switzerland_column = stops_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stops_cleaned_Switzerland = stops_Switzerland.copy()\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland_column\n",
    "\n",
    "#To make the stop_ids numerical and to remove the duplicate stop_ids\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland.drop_duplicates()\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Switzerland'\n",
    "#country_information(stops_cleaned_Switzerland, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Switzerland = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Switzerland_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merge the DataFrames'''\n",
    "\n",
    "def merge_df(stop_times, stops, routes, trips, calendar_dates, on_stop):\n",
    "    list_columns = ['stop_name', 'stop_lat', 'stop_lon', 'country']\n",
    "    if on_stop == 'stop_id':\n",
    "        list_columns.append('stop_id')\n",
    "    #To merge the stop_times df with the stops df on stop_id\n",
    "    stop_times_stops = pd.merge(stop_times, stops[list_columns], on= on_stop)\n",
    "\n",
    "    #To merge the trips df with the routes df on route_id\n",
    "    routes_trips = pd.merge(routes[['route_id']], trips, on='route_id')\n",
    "\n",
    "    #To merge the stop_times_stops df with the trips_routes df on trip_id\n",
    "    uncleaned_railway_system_information = pd.merge(routes_trips, stop_times_stops, on='trip_id')\n",
    "\n",
    "    #To take only the service_ids present in both the routes_trips_stop_times_stops df and the calendar_dates df into account\n",
    "    calendar_dates_unique = calendar_dates['service_id'].unique()\n",
    "    railway_system_information = uncleaned_railway_system_information[(uncleaned_railway_system_information['service_id'].isin(calendar_dates_unique))]\n",
    "    \n",
    "    return railway_system_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Belgium = agency_Belgium[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Belgium = routes_cleaned_Belgium[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Belgium = trips_Belgium[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Belgium = calendar_dates_cleaned_Belgium[['service_id', 'date']]\n",
    "stops_cleaned_Belgium = stops_cleaned_Belgium[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Belgium = stop_times_Belgium[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Belgium = merge_df(stop_times_cleaned_Belgium, stops_cleaned_Belgium, routes_cleaned_Belgium, trips_cleaned_Belgium, calendar_dates_cleaned_Belgium, 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Netherlands = agency_Netherlands[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Netherlands = trips_Netherlands[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Netherlands = calendar_dates_cleaned_Netherlands[['service_id', 'date']]\n",
    "stops_cleaned_Netherlands = stops_cleaned_Netherlands[['stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Netherlands = stop_times_cleaned_Netherlands[['trip_id', 'stop_name', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Netherlands = merge_df(stop_times_cleaned_Netherlands, stops_cleaned_Netherlands, routes_cleaned_Netherlands, trips_cleaned_Netherlands, calendar_dates_cleaned_Netherlands, 'stop_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Switzerland = agency_Switzerland[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Switzerland = routes_cleaned_Switzerland[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Switzerland = trips_Switzerland[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Switzerland = calendar_dates_cleaned_Switzerland[['service_id', 'date']]\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Switzerland = stop_times_cleaned_Switzerland[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Switzerland = merge_df(stop_times_cleaned_Switzerland, stops_cleaned_Switzerland, routes_cleaned_Switzerland, trips_cleaned_Switzerland, calendar_dates_cleaned_Switzerland, 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the space-of-stops representation of the railway systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>service_id</th>\n",
       "      <th>trip_headsign</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885704:8885001:4:523:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>Tournai</td>\n",
       "      <td>8885001</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>4</td>\n",
       "      <td>TOURNAI</td>\n",
       "      <td>50.61313</td>\n",
       "      <td>3.396940</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885704:8885001:4:523:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>Tournai</td>\n",
       "      <td>8885068</td>\n",
       "      <td>05:19:00</td>\n",
       "      <td>05:19:00</td>\n",
       "      <td>3</td>\n",
       "      <td>FROYENNES</td>\n",
       "      <td>50.62989</td>\n",
       "      <td>3.354835</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885704:8885001:4:523:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>Tournai</td>\n",
       "      <td>8885753</td>\n",
       "      <td>05:12:00</td>\n",
       "      <td>05:12:00</td>\n",
       "      <td>2</td>\n",
       "      <td>HERSEAUX</td>\n",
       "      <td>50.71390</td>\n",
       "      <td>3.245961</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885704:8885001:4:523:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>Tournai</td>\n",
       "      <td>8885704</td>\n",
       "      <td>05:07:00</td>\n",
       "      <td>05:07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>MOUSCRON</td>\n",
       "      <td>50.74100</td>\n",
       "      <td>3.228449</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885704:8885001:4:623:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>Tournai</td>\n",
       "      <td>8885001</td>\n",
       "      <td>06:23:00</td>\n",
       "      <td>06:23:00</td>\n",
       "      <td>4</td>\n",
       "      <td>TOURNAI</td>\n",
       "      <td>50.61313</td>\n",
       "      <td>3.396940</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431487</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>Bruxelles-Nord</td>\n",
       "      <td>8811270</td>\n",
       "      <td>17:01:00</td>\n",
       "      <td>17:01:00</td>\n",
       "      <td>19</td>\n",
       "      <td>VELTEM</td>\n",
       "      <td>50.90052</td>\n",
       "      <td>4.633520</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431488</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>Bruxelles-Nord</td>\n",
       "      <td>8811288</td>\n",
       "      <td>16:59:00</td>\n",
       "      <td>16:59:00</td>\n",
       "      <td>18</td>\n",
       "      <td>HERENT</td>\n",
       "      <td>50.90353</td>\n",
       "      <td>4.672190</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431489</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>Bruxelles-Nord</td>\n",
       "      <td>8819406</td>\n",
       "      <td>17:10:00</td>\n",
       "      <td>17:12:00</td>\n",
       "      <td>23</td>\n",
       "      <td>BRUSSELS AIRPORT-ZAVENTEM</td>\n",
       "      <td>50.89646</td>\n",
       "      <td>4.482072</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431490</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>Bruxelles-Nord</td>\n",
       "      <td>8821063</td>\n",
       "      <td>16:11:00</td>\n",
       "      <td>16:11:00</td>\n",
       "      <td>5</td>\n",
       "      <td>ANVERS-LUCHTBAL</td>\n",
       "      <td>51.24413</td>\n",
       "      <td>4.425033</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431491</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>Bruxelles-Nord</td>\n",
       "      <td>8821105</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>16:01:00</td>\n",
       "      <td>4</td>\n",
       "      <td>NOORDERKEMPEN (BRECHT)</td>\n",
       "      <td>51.35683</td>\n",
       "      <td>4.632200</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417593 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        route_id                                       trip_id  service_id  \\\n",
       "0            115    88____:007::8885704:8885001:4:523:20210418          14   \n",
       "1            115    88____:007::8885704:8885001:4:523:20210418          14   \n",
       "2            115    88____:007::8885704:8885001:4:523:20210418          14   \n",
       "3            115    88____:007::8885704:8885001:4:523:20210418          14   \n",
       "4            115    88____:007::8885704:8885001:4:623:20210418          14   \n",
       "...          ...                                           ...         ...   \n",
       "431487       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "431488       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "431489       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "431490       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "431491       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "\n",
       "         trip_headsign  stop_id arrival_time departure_time  stop_sequence  \\\n",
       "0              Tournai  8885001     05:23:00       05:23:00              4   \n",
       "1              Tournai  8885068     05:19:00       05:19:00              3   \n",
       "2              Tournai  8885753     05:12:00       05:12:00              2   \n",
       "3              Tournai  8885704     05:07:00       05:07:00              1   \n",
       "4              Tournai  8885001     06:23:00       06:23:00              4   \n",
       "...                ...      ...          ...            ...            ...   \n",
       "431487  Bruxelles-Nord  8811270     17:01:00       17:01:00             19   \n",
       "431488  Bruxelles-Nord  8811288     16:59:00       16:59:00             18   \n",
       "431489  Bruxelles-Nord  8819406     17:10:00       17:12:00             23   \n",
       "431490  Bruxelles-Nord  8821063     16:11:00       16:11:00              5   \n",
       "431491  Bruxelles-Nord  8821105     16:00:00       16:01:00              4   \n",
       "\n",
       "                        stop_name  stop_lat  stop_lon  country  \n",
       "0                         TOURNAI  50.61313  3.396940  Belgium  \n",
       "1                       FROYENNES  50.62989  3.354835  Belgium  \n",
       "2                        HERSEAUX  50.71390  3.245961  Belgium  \n",
       "3                        MOUSCRON  50.74100  3.228449  Belgium  \n",
       "4                         TOURNAI  50.61313  3.396940  Belgium  \n",
       "...                           ...       ...       ...      ...  \n",
       "431487                     VELTEM  50.90052  4.633520  Belgium  \n",
       "431488                     HERENT  50.90353  4.672190  Belgium  \n",
       "431489  BRUSSELS AIRPORT-ZAVENTEM  50.89646  4.482072  Belgium  \n",
       "431490            ANVERS-LUCHTBAL  51.24413  4.425033  Belgium  \n",
       "431491     NOORDERKEMPEN (BRECHT)  51.35683  4.632200  Belgium  \n",
       "\n",
       "[417593 rows x 12 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "railway_system_information_Belgium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the space-of-stops representation of the railway systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create a DataFrame with the departure time form the first stop sequence and with the one from last stop sequence for each trip_id'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create a DataFrame with the departure time form the first stop sequence and with the one from last stop sequence for each trip_id'''\n",
    "\n",
    "def create_trip_departure_times(railway_system_information):\n",
    "    departure_time_first = railway_system_information.reset_index().loc[railway_system_information.reset_index().groupby(['trip_id'])['stop_sequence'].idxmin()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "    departure_time_first = departure_time_first.rename(columns = {'departure_time': 'departure_time_first'})\n",
    "    departure_time_last = railway_system_information.reset_index().loc[railway_system_information.reset_index().groupby(['trip_id'])['stop_sequence'].idxmax()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "    departure_time_last = departure_time_last.rename(columns = {'departure_time': 'departure_time_last'})\n",
    "    trip_departure_times = departure_time_first.merge(departure_time_last[['trip_id', 'departure_time_last']], on='trip_id')\n",
    "    return trip_departure_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame and\\nCalculate the hash of the stop sequence in both order (ascending and descending)'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame and\n",
    "Calculate the hash of the stop sequence in both order (ascending and descending)'''\n",
    "\n",
    "def create_trip_stop_sequence(trip_departure_times):    \n",
    "    #Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame\n",
    "    trip_stop_sequence = trip_departure_times.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    trip_stop_sequence.rename(columns={'stop_name':'stops_sequence'}, inplace=True)\n",
    "    #Calculate the hash of the stop sequence in both order (ascending and descending)\n",
    "    trip_stop_sequence['hash'] = trip_stop_sequence['stops_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "    trip_stop_sequence['hash_inverse'] = trip_stop_sequence['stops_sequence'].apply(lambda x: hash(tuple(x[::-1])))\n",
    "    return trip_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regroup the days per service id in a set and count them'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Regroup the days per service id in a set and count them'''\n",
    "\n",
    "def create_service_id_dates(calendar_dates):\n",
    "    service_id_dates = calendar_dates.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "    service_id_dates.rename(columns={'date':'dates'}, inplace=True)\n",
    "    service_id_dates['count_service_id'] = service_id_dates['dates'].apply(lambda x: len(x))\n",
    "    return service_id_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Put the different trip_ids in a list and add the departure_time first and last lists'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Put the different trip_ids in a list and add the departure_time first and last lists'''\n",
    "\n",
    "def create_routes_hash(trips_hash):\n",
    "    common_columns = ['route_id','hash', 'hash_inverse', 'service_id']\n",
    "    routes_hash = trips_hash.groupby(common_columns)['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    route_hash_dep_first = trips_hash.groupby(common_columns)['departure_time_first'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    route_hash_dep_last = trips_hash.groupby(common_columns)['departure_time_last'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    routes_hash = routes_hash.merge(route_hash_dep_first, on= common_columns)\n",
    "    routes_hash = routes_hash.merge(route_hash_dep_last, on= common_columns)\n",
    "    return routes_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create DataFrames that will be used for the route_creation process'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create DataFrames that will be used for the route_creation process'''\n",
    "\n",
    "def prepartion_space(railway_system_information, calendar_dates):    \n",
    "    #Sort values by the route_id, the trip_id, and the stop_sequence fields\n",
    "    railway_system_information = railway_system_information.sort_values(by=['route_id', 'trip_id','service_id', 'stop_sequence'])\n",
    "\n",
    "    trip_departure_times = create_trip_departure_times(railway_system_information)\n",
    "\n",
    "    #Merge railway_system_information with trip_departure_times\n",
    "    trip_departure_times = railway_system_information.merge(trip_departure_times[['trip_id','departure_time_first','departure_time_last']], on='trip_id')\n",
    "\n",
    "    trip_stop_sequence = create_trip_stop_sequence(trip_departure_times)\n",
    "    \n",
    "    #Add the stop_sequence of stations to the trip_departure_times dataset by joining on trip_id\n",
    "    trips_hash = pd.merge(trip_departure_times, trip_stop_sequence, on='trip_id')\n",
    "    \n",
    "    service_id_dates = create_service_id_dates(calendar_dates)\n",
    "    \n",
    "    #Merge trips_hash with service_id_dates\n",
    "    trips_hash = pd.merge(trips_hash, service_id_dates, on='service_id', how='left')\n",
    "    \n",
    "    #Calculate generic_trips_information\n",
    "    generic_trips_information = trips_hash.groupby(['route_id', 'trip_id', 'service_id', 'hash', 'hash_inverse'], as_index=False)['stops_sequence'].first()\n",
    "    \n",
    "    routes_hash = create_routes_hash(trips_hash)\n",
    "    \n",
    "    #Add the sequence of stops, dates and service_id_count to the route_hash_freq_dep dataset\n",
    "    routes_hash = pd.merge(routes_hash, trips_hash[['route_id','hash', 'hash_inverse', 'service_id','stops_sequence','dates','count_service_id']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "    routes_hash = routes_hash.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "    \n",
    "    return trips_hash, generic_trips_information, routes_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>service_id</th>\n",
       "      <th>hash</th>\n",
       "      <th>hash_inverse</th>\n",
       "      <th>stops_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885001:8885704:4:1052:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>8515140851473385823</td>\n",
       "      <td>-1054191588631304569</td>\n",
       "      <td>[TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885001:8885704:4:1052:20210530</td>\n",
       "      <td>42</td>\n",
       "      <td>8515140851473385823</td>\n",
       "      <td>-1054191588631304569</td>\n",
       "      <td>[TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885001:8885704:4:1152:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>8515140851473385823</td>\n",
       "      <td>-1054191588631304569</td>\n",
       "      <td>[TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885001:8885704:4:1152:20210530</td>\n",
       "      <td>42</td>\n",
       "      <td>8515140851473385823</td>\n",
       "      <td>-1054191588631304569</td>\n",
       "      <td>[TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>88____:007::8885001:8885704:4:1252:20210418</td>\n",
       "      <td>14</td>\n",
       "      <td>8515140851473385823</td>\n",
       "      <td>-1054191588631304569</td>\n",
       "      <td>[TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25178</th>\n",
       "      <td>734</td>\n",
       "      <td>84____:007::8400280:8821105:4:1600:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>4949173871581555851</td>\n",
       "      <td>5386009782816802712</td>\n",
       "      <td>[DEN HAAG HS (NL), ROTTERDAM CS (NL), BREDA (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25179</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8812005:8400131:23:1218:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>-6600990888662582687</td>\n",
       "      <td>5962766524408415751</td>\n",
       "      <td>[BRUXELLES-NORD, SCHAERBEEK, BRUSSELS AIRPORT-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25180</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8812005:8400131:23:1618:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>-6600990888662582687</td>\n",
       "      <td>5962766524408415751</td>\n",
       "      <td>[BRUXELLES-NORD, SCHAERBEEK, BRUSSELS AIRPORT-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25181</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1323:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>7395534416167665609</td>\n",
       "      <td>-1014626424368754181</td>\n",
       "      <td>[NOORDERKEMPEN (BRECHT), ANVERS-LUCHTBAL, ANVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25182</th>\n",
       "      <td>734</td>\n",
       "      <td>88____:007::8821105:8812005:22:1723:20210418</td>\n",
       "      <td>25</td>\n",
       "      <td>7395534416167665609</td>\n",
       "      <td>-1014626424368754181</td>\n",
       "      <td>[NOORDERKEMPEN (BRECHT), ANVERS-LUCHTBAL, ANVE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25183 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_id                                       trip_id  service_id  \\\n",
       "0           115   88____:007::8885001:8885704:4:1052:20210418          14   \n",
       "1           115   88____:007::8885001:8885704:4:1052:20210530          42   \n",
       "2           115   88____:007::8885001:8885704:4:1152:20210418          14   \n",
       "3           115   88____:007::8885001:8885704:4:1152:20210530          42   \n",
       "4           115   88____:007::8885001:8885704:4:1252:20210418          14   \n",
       "...         ...                                           ...         ...   \n",
       "25178       734   84____:007::8400280:8821105:4:1600:20210418          25   \n",
       "25179       734  88____:007::8812005:8400131:23:1218:20210418          25   \n",
       "25180       734  88____:007::8812005:8400131:23:1618:20210418          25   \n",
       "25181       734  88____:007::8821105:8812005:22:1323:20210418          25   \n",
       "25182       734  88____:007::8821105:8812005:22:1723:20210418          25   \n",
       "\n",
       "                      hash         hash_inverse  \\\n",
       "0      8515140851473385823 -1054191588631304569   \n",
       "1      8515140851473385823 -1054191588631304569   \n",
       "2      8515140851473385823 -1054191588631304569   \n",
       "3      8515140851473385823 -1054191588631304569   \n",
       "4      8515140851473385823 -1054191588631304569   \n",
       "...                    ...                  ...   \n",
       "25178  4949173871581555851  5386009782816802712   \n",
       "25179 -6600990888662582687  5962766524408415751   \n",
       "25180 -6600990888662582687  5962766524408415751   \n",
       "25181  7395534416167665609 -1014626424368754181   \n",
       "25182  7395534416167665609 -1014626424368754181   \n",
       "\n",
       "                                          stops_sequence  \n",
       "0               [TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]  \n",
       "1               [TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]  \n",
       "2               [TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]  \n",
       "3               [TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]  \n",
       "4               [TOURNAI, FROYENNES, HERSEAUX, MOUSCRON]  \n",
       "...                                                  ...  \n",
       "25178  [DEN HAAG HS (NL), ROTTERDAM CS (NL), BREDA (N...  \n",
       "25179  [BRUXELLES-NORD, SCHAERBEEK, BRUSSELS AIRPORT-...  \n",
       "25180  [BRUXELLES-NORD, SCHAERBEEK, BRUSSELS AIRPORT-...  \n",
       "25181  [NOORDERKEMPEN (BRECHT), ANVERS-LUCHTBAL, ANVE...  \n",
       "25182  [NOORDERKEMPEN (BRECHT), ANVERS-LUCHTBAL, ANVE...  \n",
       "\n",
       "[25183 rows x 6 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_hash_Belgium, generic_trips_information_Belgium, routes_hash_Belgium = prepartion_space(railway_system_information_Belgium, calendar_dates_cleaned_Belgium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hash_Netherlands, generic_trips_information_Netherlands, routes_hash_Netherlands = prepartion_space(railway_system_information_Netherlands, calendar_dates_cleaned_Netherlands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hash_Switzerland, generic_trips_information_Switzerland, routes_hash_Switzerland = prepartion_space(railway_system_information_Switzerland, calendar_dates_cleaned_Switzerland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some functions to better factorise the functions in the coming cells'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stops_sequences(stops_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stops_sequences_df[stops_sequences_df['route_id'] == route_id].copy()\n",
    "\n",
    "\n",
    "def take_leftovers_list_c_from_intersection_AAndB(list_a, list_b, list_c):\n",
    "    '''take the indexes of the intersection of list a with list b and retain the elments of list c with that index'''\n",
    "    ind_dict = dict((k,i) for i,k in enumerate(list_a))\n",
    "    return [list_c[ind_dict[x]] for x in (set(list_a).intersection(list_b))]\n",
    "\n",
    "def get_extentions (after_or_behind, route_sequences_route_id, trip):\n",
    "    '''returns the extentions for the trip (behind or after)'''\n",
    "    if after_or_behind == 'after':\n",
    "        #checks the extentions possible for the trip that can follow after its last stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stops_sequence'].apply(lambda x: any(item for item in [trip['stops_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stops_sequence']))))].copy()\n",
    "    elif after_or_behind == 'behind':\n",
    "        #checks the extentions possible for the trip that can follow before its first stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stops_sequence'].apply(lambda x: any(item for item in [trip['stops_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stops_sequence']))))].copy()        \n",
    "    #checks that those extentions have a common date as the trip\n",
    "    possible_extentions = possible_extentions[possible_extentions['dates'].apply(lambda x: any(item for item in trip['dates'] if item in x))].copy()   \n",
    "    if not possible_extentions.empty: \n",
    "        if after_or_behind == 'after':\n",
    "            #checks that those extentions have a matching time schedule as the trip\n",
    "            possible_extentions = possible_extentions[possible_extentions['departure_time_first'].apply(lambda x: any(item for item in trip['departure_time_last'] if item in x))].copy()\n",
    "        elif after_or_behind == 'behind':\n",
    "            #checks that those extentions have a matching time schedule as the trip\n",
    "            possible_extentions = possible_extentions[possible_extentions['departure_time_last'].apply(lambda x: any(item for item in trip['departure_time_first'] if item in x))].copy()\n",
    "    return possible_extentions      \n",
    "\n",
    "def calculate_frequency (sequences_df):\n",
    "    '''calculate the frequency based on the length of the dates and departure_time and put the hash in as a column of list'''\n",
    "    sequences_df['number_dates'] = sequences_df['dates'].apply(lambda x: len(x))\n",
    "    sequences_df['number_times'] = sequences_df['departure_time_last'].apply(lambda x: len(x))\n",
    "    sequences_df['frequency'] = sequences_df['number_dates']* sequences_df['number_times'] \n",
    "    sequences_df = sequences_df.drop(['dates', 'departure_time_last', 'number_dates', 'number_times'], axis=1)\n",
    "    sequences_df['hash'] = sequences_df['hash'].apply(lambda x: [x])\n",
    "    return sequences_df.copy()\n",
    "         \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "def calculate_time_difference(time_df, later_time, earlier_time, column_name):\n",
    "    '''calculates the time difference between later time and earlier time and put it in time_df[column_name]'''\n",
    "    #transform 24:00:00 into 00:00:00\n",
    "    time_df['departure_time'] = time_df['departure_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >= 24 else x)\n",
    "    time_df['arrival_time'] = time_df['arrival_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >=  24 else x)\n",
    "    #calculate the waiting_time\n",
    "    time_df[column_name] = time_df[['arrival_time','departure_time']].apply(lambda x: int((datetime.strptime(x[later_time], FMT) - datetime.strptime(x[earlier_time], FMT)).total_seconds()/60), axis=1)\n",
    "    #if one day as past, take it into consideration\n",
    "    time_df[column_name] = time_df[column_name].apply(lambda x: day_in_seconds/60 + x if x < 0 else x)\n",
    "    return time_df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finds the routes that can be either extended from behind or from after and those which are complete sequences'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Finds the routes that can be either extended from behind or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the tree indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = get_extentions('after', route_sequences_route_id, trip)\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_behind = get_extentions('behind', route_sequences_route_id, trip)\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_behind, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                if possible_extentions_behind.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                if route_id not in index_of_complete_sequences:\n",
    "                    index_of_complete_sequences[route_id] = []\n",
    "                index_of_complete_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates all the sequences of routes possible to reconstruct the real route and calculates their frequency'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route and calculates their frequency'''\n",
    "\n",
    "def possible_sequences_construction(stops_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stops_sequences(stops_sequences_df, route_id)\n",
    "            #set default frequency to NaN\n",
    "            routes_with_route_id['frequency'] = np.nan\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stops_sequence', 'dates', 'departure_time_last','frequency']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stops_sequence', 'dates', 'departure_time_first', 'departure_time_last','frequency']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route \n",
    "                    #and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = get_extentions('after', route_creation_extensions_route_id, route_part)\n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not possible_extentions.empty:\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stops_sequence'] + possible_extention['stops_sequence'][1:]\n",
    "                            common_dates = possible_extention['dates'] & route_part['dates']\n",
    "                            new_departure_time_last = take_leftovers_list_c_from_intersection_AAndB(list(possible_extentions['departure_time_first'])[0], list(route_part['departure_time_last']), list(possible_extentions['departure_time_last'])[0])\n",
    "                            new_frequency = len(new_departure_time_last) * len(common_dates)\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_dates, new_departure_time_last, new_frequency]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    if 'departure_time_last' in route_creation.columns:\n",
    "        route_creation = route_creation.drop(['dates', 'departure_time_last'], axis=1)\n",
    "    route_creation = route_creation.reindex(columns=['route_id','hash','stops_sequence', 'frequency'])\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adds the full sequences to the route_creation dataframe'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stops_sequence', 'dates', 'departure_time_last']].copy()\n",
    "        copy_complete_sequences_df = calculate_frequency(copy_complete_sequences_df)\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adds the sequences that were not yet added in the route_creation dataframe'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        if route_id in route_creation['route_id'].unique():\n",
    "            #get a set of the hashes that were employed to create the routes for that route_id\n",
    "            used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "            #get a tuple of all the route sequences for that route_id\n",
    "            used_sequences = tuple(route_creation[route_creation['route_id'] == route_id]['stops_sequence'])\n",
    "            copy_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)[['route_id','hash','stops_sequence', 'dates', 'departure_time_last']]\n",
    "            copy_sequences_route_id = calculate_frequency(copy_sequences_route_id)\n",
    "            #adds the hashes that were not employed in any route creations for that route_id\n",
    "            for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "                #first element of the list because there is always only one element\n",
    "                if trip['hash'][0] not in used_sequences_hash:\n",
    "                    route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates a column in the df that calculates the travel time between the first and last stop (waiting time included)\\nand another column with the waiting time (calculated with a weighted average based on the frequency)'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creates a column in the df that calculates the travel time between the first and last stop (waiting time included)\n",
    "and another column with the waiting time (calculated with a weighted average based on the frequency)'''\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "\n",
    "def give_begin_end_time(route_creation_frequency_single, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates):\n",
    "    #create a copy to not change the input DataFrame\n",
    "    route_creation_frequency_single = route_creation_frequency_single.copy()\n",
    "    #makes a column with the a representative begin time and end time of the route\n",
    "    route_creation_frequency_single['travel_time'] = np.nan\n",
    "    for index_sequence, sequence in route_creation_frequency_single.iterrows():\n",
    "        constructed_route = pd.DataFrame()\n",
    "        for index_hash, hash_value in enumerate(sequence['hash']):\n",
    "            index_plus_one = index_hash + 1\n",
    "            #take all the trips with that hash\n",
    "            next_representative_trips = trips_hash_stops_sequence[(trips_hash_stops_sequence['hash'] == hash_value) & (trips_hash_stops_sequence['route_id'] == sequence['route_id'])].copy()['trip_id']\n",
    "            #take all the stop sequences and their time that belongs \n",
    "            full_times = stops_cleaned_stop_times_trips_merge_dates[stops_cleaned_stop_times_trips_merge_dates['trip_id'].isin(next_representative_trips)].copy()\n",
    "            #select) only the last stop sequences of full_times for each trip_id\n",
    "            new_index_max_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmax()\n",
    "            max_per_trip_id = full_times.reset_index().loc[new_index_max_per_trip_id]\n",
    "            #select only the first stop sequences of full_times for each trip_id            \n",
    "            new_index_min_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmin()            \n",
    "            min_per_trip_id = full_times.reset_index().loc[new_index_min_per_trip_id]\n",
    "            #merge max_per_trip_id and min_per_trip_id\n",
    "            merged = min_per_trip_id[['trip_id', 'dates', 'departure_time']].merge(max_per_trip_id[['trip_id', 'arrival_time', 'departure_time']], on='trip_id')\n",
    "            #take all the stop sequences except the first one, and the last one if it is not the last sequence of the route\n",
    "            if index_hash == len(sequence['hash']) - 1:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(pd.concat([new_index_min_per_trip_id,new_index_max_per_trip_id]))\n",
    "            else:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(new_index_min_per_trip_id)            \n",
    "            #ONLY NEEDED FOR SWITZERLAND\n",
    "            rest_per_trip_id = rest_per_trip_id.dropna()\n",
    "            if not rest_per_trip_id.empty:\n",
    "                rest_per_trip_id = calculate_time_difference(rest_per_trip_id, 'departure_time', 'arrival_time', 'waiting_time')\n",
    "                #calculate the total waiting_time\n",
    "                rest_per_trip_id_grouped = rest_per_trip_id.groupby(['trip_id'], as_index=False)['waiting_time'].sum()\n",
    "                merged_waiting_time = merged.merge(rest_per_trip_id_grouped, on='trip_id')\n",
    "            #in case there are only two stops in for the hash\n",
    "            else:\n",
    "                merged_waiting_time = merged.copy()\n",
    "                merged_waiting_time['waiting_time'] = 0\n",
    "            #rename the columns     \n",
    "            merged_waiting_time = merged_waiting_time.rename(columns = {'trip_id': 'trip_id_' + str(index_plus_one),'departure_time_x':'departure_time_'+ str(index_plus_one), 'arrival_time':'arrival_time_'+ str(index_plus_one),\n",
    "                                          'departure_time_y':'departure_time_'+ str(index_plus_one + 1), 'waiting_time': 'waiting_time_' + str(index_plus_one)})\n",
    "            if index_hash == 0:\n",
    "                constructed_route = merged_waiting_time\n",
    "            elif index_hash > 0:\n",
    "                constructed_route = constructed_route.merge(merged_waiting_time, how='inner', on=['departure_time_' + str(index_plus_one)])\n",
    "                #take the intersection of the dates => only get the common dates and retain those rows with common dates\n",
    "                constructed_route['dates'] = [a & b for a,b in zip(constructed_route['dates_x'], constructed_route['dates_y'])]\n",
    "                constructed_route = constructed_route[constructed_route['dates'].map(lambda d: len(d)) > 0]\n",
    "                constructed_route = constructed_route.drop(['dates_x','dates_y'], axis=1)        \n",
    "        #make a list of all the columns of waiting_times\n",
    "        list_column_waiting_time = []\n",
    "        for i in range(1, index_plus_one + 1):\n",
    "            list_column_waiting_time.append('waiting_time_' + str(i))\n",
    "        #sum all the waiting times together for each route itinerary\n",
    "        constructed_route['waiting_time'] = constructed_route[list_column_waiting_time].astype(int).sum(1)\n",
    "        \n",
    "        #sometimes it is impossible to find trips that follow each other\n",
    "        if not constructed_route.empty:\n",
    "            #when the loop is finished, take the last arrival time, that will be used to calculate the travel time\n",
    "            time_constructed_route = constructed_route[['departure_time_1', 'arrival_time_' + str(index_plus_one), 'waiting_time', 'dates']]\n",
    "            time_constructed_route = time_constructed_route.rename(columns = {'departure_time_1':'departure_time', 'arrival_time_' + str(index_plus_one):'arrival_time'})\n",
    "            time_constructed_route = calculate_time_difference(time_constructed_route, 'arrival_time', 'departure_time', 'time_diff_min')\n",
    "            #add here a new column count dates that is the sum of the common dates\n",
    "            time_constructed_route['count_dates'] = time_constructed_route['dates'].apply(lambda x: len(x))\n",
    "            sum_count_dates = time_constructed_route['count_dates'].sum()\n",
    "            #take the first most frequent one\n",
    "            #create the weighted sum\n",
    "            time_constructed_route['WS_travel_time'] = (time_constructed_route['time_diff_min'] * time_constructed_route['count_dates'])/sum_count_dates\n",
    "            time_constructed_route['WS_waiting_time'] = (time_constructed_route['waiting_time'] * time_constructed_route['count_dates'])/sum_count_dates    \n",
    "            weighted_sum_tt = time_constructed_route['WS_travel_time'].sum()\n",
    "            weighted_sum_wt = time_constructed_route['WS_waiting_time'].sum()\n",
    "            #Add this to the first dataframe\n",
    "            route_creation_frequency_single.loc[index_sequence,'travel_time'] = weighted_sum_tt\n",
    "            route_creation_frequency_single.loc[index_sequence,'waiting_time'] = weighted_sum_wt\n",
    "        #if there is no trips that follow each other with the hash from the array\n",
    "        else:\n",
    "            route_creation_frequency_single = route_creation_frequency_single.drop(index_sequence)\n",
    "            \n",
    "    return route_creation_frequency_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    route_creation_hash['hash'] = route_creation_hash['stops_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "    route_creation_hash['hash_inverse'] = route_creation_hash['stops_sequence'].apply(lambda x: hash(tuple(x[::-1])))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regroup the routes that are the same (even though they are in the opposite direction)'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stops_sequences(route_creation_hash):\n",
    "    '''regroups the stops_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_hash.copy().drop(['frequency'], axis = 1)\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stops_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stops_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id'])\n",
    "    route_hash_freq_combined = route_hash_freq_combined.reset_index(drop = True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #drop the routes with the same hash as others\n",
    "    route_hash_freq_treshold['max_hash'] = route_hash_freq_treshold[['hash', 'hash_inverse']].max(axis=1)\n",
    "    route_hash_freq_treshold = route_hash_freq_treshold.drop_duplicates(subset='max_hash')\n",
    "    route_hash_freq_treshold  = route_hash_freq_treshold.drop(['hash_inverse', 'max_hash'], axis = 1)\n",
    "    #selects the sequences that are not the first most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == sequences_max_freq_merged['max_frequency']].drop_duplicates(subset='route_id').index\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[~sequences_max_freq_merged.index.isin(sequences_max_freq_index)].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    if route_hash_freq_combined['route_id'].dtype == np.int64:\n",
    "        route_id_creation = route_hash_freq_combined['route_id'].max() + 1\n",
    "    else:\n",
    "        route_id_creation =  0 + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    #keep only the column route_id and stops_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == 0].index)\n",
    "    final_routes = final_routes.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To keep only the routes that have at least one country station in their route_sequence'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' To keep only the routes that have at least one country station in their route_sequence'''\n",
    "\n",
    "def keep_country_routes(final_routes, stops_country_series):\n",
    "    non_country_routes = set()\n",
    "    for index_route, route in final_routes.iterrows():\n",
    "        is_in_country = False\n",
    "        for stop in route['stops_sequence']:\n",
    "            if stop in set(stops_country_series):\n",
    "                is_in_country = True\n",
    "                break\n",
    "        if not is_in_country:\n",
    "            route_id = route['route_id']\n",
    "            non_country_routes.add(route_id)\n",
    "    country_routes = final_routes.loc[~final_routes['route_id'].isin(non_country_routes)]    \n",
    "\n",
    "    return country_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calculates the distances of the trip, by taking the distance between each stop of the stop_sequence'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Calculates the distances of the trip, by taking the distance between each stop of the stop_sequence'''\n",
    "\n",
    "def calculate_distance_from_lat_long(name_first, name_second, stop_df):\n",
    "        lon_first, lat_first = math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lat'])\n",
    "        lon_second, lat_second = math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lat'])\n",
    "        # The radius of the earth\n",
    "        R = 6373.0 \n",
    "        # To calculate the change in coordinates\n",
    "        dlon = lon_second - lon_first\n",
    "        dlat = lat_second - lat_first\n",
    "        # To use the Haversine formula to get the distance in kilometers between the starting_station and the ending_station\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat_first) * math.cos(lat_second) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        # To calculate the distance\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "def calculate_distance(stop_sequence, stop_df):\n",
    "    distance = 0\n",
    "    for index_stop ,stop in enumerate(stop_sequence):\n",
    "        index_plus_one = index_stop + 1\n",
    "        if index_plus_one <= len(stop_sequence) - 1:\n",
    "            distance += calculate_distance_from_lat_long(stop, stop_sequence[index_plus_one], stop_df)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makes a df that can be used for building the nodes and edges of the graph using Networkx package'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Makes a df that can be used for building the nodes and edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx L-space graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stops_sequence_values = final_routes.apply(lambda x: pd.Series(x['stops_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stops_sequence_values.name = 'stops_sequence'\n",
    "    final_routes_stops = final_routes.drop('stops_sequence', axis=1).join(stops_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stops_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stops_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stops_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applies all the functions from 1 get_extention_indexes to 11 create_df_for_Networkx'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Applies all the functions from 1 get_extention_indexes to 11 create_df_for_Networkx'''\n",
    "\n",
    "def full_route_creation(stops_sequences_df, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned, stops_country_series):\n",
    "    '''return a df that can be used to make a Networkx L-space (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stops_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stops_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stops_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stops_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single_travel_time = give_begin_end_time(route_creation_third, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single_travel_time)\n",
    "    route_hash_freq_combined = regroup_same_stops_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    country_routes = keep_country_routes(final_routes, stops_country_series)\n",
    "    country_routes['distance'] = country_routes['stops_sequence'].apply(lambda x: calculate_distance(x, stops_cleaned))\n",
    "    df_for_edges = create_df_for_Networkx(country_routes)\n",
    "    \n",
    "    return country_routes, df_for_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belgian_routes, df_for_edges_Belgium = full_route_creation(routes_hash_Belgium, generic_trips_information_Belgium, trips_hash_Belgium, stops_cleaned_Belgium, stops_Belgium_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save those two last DataFrames as .csv files'''\n",
    "belgian_routes.to_csv(f'{stops_cleaned_loc}stops_cleaned_{country_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regroup_same_stops_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-fe5a7a9af8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mroute_creation_frequency_single_travel_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgive_begin_end_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute_creation_third\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrips_hash_stops_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstops_cleaned_stop_times_trips_merge_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mroute_creation_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_hash_route_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute_creation_frequency_single_travel_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mroute_hash_freq_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregroup_same_stops_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute_creation_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfinal_routes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_treshold_route_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute_hash_freq_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcountry_routes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_swiss_routes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_routes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstops_country_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regroup_same_stops_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "stops_sequences_df, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned, stops_country_series = routes_hash_Belgium, generic_trips_information_Belgium, trips_hash_Belgium, stops_cleaned_Belgium, stops_Belgium_series\n",
    "\n",
    "index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stops_sequences_df)\n",
    "route_creation_first = possible_sequences_construction(stops_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "route_creation_second = add_full_sequences(stops_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "route_creation_third = add_unused_sequences(stops_sequences_df, route_creation_second)\n",
    "route_creation_frequency_single_travel_time = give_begin_end_time(route_creation_third, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates)\n",
    "route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single_travel_time)\n",
    "route_hash_freq_combined = regroup_same_stops_sequences(route_creation_hash)\n",
    "final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "country_routes = keep_country_routes(final_routes, stops_country_series)\n",
    "country_routes['distance'] = country_routes['stops_sequence'].apply(lambda x: calculate_distance(x, stops_cleaned))\n",
    "df_for_edges = create_df_for_Networkx(country_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_sequences_df, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned, stops_country_series = routes_hash_Belgium, generic_trips_information_Belgium, trips_hash_Belgium, stops_cleaned_Belgium, stops_Belgium_series\n",
    "\n",
    "final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "country_routes = keep_country_routes(final_routes, stops_country_series)\n",
    "country_routes['distance'] = country_routes['stops_sequence'].apply(lambda x: calculate_distance(x, stops_cleaned))\n",
    "df_for_edges = create_df_for_Networkx(country_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        SCHAERBEEK\n",
       "1             EVERE\n",
       "2         HAREN-SUD\n",
       "3              BUDA\n",
       "4             HAREN\n",
       "           ...     \n",
       "559           YPRES\n",
       "560       POPERINGE\n",
       "561         ROULERS\n",
       "562          IZEGEM\n",
       "563    INGELMUNSTER\n",
       "Name: stop_name, Length: 564, dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_country_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>stops_sequence</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>waiting_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>167</td>\n",
       "      <td>[ANVERS-CENTRAL, ANVERS-BERCHEM, MORTSEL, BOEC...</td>\n",
       "      <td>52.30303</td>\n",
       "      <td>6.69697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>167</td>\n",
       "      <td>[HAL, BRUXELLES-MIDI]</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     route_id                                     stops_sequence  travel_time  \\\n",
       "50        167  [ANVERS-CENTRAL, ANVERS-BERCHEM, MORTSEL, BOEC...     52.30303   \n",
       "617       167                              [HAL, BRUXELLES-MIDI]      8.00000   \n",
       "\n",
       "     waiting_time  \n",
       "50        6.69697  \n",
       "617       0.00000  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_routes[final_routes['route_id'] == 167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_routes = keep_country_routes(final_routes, stops_country_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_routes['waiting_time'].dtype == np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
