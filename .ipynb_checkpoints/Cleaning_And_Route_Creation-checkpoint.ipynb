{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ensure that the output results of extensive output results are not truncated.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Change the width of the Notebook to see the output on the screen'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Change the width of the Notebook to see the output on the screen'''\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If your computer is a Windows and that you are using the file locally (repository_loc == os.getcwd()) put True, False otherwise'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''If your computer is a Windows and that you are using the file locally (repository_loc == os.getcwd()) put True, False otherwise'''\n",
    "windows_locally = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Register the GitHub link or the file relative location'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Register the GitHub link or the file relative location'''\n",
    "#the Github link\n",
    "#repository_loc, windows_locally = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main\", False\n",
    "#the local link\n",
    "repository_loc = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get the other folder locations'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the other folder locations'''\n",
    "\n",
    "belgian_GTFS_loc = repository_loc + '/gtfs_train_Belgium_1503/'\n",
    "dutch_GTFS_loc = repository_loc + '/gtfs_train_Netherlands_1503/'\n",
    "swiss_GTFS_loc = repository_loc + '/gtfs_train_Switzerland_1503/'\n",
    "\n",
    "stops_series_loc = repository_loc + '/country_stops_series/'\n",
    "stops_cleaned_loc = repository_loc + '/stops_cleaned/'\n",
    "df_for_edges_loc = repository_loc + '/df_for_edges/'\n",
    "routes_loc = repository_loc + '/routes/'\n",
    "\n",
    "if windows_locally:\n",
    "    belgian_GTFS_loc = belgian_GTFS_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "    dutch_GTFS_loc = dutch_GTFS_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "    swiss_GTFS_loc = swiss_GTFS_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "\n",
    "    stops_series_loc = stops_series_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "    stops_cleaned_loc = stops_cleaned_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "    df_for_edges_loc = df_for_edges_loc.replace('/', \" \\\\ \").replace(' ', \"\")\n",
    "    routes_loc = routes_loc.replace('/', \" \\\\ \").replace(' ', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import all the DataFrames that are common for the three train networks'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import all the DataFrames that are common for the three train networks'''\n",
    "\n",
    "def common_imports(datalink):\n",
    "    #To import the agency dataset that contains limited information about the railway agency.\n",
    "    agency = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "    #To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "    calendar_dates = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n",
    "    #To import the routes dataset that provides the id, the name and the type of vehicle used for all railway routes.\n",
    "    routes = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "    #To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the railway stations.\n",
    "    stops = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "    #To import the transfers dataset that gives the minimum transfer time to switch routes at each railway station.\n",
    "    transfers = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "    #To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the railway route.\n",
    "    #The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "    trips = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "    return agency, calendar_dates, routes, stops, transfers, trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Belgium, calendar_dates_Belgium, routes_Belgium, stops_Belgium, transfers_Belgium, trips_Belgium = common_imports(belgian_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the translations dataset that provides the French-, Dutch-, German- and English-language translations of the Belgian railway stations.\n",
    "translations_Belgium = pd.read_csv(belgian_GTFS_loc + \"translations.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Belgium = pd.read_csv(belgian_GTFS_loc + \"calendar.txt\", sep=\",\")\n",
    "#To import the stop_time_overrides dataset \n",
    "stop_time_overrides_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_time_overrides.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Netherlands, calendar_dates_Netherlands, routes_Netherlands, stops_Netherlands, transfers_not_cleaned_Netherlands, trips_Netherlands = common_imports(dutch_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Dutch NS railway feed.\n",
    "feed_info_Netherlands = pd.read_csv(dutch_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_range = [*range(2, 19)]\n",
    "stop_times_Netherlands = pd.read_csv(dutch_GTFS_loc + \"stop_times-1.csv\", sep=\",\")\n",
    "for index in stop_times_range:\n",
    "    stop_times_Netherlands = pd.concat([stop_times_Netherlands, pd.read_csv(dutch_GTFS_loc + \"stop_times-\" + str(index)+ \".csv\", sep=\",\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Switzerland, calendar_dates_Switzerland, routes_Switzerland, stops_Switzerland, transfers_not_cleaned_Switzerland, trips_Switzerland = common_imports(swiss_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Swiss SBB railway feed.\n",
    "feed_info_Switzerland = pd.read_csv(swiss_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Switzerland = pd.read_csv(swiss_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Switzerland = pd.read_csv(swiss_GTFS_loc + \"calendar.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the railway data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the calendar_dates DataFrame'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the calendar_dates DataFrame'''\n",
    "\n",
    "def clean_calendar_dates(calendar_dates):\n",
    "    #To filter the dates from the selected begin to the end date\n",
    "    begin_date = 20210314\n",
    "    end_date = 20210713\n",
    "    calendar_dates_cleaned = calendar_dates.copy()\n",
    "    calendar_dates_cleaned = calendar_dates_cleaned.drop(calendar_dates_cleaned[(calendar_dates_cleaned['date'] > end_date) | (calendar_dates_cleaned['date'] < begin_date)].index)\n",
    "    return calendar_dates_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'''\n",
    "\n",
    "def country_information(stops, country_name, stops_cleaned_loc, stops_series_loc):\n",
    "    #To initialize the Nominatim API to get the location from the input string \n",
    "    geolocator = Nominatim(user_agent=\"application\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "    #To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "    country_list = []\n",
    "    for index, row in stops.iterrows():\n",
    "        latitude = row['stop_lat']\n",
    "        longitude = row['stop_lon']\n",
    "        # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "        location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "        # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "        address = location.raw['address']\n",
    "        country = address.get('country', '')\n",
    "        country_list.append(country)\n",
    "\n",
    "    #To add the values of country_list as a new attribute country \n",
    "    stops.loc[:,'country'] = country_list\n",
    "\n",
    "    #To calculate the total number of Belgian stations in the stops dataset\n",
    "    country_stops = stops[stops['country'] == country_name]\n",
    "    country_stops_series = stops.loc[stops['country'] == country_name, 'stop_name']\n",
    "    \n",
    "    stops.to_csv(f'{stops_cleaned_loc}stops_cleaned_{country_name}.csv')\n",
    "    country_stops_series.to_csv(f'{stops_series_loc}stops_{country_name}_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remove the accents from a string'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Remove the accents from a string'''\n",
    "\n",
    "def remove_accents(text):\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Belgium df'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Belgium df'''\n",
    "allowed_route_type = {'IC', 'L', 'P', 'ICT', 'IZY'}\n",
    "routes_cleaned_Belgium = routes_Belgium[(routes_Belgium['route_short_name'].isin(allowed_route_type)) | (routes_Belgium['route_short_name'].str.startswith('S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Belgium = clean_calendar_dates(calendar_dates_Belgium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops_Belgium df.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops_Belgium df.''' \n",
    "#To eliminate the stop_ids in the stops dataset that contain an underscore or that start with a character 'S'. \n",
    "stops_cleaned_Belgium = stops_Belgium[(~stops_Belgium['stop_id'].str.contains('_')) & (~stops_Belgium['stop_id'].str.contains('S'))]\n",
    "\n",
    "#To modify the object datatype of the stop_id column to the NumPy int64 datatype\n",
    "stops_cleaned_Belgium.loc[:,'stop_id'] = stops_cleaned_Belgium.loc[:,'stop_id'].astype(np.int64)\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Belgium'\n",
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE COUNTRIES, BUT IT TAKES A LONG TIME\n",
    "#country_information(stops_cleaned_Belgium, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Belgium = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Belgium_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Netherlands DataFrame'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>agency_id</th>\n",
       "      <th>route_short_name</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>route_desc</th>\n",
       "      <th>route_type</th>\n",
       "      <th>route_color</th>\n",
       "      <th>route_text_color</th>\n",
       "      <th>route_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>145</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>76977</td>\n",
       "      <td>IFF:NS</td>\n",
       "      <td>Sprinter</td>\n",
       "      <td>Nachtnettrein Utrecht Centraal &lt;-&gt; Rotterdam C...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_id agency_id route_short_name  \\\n",
       "count       145       145              145   \n",
       "unique      145        11               15   \n",
       "top       76977    IFF:NS         Sprinter   \n",
       "freq          1        87               47   \n",
       "\n",
       "                                          route_long_name route_desc  \\\n",
       "count                                                 145        145   \n",
       "unique                                                144          1   \n",
       "top     Nachtnettrein Utrecht Centraal <-> Rotterdam C...        nan   \n",
       "freq                                                    2        145   \n",
       "\n",
       "       route_type route_color route_text_color route_url  \n",
       "count         145         145              145       145  \n",
       "unique          1           1                1         1  \n",
       "top             2         nan              nan       nan  \n",
       "freq          145         145              145       145  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Netherlands DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Netherlands = routes_Netherlands[routes_Netherlands['route_type'] == 2]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands.astype(str)\n",
    "routes_cleaned_Netherlands.describe(include=['object'])\n",
    "\n",
    "#To change the route_id object datatype to a NumPy int64 datatype\n",
    "routes_cleaned_Netherlands.loc[:,'route_id'] = routes_cleaned_Netherlands.loc[:,'route_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Netherlands = clean_calendar_dates(calendar_dates_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops DataFrame'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops DataFrame'''\n",
    "#To take from the stops_initial_Netherlands df all stop_ids that contain a 'stoparea:' to get the correct stop coordinates\n",
    "stops_cleaned_Netherlands = stops_Netherlands[stops_Netherlands['stop_id'].str.contains('stoparea:')]\n",
    "\n",
    "#To remove the accents from the accented characters and to convert the remaining characters to uppercase characters\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Netherlands'\n",
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE COUNTRIES, BUT IT TAKES A LONG TIME\n",
    "#country_information(stops_cleaned_Netherlands, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Netherlands = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Netherlands_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stop_times df'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stop_times df'''\n",
    "stop_times_cleaned_Netherlands = stop_times_Netherlands.copy()\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_id'] = stop_times_cleaned_Netherlands.stop_id.apply(str)\n",
    "stop_times_cleaned_Netherlands = pd.merge(stop_times_cleaned_Netherlands, stops_Netherlands[['stop_id', 'stop_name']], on='stop_id')\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Switzerland DataFrame'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Switzerland DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Switzerland = routes_Switzerland[routes_Switzerland['route_type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Switzerland = clean_calendar_dates(calendar_dates_Switzerland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stop_times_Switzerland DataFrame'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stop_times_Switzerland DataFrame'''\n",
    "# To remove the superfluous characters of the stop_id (platform codes)\n",
    "stop_times_cleaned_Switzerland = stop_times_Switzerland.copy()\n",
    "stop_times_cleaned_Switzerland_column = stop_times_cleaned_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland_column\n",
    "\n",
    "# To make the stop_ids numerical \n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops_Switzerland DataFrame'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stops_Switzerland DataFrame'''\n",
    "#To remove the superfluous characters (platform codes)\n",
    "stops_cleaned_Switzerland_column = stops_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stops_cleaned_Switzerland = stops_Switzerland.copy()\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland_column\n",
    "\n",
    "#To make the stop_ids numerical and to remove the duplicate stop_ids\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland.drop_duplicates()\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Switzerland'\n",
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE COUNTRIES, BUT IT TAKES A LONG TIME\n",
    "#country_information(stops_cleaned_Switzerland, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Switzerland = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")\n",
    "stops_Switzerland_series = pd.read_csv(f\"{stops_series_loc}stops_{country_name}_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merge the DataFrames'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Merge the DataFrames'''\n",
    "\n",
    "def merge_df(stop_times, stops, routes, trips, calendar_dates, on_stop):\n",
    "    list_columns = ['stop_name', 'stop_lat', 'stop_lon', 'country']\n",
    "    if on_stop == 'stop_id':\n",
    "        list_columns.append('stop_id')\n",
    "    #To merge the stop_times df with the stops df on stop_id\n",
    "    stop_times_stops = pd.merge(stop_times, stops[list_columns], on= on_stop)\n",
    "\n",
    "    #To merge the trips df with the routes df on route_id\n",
    "    routes_trips = pd.merge(routes[['route_id']], trips, on='route_id')\n",
    "\n",
    "    #To merge the stop_times_stops df with the trips_routes df on trip_id\n",
    "    uncleaned_railway_system_information = pd.merge(routes_trips, stop_times_stops, on='trip_id')\n",
    "\n",
    "    #To take only the service_ids present in both the routes_trips_stop_times_stops df and the calendar_dates df into account\n",
    "    calendar_dates_unique = calendar_dates['service_id'].unique()\n",
    "    railway_system_information = uncleaned_railway_system_information[(uncleaned_railway_system_information['service_id'].isin(calendar_dates_unique))]\n",
    "    \n",
    "    return railway_system_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Belgium = agency_Belgium[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Belgium = routes_cleaned_Belgium[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Belgium = trips_Belgium[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Belgium = calendar_dates_cleaned_Belgium[['service_id', 'date']]\n",
    "stops_cleaned_Belgium = stops_cleaned_Belgium[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Belgium = stop_times_Belgium[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Belgium = merge_df(stop_times_cleaned_Belgium, stops_cleaned_Belgium, routes_cleaned_Belgium, trips_cleaned_Belgium, calendar_dates_cleaned_Belgium, 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Netherlands = agency_Netherlands[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Netherlands = trips_Netherlands[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Netherlands = calendar_dates_cleaned_Netherlands[['service_id', 'date']]\n",
    "stops_cleaned_Netherlands = stops_cleaned_Netherlands[['stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Netherlands = stop_times_cleaned_Netherlands[['trip_id', 'stop_name', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Netherlands = merge_df(stop_times_cleaned_Netherlands, stops_cleaned_Netherlands, routes_cleaned_Netherlands, trips_cleaned_Netherlands, calendar_dates_cleaned_Netherlands, 'stop_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Switzerland = agency_Switzerland[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Switzerland = routes_cleaned_Switzerland[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Switzerland = trips_Switzerland[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Switzerland = calendar_dates_cleaned_Switzerland[['service_id', 'date']]\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Switzerland = stop_times_cleaned_Switzerland[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "railway_system_information_Switzerland = merge_df(stop_times_cleaned_Switzerland, stops_cleaned_Switzerland, routes_cleaned_Switzerland, trips_cleaned_Switzerland, calendar_dates_cleaned_Switzerland, 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the space-of-stops representation of the railway systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create a DataFrame with the departure time form the first stop sequence and with the one from last stop sequence for each trip_id'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create a DataFrame with the departure time form the first stop sequence and with the one from last stop sequence for each trip_id'''\n",
    "\n",
    "def create_trip_departure_times(railway_system_information):\n",
    "    departure_time_first = railway_system_information.reset_index().loc[railway_system_information.reset_index().groupby(['trip_id'])['stop_sequence'].idxmin()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "    departure_time_first = departure_time_first.rename(columns = {'departure_time': 'departure_time_first'})\n",
    "    departure_time_last = railway_system_information.reset_index().loc[railway_system_information.reset_index().groupby(['trip_id'])['stop_sequence'].idxmax()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "    departure_time_last = departure_time_last.rename(columns = {'departure_time': 'departure_time_last'})\n",
    "    trip_departure_times = departure_time_first.merge(departure_time_last[['trip_id', 'departure_time_last']], on='trip_id')\n",
    "    return trip_departure_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame and\\nCalculate the hash of the stop sequence in both order (ascending and descending)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame and\n",
    "Calculate the hash of the stop sequence in both order (ascending and descending)'''\n",
    "\n",
    "def create_trip_stop_sequence(trip_departure_times):    \n",
    "    #Put the stop_names per trip_id in a list in the new trip_stops_sequence DataFrame\n",
    "    trip_stop_sequence = trip_departure_times.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    trip_stop_sequence.rename(columns={'stop_name':'stops_sequence'}, inplace=True)\n",
    "    #Calculate the hash of the stop sequence in both order (ascending and descending)\n",
    "    trip_stop_sequence['hash'] = trip_stop_sequence['stops_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "    trip_stop_sequence['hash_inverse'] = trip_stop_sequence['stops_sequence'].apply(lambda x: hash(tuple(x[::-1])))\n",
    "    return trip_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regroup the days per service id in a set and count them'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Regroup the days per service id in a set and count them'''\n",
    "\n",
    "def create_service_id_dates(calendar_dates):\n",
    "    service_id_dates = calendar_dates.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "    service_id_dates.rename(columns={'date':'dates'}, inplace=True)\n",
    "    service_id_dates['count_service_id'] = service_id_dates['dates'].apply(lambda x: len(x))\n",
    "    return service_id_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Put the different trip_ids in a list and add the departure_time first and last lists'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Put the different trip_ids in a list and add the departure_time first and last lists'''\n",
    "\n",
    "def create_routes_hash(trips_hash):\n",
    "    common_columns = ['route_id','hash', 'hash_inverse', 'service_id']\n",
    "    routes_hash = trips_hash.groupby(common_columns)['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    route_hash_dep_first = trips_hash.groupby(common_columns)['departure_time_first'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    route_hash_dep_last = trips_hash.groupby(common_columns)['departure_time_last'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "    routes_hash = routes_hash.merge(route_hash_dep_first, on= common_columns)\n",
    "    routes_hash = routes_hash.merge(route_hash_dep_last, on= common_columns)\n",
    "    return routes_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create DataFrames that will be used for the route_creation process'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create DataFrames that will be used for the route_creation process'''\n",
    "\n",
    "def prepartion_space(railway_system_information, calendar_dates):    \n",
    "    #Sort values by the route_id, the trip_id, and the stop_sequence fields\n",
    "    railway_system_information = railway_system_information.sort_values(by=['route_id', 'trip_id','service_id', 'stop_sequence'])\n",
    "\n",
    "    trip_departure_times = create_trip_departure_times(railway_system_information)\n",
    "\n",
    "    #Merge railway_system_information with trip_departure_times\n",
    "    trip_departure_times = railway_system_information.merge(trip_departure_times[['trip_id','departure_time_first','departure_time_last']], on='trip_id')\n",
    "\n",
    "    trip_stop_sequence = create_trip_stop_sequence(trip_departure_times)\n",
    "    \n",
    "    #Add the stop_sequence of stations to the trip_departure_times dataset by joining on trip_id\n",
    "    trips_hash = pd.merge(trip_departure_times, trip_stop_sequence, on='trip_id')\n",
    "    \n",
    "    service_id_dates = create_service_id_dates(calendar_dates)\n",
    "    \n",
    "    #Merge trips_hash with service_id_dates\n",
    "    trips_hash = pd.merge(trips_hash, service_id_dates, on='service_id', how='left')\n",
    "    \n",
    "    #Calculate generic_trips_information\n",
    "    generic_trips_information = trips_hash.groupby(['route_id', 'trip_id', 'service_id', 'hash', 'hash_inverse', 'departure_time_first','departure_time_last', 'count_service_id'], as_index=False)[['stops_sequence', 'dates']].first()\n",
    "    \n",
    "    routes_hash = create_routes_hash(generic_trips_information)\n",
    "    \n",
    "    #Add the sequence of stops, dates and service_id_count to the route_hash_freq_dep dataset\n",
    "    routes_hash = pd.merge(routes_hash, trips_hash[['route_id','hash', 'hash_inverse', 'service_id','stops_sequence','dates','count_service_id']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "    routes_hash = routes_hash.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "    \n",
    "    #Sort the rows, so that they will always output on the same order\n",
    "    routes_hash['departure_time_first_first'] = routes_hash['departure_time_first'].apply(lambda x: x[0])\n",
    "    routes_hash = routes_hash.sort_values(by=['route_id', 'service_id', 'departure_time_first_first'])\n",
    "    routes_hash = routes_hash.drop(columns=['departure_time_first_first'])\n",
    "    routes_hash = routes_hash.reset_index(drop=True)\n",
    "    \n",
    "    return trips_hash, generic_trips_information, routes_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hash_Belgium, generic_trips_information_Belgium, routes_hash_Belgium = prepartion_space(railway_system_information_Belgium, calendar_dates_cleaned_Belgium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hash_Netherlands, generic_trips_information_Netherlands, routes_hash_Netherlands = prepartion_space(railway_system_information_Netherlands, calendar_dates_cleaned_Netherlands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hash_Switzerland, generic_trips_information_Switzerland, routes_hash_Switzerland = prepartion_space(railway_system_information_Switzerland, calendar_dates_cleaned_Switzerland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some functions to better factorise the functions in the coming cells'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stops_sequences(stops_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stops_sequences_df[stops_sequences_df['route_id'] == route_id].copy()\n",
    "\n",
    "\n",
    "def take_leftovers_list_c_from_intersection_AAndB(list_a, list_b, list_c):\n",
    "    '''take the indexes of the intersection of list a with list b and retain the elments of list c with that index'''\n",
    "    ind_dict = dict((k,i) for i,k in enumerate(list_a))\n",
    "    return [list_c[ind_dict[x]] for x in (set(list_a).intersection(list_b))]\n",
    "\n",
    "def get_extentions (after_or_before, time_compatibility, route_sequences_route_id, trip):\n",
    "    '''returns the extentions for the trip (before or after)'''\n",
    "    if after_or_before == 'after':\n",
    "        #checks the extentions possible for the trip that can follow after its last stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stops_sequence'].apply(lambda x: any(item for item in [trip['stops_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stops_sequence']))))].copy()\n",
    "    elif after_or_before == 'before':\n",
    "        #checks the extentions possible for the trip that can follow before its first stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stops_sequence'].apply(lambda x: any(item for item in [trip['stops_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stops_sequence']))))].copy()        \n",
    "    if time_compatibility == True:    \n",
    "        #checks that those extentions have a common date as the trip\n",
    "        possible_extentions = possible_extentions[possible_extentions['dates'].apply(lambda x: any(item for item in trip['dates'] if item in x))].copy()   \n",
    "        if not possible_extentions.empty: \n",
    "            if after_or_before == 'after':\n",
    "                #checks that those extentions have a matching time schedule as the trip\n",
    "                possible_extentions = possible_extentions[possible_extentions['departure_time_first'].apply(lambda x: any(item for item in trip['departure_time_last'] if item in x))].copy()\n",
    "            elif after_or_before == 'before':\n",
    "                #checks that those extentions have a matching time schedule as the trip\n",
    "                possible_extentions = possible_extentions[possible_extentions['departure_time_last'].apply(lambda x: any(item for item in trip['departure_time_first'] if item in x))].copy()\n",
    "    return possible_extentions      \n",
    "\n",
    "def calculate_frequency (sequences_df):\n",
    "    '''calculate the frequency based on the length of the dates and departure_time and put the hash in as a column of list'''\n",
    "    sequences_df['number_dates'] = sequences_df['dates'].apply(lambda x: len(x))\n",
    "    sequences_df['number_times'] = sequences_df['departure_time_last'].apply(lambda x: len(x))\n",
    "    sequences_df['frequency'] = sequences_df['number_dates']* sequences_df['number_times'] \n",
    "    sequences_df = sequences_df.drop(['dates', 'departure_time_last', 'number_dates', 'number_times'], axis=1)\n",
    "    sequences_df['hash'] = sequences_df['hash'].apply(lambda x: [x])\n",
    "    return sequences_df.copy()\n",
    "         \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "def calculate_time_difference(time_df, later_time, earlier_time, column_name):\n",
    "    '''calculates the time difference between later time and earlier time and put it in time_df[column_name]'''\n",
    "    #transform 24:00:00 into 00:00:00\n",
    "    time_df['departure_time'] = time_df['departure_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >= 24 else x)\n",
    "    time_df['arrival_time'] = time_df['arrival_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >=  24 else x)\n",
    "    #calculate the waiting_time\n",
    "    time_df[column_name] = time_df[['arrival_time','departure_time']].apply(lambda x: int((datetime.strptime(x[later_time], FMT) - datetime.strptime(x[earlier_time], FMT)).total_seconds()/60), axis=1)\n",
    "    #if one day as past, take it into consideration\n",
    "    time_df[column_name] = time_df[column_name].apply(lambda x: day_in_seconds/60 + x if x < 0 else x)\n",
    "    return time_df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finds the routes that can be either extended from before or from after and those which are complete sequences'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Finds the routes that can be either extended from before or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the four indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences and index_of_unused_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    index_of_unused_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stops_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = get_extentions('after', True, route_sequences_route_id, trip)\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_before = get_extentions('before', True, route_sequences_route_id, trip)\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_before, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                #checks if it can only be extended after and not before\n",
    "                if possible_extentions_before.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                #check if the trip is not extendable, just because it is a full sequences and not a problem of matching with time\n",
    "                if (get_extentions('after', False, route_sequences_route_id, trip).empty) and (get_extentions('before', False, route_sequences_route_id, trip).empty):\n",
    "                    if route_id not in index_of_complete_sequences:\n",
    "                        index_of_complete_sequences[route_id] = []\n",
    "                    index_of_complete_sequences[route_id].append(index_trip)\n",
    "                #the trip does not match with the time of others but could have been extended\n",
    "                else:\n",
    "                    if route_id not in index_of_unused_sequences:\n",
    "                        index_of_unused_sequences[route_id] = []\n",
    "                    index_of_unused_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences, index_of_unused_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates all the sequences of routes possible to reconstruct the real route and calculates their frequency'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route and calculates their frequency'''\n",
    "\n",
    "def possible_sequences_construction(stops_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stops_sequences(stops_sequences_df, route_id)\n",
    "            #set default frequency to NaN\n",
    "            routes_with_route_id['frequency'] = np.nan\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stops_sequence', 'dates', 'departure_time_last','frequency']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stops_sequence', 'dates', 'departure_time_first', 'departure_time_last','frequency']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route \n",
    "                    #and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = get_extentions('after', True, route_creation_extensions_route_id, route_part)\n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not possible_extentions.empty:\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stops_sequence'] + possible_extention['stops_sequence'][1:]\n",
    "                            common_dates = possible_extention['dates'] & route_part['dates']\n",
    "                            new_departure_time_last = take_leftovers_list_c_from_intersection_AAndB(list(possible_extentions['departure_time_first'])[0], list(route_part['departure_time_last']), list(possible_extentions['departure_time_last'])[0])\n",
    "                            new_frequency = len(new_departure_time_last) * len(common_dates)\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_dates, new_departure_time_last, new_frequency]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    if 'departure_time_last' in route_creation.columns:\n",
    "        route_creation = route_creation.drop(['dates', 'departure_time_last'], axis=1)\n",
    "    route_creation = route_creation.reindex(columns=['route_id','hash','stops_sequence', 'frequency'])\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adds the full sequences to the route_creation dataframe'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stops_sequence', 'dates', 'departure_time_last']].copy()\n",
    "        copy_complete_sequences_df = calculate_frequency(copy_complete_sequences_df)\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id', 'frequency'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adds the sequences that were not yet added in the route_creation dataframe'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in route_creation['route_id'].unique():\n",
    "        #get a set of the hashes that were employed to create the routes for that route_id\n",
    "        used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "        copy_sequences_route_id = select_stops_sequences(stop_sequences_df, route_id)[['route_id','hash','stops_sequence', 'dates', 'departure_time_last']]\n",
    "        copy_sequences_route_id = calculate_frequency(copy_sequences_route_id)\n",
    "        #adds the hashes that were not employed in any route creations for that route_id\n",
    "        for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "            #first element of the list because there is always only one element\n",
    "            #the float conversion is for the hash to be written with e+18 because it is sometimes needed\n",
    "            if float(trip['hash'][0]) not in used_sequences_hash  and (trip['hash'][0] not in used_sequences_hash):\n",
    "                route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id', 'frequency'], ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates a column in the df that calculates the travel time between the first and last stop (waiting time included)\\nand another column with the waiting time (calculated with a weighted average based on the frequency)'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creates a column in the df that calculates the travel time between the first and last stop (waiting time included)\n",
    "and another column with the waiting time (calculated with a weighted average based on the frequency)'''\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "\n",
    "def give_begin_end_time(route_creation_frequency_single, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates):\n",
    "    #create a copy to not change the input DataFrame\n",
    "    route_creation_frequency_single = route_creation_frequency_single.copy()\n",
    "    #makes a column with the a representative begin time and end time of the route\n",
    "    route_creation_frequency_single['travel_time'] = np.nan\n",
    "    for index_sequence, sequence in route_creation_frequency_single.iterrows():\n",
    "        constructed_route = pd.DataFrame()\n",
    "        for index_hash, hash_value in enumerate(sequence['hash']):\n",
    "            index_plus_one = index_hash + 1\n",
    "            #take all the trips with that hash\n",
    "            next_representative_trips = trips_hash_stops_sequence[(trips_hash_stops_sequence['hash'] == hash_value) & (trips_hash_stops_sequence['route_id'] == sequence['route_id'])].copy()['trip_id']\n",
    "            #take all the stop sequences and their time that belongs \n",
    "            full_times = stops_cleaned_stop_times_trips_merge_dates[stops_cleaned_stop_times_trips_merge_dates['trip_id'].isin(next_representative_trips)].copy()\n",
    "            #select) only the last stop sequences of full_times for each trip_id\n",
    "            new_index_max_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmax()\n",
    "            max_per_trip_id = full_times.reset_index().loc[new_index_max_per_trip_id]\n",
    "            #select only the first stop sequences of full_times for each trip_id            \n",
    "            new_index_min_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmin()            \n",
    "            min_per_trip_id = full_times.reset_index().loc[new_index_min_per_trip_id]\n",
    "            #merge max_per_trip_id and min_per_trip_id\n",
    "            merged = min_per_trip_id[['trip_id', 'dates', 'departure_time']].merge(max_per_trip_id[['trip_id', 'arrival_time', 'departure_time']], on='trip_id')\n",
    "            #take all the stop sequences except the first one, and the last one if it is not the last sequence of the route\n",
    "            if index_hash == len(sequence['hash']) - 1:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(pd.concat([new_index_min_per_trip_id,new_index_max_per_trip_id]))\n",
    "            else:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(new_index_min_per_trip_id)            \n",
    "            #ONLY NEEDED FOR SWITZERLAND\n",
    "            rest_per_trip_id = rest_per_trip_id.dropna()\n",
    "            if not rest_per_trip_id.empty:\n",
    "                rest_per_trip_id = calculate_time_difference(rest_per_trip_id, 'departure_time', 'arrival_time', 'waiting_time')\n",
    "                #calculate the total waiting_time\n",
    "                rest_per_trip_id_grouped = rest_per_trip_id.groupby(['trip_id'], as_index=False)['waiting_time'].sum()\n",
    "                merged_waiting_time = merged.merge(rest_per_trip_id_grouped, on='trip_id')\n",
    "            #in case there are only two stops in for the hash\n",
    "            else:\n",
    "                merged_waiting_time = merged.copy()\n",
    "                merged_waiting_time['waiting_time'] = 0\n",
    "            #rename the columns     \n",
    "            merged_waiting_time = merged_waiting_time.rename(columns = {'trip_id': 'trip_id_' + str(index_plus_one),'departure_time_x':'departure_time_'+ str(index_plus_one), 'arrival_time':'arrival_time_'+ str(index_plus_one),\n",
    "                                          'departure_time_y':'departure_time_'+ str(index_plus_one + 1), 'waiting_time': 'waiting_time_' + str(index_plus_one)})\n",
    "            if index_hash == 0:\n",
    "                constructed_route = merged_waiting_time\n",
    "            elif index_hash > 0:\n",
    "                constructed_route = constructed_route.merge(merged_waiting_time, how='inner', on=['departure_time_' + str(index_plus_one)])\n",
    "                #take the intersection of the dates => only get the common dates and retain those rows with common dates\n",
    "                constructed_route['dates'] = [a & b for a,b in zip(constructed_route['dates_x'], constructed_route['dates_y'])]\n",
    "                constructed_route = constructed_route[constructed_route['dates'].map(lambda d: len(d)) > 0]\n",
    "                constructed_route = constructed_route.drop(['dates_x','dates_y'], axis=1)        \n",
    "        #make a list of all the columns of waiting_times\n",
    "        list_column_waiting_time = []\n",
    "        for i in range(1, index_plus_one + 1):\n",
    "            list_column_waiting_time.append('waiting_time_' + str(i))\n",
    "        #sum all the waiting times together for each route itinerary\n",
    "        constructed_route['waiting_time'] = constructed_route[list_column_waiting_time].astype(int).sum(1)\n",
    "        \n",
    "        #sometimes it is impossible to find trips that follow each other\n",
    "        if not constructed_route.empty:\n",
    "            #when the loop is finished, take the last arrival time, that will be used to calculate the travel time\n",
    "            time_constructed_route = constructed_route[['departure_time_1', 'arrival_time_' + str(index_plus_one), 'waiting_time', 'dates']]\n",
    "            time_constructed_route = time_constructed_route.rename(columns = {'departure_time_1':'departure_time', 'arrival_time_' + str(index_plus_one):'arrival_time'})\n",
    "            time_constructed_route = calculate_time_difference(time_constructed_route, 'arrival_time', 'departure_time', 'time_diff_min')\n",
    "            #add here a new column count dates that is the sum of the common dates\n",
    "            time_constructed_route['count_dates'] = time_constructed_route['dates'].apply(lambda x: len(x))\n",
    "            sum_count_dates = time_constructed_route['count_dates'].sum()\n",
    "            #take the first most frequent one\n",
    "            #create the weighted sum\n",
    "            time_constructed_route['WS_travel_time'] = (time_constructed_route['time_diff_min'] * time_constructed_route['count_dates'])/sum_count_dates\n",
    "            time_constructed_route['WS_waiting_time'] = (time_constructed_route['waiting_time'] * time_constructed_route['count_dates'])/sum_count_dates    \n",
    "            weighted_sum_tt = time_constructed_route['WS_travel_time'].sum()\n",
    "            weighted_sum_wt = time_constructed_route['WS_waiting_time'].sum()\n",
    "            #Add this to the first dataframe\n",
    "            route_creation_frequency_single.loc[index_sequence,'travel_time'] = weighted_sum_tt\n",
    "            route_creation_frequency_single.loc[index_sequence,'waiting_time'] = weighted_sum_wt\n",
    "        #if there is no trips that follow each other with the hash from the array\n",
    "        else:\n",
    "            route_creation_frequency_single = route_creation_frequency_single.drop(index_sequence)\n",
    "    route_creation_frequency_single = route_creation_frequency_single.sort_values(by=['route_id', 'frequency', 'travel_time'], ignore_index=True)    \n",
    "    return route_creation_frequency_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    route_creation_hash['hash'] = route_creation_hash['stops_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "    route_creation_hash['hash_inverse'] = route_creation_hash['stops_sequence'].apply(lambda x: hash(tuple(x[::-1])))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regroup the routes that are the same (even though they are in the opposite direction)'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stops_sequences(route_creation_hash):\n",
    "    '''regroups the stops_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_max_hash.copy().drop(['frequency', 'travel_time', 'waiting_time'], axis = 1)\n",
    "    #retains only one element per pair of route_id and hash\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "\n",
    "    #creates a df for the calculation of the weighted avg of travel_time and waiting_time for each max_hash\n",
    "    calculation_weighted_avg = pd.merge(route_creation_max_hash, route_creation_max_hash_freq, right_on=['route_id','max_hash'], left_on=['route_id','max_hash'])\n",
    "    calculation_weighted_avg = calculation_weighted_avg.rename(columns = {'frequency_y': 'sum_frequency', 'frequency_x':'frequency'})\n",
    "    calculation_weighted_avg['WS_travel_time'] = (calculation_weighted_avg['travel_time'] * calculation_weighted_avg['frequency'])/calculation_weighted_avg['sum_frequency']\n",
    "    calculation_weighted_avg['WS_waiting_time'] = (calculation_weighted_avg['waiting_time'] * calculation_weighted_avg['frequency'])/calculation_weighted_avg['sum_frequency']    \n",
    "    calculation_weighted_avg = calculation_weighted_avg.groupby(by=['route_id', 'max_hash'])[['WS_travel_time', 'WS_waiting_time']].sum()\n",
    "    calculation_weighted_avg = calculation_weighted_avg.rename(columns = {'WS_travel_time': 'travel_time', 'WS_waiting_time':'waiting_time'})\n",
    "    \n",
    "    #merge the weighted avg of travel_time and waiting_time with each combination of route_id and hash\n",
    "    route_hash_without_freq = pd.merge(route_hash_without_freq, calculation_weighted_avg, right_on=['route_id','max_hash'], left_on=['route_id','max_hash'] )\n",
    "    route_hash_without_freq = route_hash_without_freq.drop(['max_hash'], axis=1)\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stops_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stops_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id','frequency', 'travel_time'], ignore_index=True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #drop the routes with the same hash as others\n",
    "    route_hash_freq_treshold['max_hash'] = route_hash_freq_treshold[['hash', 'hash_inverse']].max(axis=1)\n",
    "    route_hash_freq_treshold = route_hash_freq_treshold.drop_duplicates(subset='max_hash')\n",
    "    route_hash_freq_treshold  = route_hash_freq_treshold.drop(['hash_inverse', 'max_hash'], axis = 1)\n",
    "    #selects the sequences that are not the first most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == sequences_max_freq_merged['max_frequency']].drop_duplicates(subset='route_id').index\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[~sequences_max_freq_merged.index.isin(sequences_max_freq_index)].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    if route_hash_freq_combined['route_id'].dtype == np.int64:\n",
    "        route_id_creation = route_hash_freq_combined['route_id'].max() + 1\n",
    "    else:\n",
    "        route_id_creation =  0 + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    #keep only the column route_id and stops_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == 0].index)\n",
    "    final_routes = final_routes.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To keep only the routes that have at least one country station in their route_sequence'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' To keep only the routes that have at least one country station in their route_sequence'''\n",
    "\n",
    "def keep_country_routes(final_routes, stops_country_series):\n",
    "    non_country_routes = set()\n",
    "    for index_route, route in final_routes.iterrows():\n",
    "        is_in_country = False\n",
    "        for stop in route['stops_sequence']:\n",
    "            if stop in set(stops_country_series):\n",
    "                is_in_country = True\n",
    "                break\n",
    "        if not is_in_country:\n",
    "            route_id = route['route_id']\n",
    "            non_country_routes.add(route_id)\n",
    "    country_routes = final_routes.loc[~final_routes['route_id'].isin(non_country_routes)] \n",
    "    if country_routes['route_id'].dtype == np.int64:\n",
    "        country_routes = country_routes.sort_values(by=['route_id'], ignore_index=True)\n",
    "\n",
    "    return country_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calculates the distances of the trip, by taking the distance between each stop of the stop_sequence'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Calculates the distances of the trip, by taking the distance between each stop of the stop_sequence'''\n",
    "\n",
    "def calculate_distance_from_lat_long(name_first, name_second, stop_df):\n",
    "        lon_first, lat_first = math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lat'])\n",
    "        lon_second, lat_second = math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lat'])\n",
    "        # The radius of the earth\n",
    "        R = 6373.0 \n",
    "        # To calculate the change in coordinates\n",
    "        dlon = lon_second - lon_first\n",
    "        dlat = lat_second - lat_first\n",
    "        # To use the Haversine formula to get the distance in kilometers between the starting_station and the ending_station\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat_first) * math.cos(lat_second) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        # To calculate the distance\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "def calculate_distance(stop_sequence, stop_df):\n",
    "    distance = 0\n",
    "    for index_stop ,stop in enumerate(stop_sequence):\n",
    "        index_plus_one = index_stop + 1\n",
    "        if index_plus_one <= len(stop_sequence) - 1:\n",
    "            distance += calculate_distance_from_lat_long(stop, stop_sequence[index_plus_one], stop_df)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makes a df that can be used for building the nodes and edges of the graph using Networkx package'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Makes a df that can be used for building the nodes and edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx space-of-stops graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stops_sequence_values = final_routes.apply(lambda x: pd.Series(x['stops_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stops_sequence_values.name = 'stops_sequence'\n",
    "    final_routes_stops = final_routes.drop('stops_sequence', axis=1).join(stops_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stops_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stops_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stops_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applies all the functions from 1 get_extention_indexes to 11 create_df_for_Networkx'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Applies all the functions from 1 get_extention_indexes to 11 create_df_for_Networkx'''\n",
    "\n",
    "def full_route_creation(stops_sequences_df, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned, stops_country_series):\n",
    "    '''return a df that can be used to make a Networkx space-of-stops (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences, index_of_unsued_sequences = get_extention_indexes(stops_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stops_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stops_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stops_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single_travel_time = give_begin_end_time(route_creation_third, trips_hash_stops_sequence, stops_cleaned_stop_times_trips_merge_dates)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single_travel_time)\n",
    "    route_hash_freq_combined = regroup_same_stops_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    country_routes = keep_country_routes(final_routes, stops_country_series)\n",
    "    country_routes['distance'] = country_routes['stops_sequence'].apply(lambda x: calculate_distance(x, stops_cleaned))\n",
    "    df_for_edges = create_df_for_Networkx(country_routes)\n",
    "    \n",
    "    return country_routes, df_for_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal route creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE ROUTE CREATIONS, BUT IT TAKES A LONG TIME\n",
    "belgian_routes, df_for_edges_Belgium = full_route_creation(routes_hash_Belgium, generic_trips_information_Belgium, trips_hash_Belgium, stops_cleaned_Belgium, stops_Belgium_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Save those two last DataFrames as .csv files'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Save those two last DataFrames as .csv files'''\n",
    "#belgian_routes.reset_index(drop=True).to_csv(f'{routes_loc}belgian_routes_Belgium.csv')\n",
    "#df_for_edges_Belgium.reset_index(drop=True).to_csv(f'{df_for_edges_loc}df_for_edges_Belgium.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#belgian_routes = pd.read_csv(routes_loc + 'belgian_routes_Belgium.csv', sep=\",\")\n",
    "#df_for_edges_Belgium = pd.read_csv( df_for_edges_loc + 'df_for_edges_Belgium.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE ROUTE CREATIONS, BUT IT TAKES A LONG TIME\n",
    "#dutch_routes, df_for_edges_Netherlands = full_route_creation(routes_hash_Netherlands, generic_trips_information_Netherlands, trips_hash_Netherlands, stops_cleaned_Netherlands, stops_Netherlands_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Save those two last DataFrames as .csv files'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Save those two last DataFrames as .csv files'''\n",
    "#dutch_routes.reset_index(drop=True).to_csv(f'{routes_loc}dutch_routes_Netherlands.csv')\n",
    "#df_for_edges_Netherlands.reset_index(drop=True).to_csv(f'{df_for_edges_loc}df_for_edges_Netherlands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_routes = pd.read_csv(routes_loc + 'dutch_routes_Netherlands.csv', sep=\",\")\n",
    "df_for_edges_Netherlands = pd.read_csv(df_for_edges_loc + 'df_for_edges_Netherlands.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE FOLLOWING LINE MIGHT BE RUNNED IF WANTED TO RECALCULATE THE ROUTE CREATIONS, BUT IT TAKES A LONG TIME\n",
    "#swiss_routes, df_for_edges_Switzerland = full_route_creation(routes_hash_Switzerland, generic_trips_information_Switzerland, trips_hash_Switzerland, stops_cleaned_Switzerland, stops_Switzerland_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Save those two last DataFrames as .csv files'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Save those two last DataFrames as .csv files'''\n",
    "#swiss_routes.reset_index(drop=True).to_csv(f'{routes_loc}swiss_routes_Switzerland.csv')\n",
    "#df_for_edges_Switzerland.reset_index(drop=True).to_csv(f'{df_for_edges_loc}df_for_edges_Switzerland.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swiss_routes = pd.read_csv(routes_loc + 'swiss_routes_Switzerland.csv', sep=\",\")\n",
    "#df_for_edges_Switzerland = pd.read_csv(df_for_edges_loc + 'df_for_edges_Switzerland.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_extendable, index_of_begin_sequences, index_of_complete_sequences, index_of_unsued_sequences = get_extention_indexes(stops_sequences_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
