{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3892,
     "status": "ok",
     "timestamp": 1616073508385,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "FdWaEGPXXxUD",
    "outputId": "ac18259e-0c88-444a-bf1d-cfd6df365a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (from geopy) (1.50)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4656,
     "status": "ok",
     "timestamp": 1616073509205,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "4wVjHDm2CRQc"
   },
   "outputs": [],
   "source": [
    "'''To import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqdIKKTU8sTj"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4659,
     "status": "ok",
     "timestamp": 1616073509253,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "qBMlbdFH8tbl"
   },
   "outputs": [],
   "source": [
    "'''To display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 4640,
     "status": "ok",
     "timestamp": 1616073509262,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "yx6ev1dJ8uFy",
    "outputId": "4d2a5e36-710d-424c-e058-9eb9e97ed843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To ensure that the output results of extensive output results are not truncated.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBJzOAhgX4ss"
   },
   "source": [
    "# Import of the Dutch railway datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 4645,
     "status": "ok",
     "timestamp": 1616073509287,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "fV4GmqXeZBs8",
    "outputId": "61035f3f-1995-4a57-cd11-f3b1112618a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To register the GitHub link with the Dutch data as a variable.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To register the GitHub link with the Dutch data as a variable.'''\n",
    "datalink = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/Pol/gtfs_train_Netherlands_1503/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 10160,
     "status": "ok",
     "timestamp": 1616073514822,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "DhWzAtwEXzMg",
    "outputId": "5f517e2d-c84d-4216-aad5-b8ccfa451d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import all the GTFS data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-463c08c09356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#To import the agency dataset that contains limited information about the Dutch NS railway agency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magency_Netherlands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatalink\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"agency.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the Dutch NS railway stations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstops_Netherlands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatalink\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"stops.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# though mypy handling of conditional imports is difficult.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: fsspec can also handle HTTP via requests, but leaving this unchanged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "'''Import all the GTFS data'''\n",
    "\n",
    "#To import the agency dataset that contains limited information about the Dutch NS railway agency.\n",
    "agency_Netherlands = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "#To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the Dutch NS railway stations.\n",
    "stops_Netherlands = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "#To import the feed_info dataset that contains limited information about the Dutch NS railway feed.\n",
    "feed_info_Netherlands = pd.read_csv(datalink + \"feed_info.txt\", sep=\",\")\n",
    "#To import the transfers dataset that gives the minimum transfer time to switch routes at each Belgian railway station.\n",
    "transfers_not_cleaned_Netherlands = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "#To import the routes dataset that provides the id, the name and the type of vehicle used for all Dutch NS railway routes.\n",
    "routes_Netherlands = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "#To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the Dutch NS railway route.\n",
    "#The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "trips_Netherlands = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "#To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "calendar_dates_Netherlands = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 81498,
     "status": "ok",
     "timestamp": 1616073586185,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "_741TlIWZZoX",
    "outputId": "eee7b1eb-bbd5-4aa1-fda7-f6d9e66176af"
   },
   "outputs": [],
   "source": [
    "'''Import stop_times that is is of multiple csv files'''\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_range = [*range(2, 19)]\n",
    "stop_times_Netherlands = pd.read_csv(datalink + \"stop_times-1.csv\", sep=\",\")\n",
    "for index in stop_times_range:\n",
    "    stop_times_Netherlands = pd.concat([stop_times_Netherlands, pd.read_csv(datalink + \"stop_times-\" + str(index)+ \".csv\", sep=\",\")])\n",
    "stop_times_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 88685,
     "status": "ok",
     "timestamp": 1616073593395,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "dx5QvhqXcHw-",
    "outputId": "358eac75-d456-4837-f34d-bff7f89882c6"
   },
   "outputs": [],
   "source": [
    "'''Import shapes that consists of multiple csv files'''\n",
    "# ???\n",
    "shapes_range = [*range(2, 4)]\n",
    "shapes_Netherlands = pd.read_csv(datalink + \"shapes-1.csv\", sep=\",\")\n",
    "for index in shapes_range:\n",
    "    shapes_Netherlands = pd.concat([shapes_Netherlands, pd.read_csv(datalink + \"shapes-\" + str(index)+ \".csv\", sep=\",\")])\n",
    "shapes_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4y5ul9hdC1E"
   },
   "source": [
    "# Cleaning of the Dutch railway data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 88679,
     "status": "ok",
     "timestamp": 1616073593398,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "JiQ0GHsPdF9J"
   },
   "outputs": [],
   "source": [
    "# To define a definition to remove the accents from a string\n",
    "def remove_accents(text):\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 88654,
     "status": "ok",
     "timestamp": 1616073593400,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "z716A6eWePgC",
    "outputId": "1a9a056a-11ab-4fde-9237-80d66b86601e"
   },
   "outputs": [],
   "source": [
    "''' To clean the routes_Netherlands df.'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Netherlands = routes_Netherlands[routes_Netherlands['route_type'] == 2]\n",
    "\n",
    "# To remove the accents from the route_long_name and to change to uppercase\n",
    "routes_cleaned_Netherlands.loc[:,'route_long_name'] = routes_cleaned_Netherlands.loc[:,'route_long_name'].apply(remove_accents)\n",
    "routes_cleaned_Netherlands.loc[:,'route_long_name'] = routes_cleaned_Netherlands.loc[:,'route_long_name'].str.upper()\n",
    "routes_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "executionInfo": {
     "elapsed": 88844,
     "status": "ok",
     "timestamp": 1616073593609,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "paayjLIceRbW",
    "outputId": "aaf36e05-0316-435f-be69-55afeec95522"
   },
   "outputs": [],
   "source": [
    "''' To clean the trips_Netherlands df.'''\n",
    "# To remove the routes that are not train routes\n",
    "no_route_id_train_routes = routes_Netherlands.loc[routes_Netherlands['route_type'] != 2, 'route_id']\n",
    "trips_cleaned_Netherlands = trips_Netherlands[(~trips_Netherlands['route_id'].isin(no_route_id_train_routes))]\n",
    "\n",
    "# To remove the accents from the trip_headsign and to change to uppercase\n",
    "trips_cleaned_Netherlands.loc[:,'trip_headsign'] = trips_cleaned_Netherlands.loc[:,'trip_headsign'].apply(remove_accents)\n",
    "trips_cleaned_Netherlands.loc[:,'trip_headsign'] = trips_cleaned_Netherlands.loc[:,'trip_headsign'].str.upper()\n",
    "trips_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "executionInfo": {
     "elapsed": 90796,
     "status": "ok",
     "timestamp": 1616073595583,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "mjdjoSPieVGn",
    "outputId": "9b4ff8e8-f5b8-418c-8ce4-a1a3fc6b44f2"
   },
   "outputs": [],
   "source": [
    "''' To clean the stop_times_Netherlands df.'''\n",
    "# To remove the stop_times trip_ids that are not trip_ids that belong to train routes and \n",
    "# to change the data type of the stop_id column\n",
    "no_trip_id_train_routes = trips_Netherlands.loc[trips_Netherlands['route_id'].isin(no_route_id_train_routes), 'trip_id']\n",
    "stop_times_cleaned_Netherlands = stop_times_Netherlands[(~stop_times_Netherlands['trip_id'].isin(no_trip_id_train_routes))]\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_id'] = stop_times_cleaned_Netherlands.stop_id.apply(str)\n",
    "\n",
    "# To take a subset of the stops_Netherlands df and to remove the accents from the stop_name and to change the stop_name to uppercase\n",
    "stop_id_name_Netherlands = stops_Netherlands[['stop_id', 'stop_name']]\n",
    "stop_id_name_Netherlands.loc[:,'stop_name'] = stop_id_name_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stop_id_name_Netherlands.loc[:,'stop_name'] = stop_id_name_Netherlands.loc[:,'stop_name'].str.upper()\n",
    "\n",
    "# To add the stop_name attribute of the stop_times_stop_id_name_Netherlands df to the stop_times_cleaned_Netherlands df and\n",
    "# to remove the stop_id attribute\n",
    "stop_times_cleaned_Netherlands = pd.merge(stop_times_cleaned_Netherlands, stop_id_name_Netherlands, on='stop_id', how='left')\n",
    "stop_times_cleaned_Netherlands.drop('stop_id', axis=1, inplace=True)\n",
    "stop_times_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 91022,
     "status": "ok",
     "timestamp": 1616073595828,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "LPyNzczxeWEX",
    "outputId": "138f3c00-e515-4958-ac58-2da01da39591"
   },
   "source": [
    "''' To clean the stops_Netherlands df.  (1) ''' \n",
    "##### To take all unique stop_names that appear in the stop_times df\n",
    "unique_stop_names_stop_times_Netherlands = stop_times_cleaned_Netherlands['stop_name'].drop_duplicates()\n",
    "\n",
    "##### To select all rows of the stops_Netherlands df that contain a stop_name that is in unique_stop_names_stop_times_Netherlands\n",
    "stops_initial_Netherlands = stops_Netherlands.copy()\n",
    "stops_initial_Netherlands['stop_name'] = stops_Netherlands['stop_name'].apply(remove_accents)\n",
    "stops_initial_Netherlands['stop_name'] = stops_initial_Netherlands['stop_name'].str.upper()\n",
    "stops_initial_Netherlands = stops_initial_Netherlands[stops_initial_Netherlands['stop_name'].isin(unique_stop_names_stop_times_Netherlands)]\n",
    "\n",
    "##### To take from the stops_initial_Netherlands df all stop_ids that contain a 'stoparea:' to get the correct stop coordinates\n",
    "stops_cleaned_Netherlands = stops_initial_Netherlands[stops_initial_Netherlands['stop_id'].str.contains('stoparea:')]\n",
    "stops_cleaned_Netherlands = stops_cleaned_Netherlands.drop_duplicates()\n",
    "\n",
    "##### To verify that there is an equal number of unique stop_names in the unique_stop_names_stop_times_Netherlands series and the stops_cleaned_Netherlands df\n",
    "stop_names_stops_cleaned_Netherlands = stops_cleaned_Netherlands[['stop_name']].drop_duplicates()\n",
    "len(unique_stop_names_stop_times_Netherlands)\n",
    "len(stop_names_stops_cleaned_Netherlands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "executionInfo": {
     "elapsed": 359308,
     "status": "ok",
     "timestamp": 1616073864135,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Pq4qH9eIeYFF",
    "outputId": "791fedd4-c898-42d1-e992-baeba057b797"
   },
   "source": [
    "''' To clean the stops_Netherlands df.  (2) ''' \n",
    "###### To initialize the Nominatim API to get the location from the input string \n",
    "geolocator = Nominatim(user_agent=\"application\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "##### To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "country_list = []\n",
    "for index, row in stops_cleaned_Netherlands.iterrows():\n",
    "    latitude = row['stop_lat']\n",
    "    longitude = row['stop_lon']\n",
    "    # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "    location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "    # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "    address = location.raw['address']\n",
    "    country = address.get('country', '')\n",
    "    country_list.append(country)\n",
    "\n",
    "##### To add the values of country_list as a new attribute country     \n",
    "stops_cleaned_Netherlands.loc[:,'country'] = country_list\n",
    "stops_cleaned_Netherlands\n",
    "\n",
    "###### To calculate the total number of Belgian stations in the stops_cleaned dataset\n",
    "dutch_stops_Netherlands = stops_cleaned_Netherlands[stops_cleaned_Netherlands['country'] == 'Netherlands']\n",
    "dutch_stops_Netherlands_series = stops_cleaned_Netherlands.loc[stops_cleaned_Netherlands['country'] == 'Netherlands', 'stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stops_cleaned_Netherlands.to_csv(r'/Users/pol/Desktop/CSV_export/stops_cleaned_Netherlands.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dutch_stops_Netherlands_series.to_csv(r'/Users/pol/Desktop/CSV_export/dutch_stops_Netherlands_series.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import the cleaned version of the stops with their country'''\n",
    "stops_cleaned_Netherlands = pd.read_csv(\"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/Pol/stops_cleaned/stops_cleaned_Netherlands.csv\", sep=\",\")\n",
    "dutch_stops_Netherlands_series = pd.read_csv(\"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/Pol/country_stops_series/dutch_stops_Netherlands_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbAxq5n7ec32"
   },
   "source": [
    "# Exploratory data analysis with the Dutch railway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 359301,
     "status": "ok",
     "timestamp": 1616073864147,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "ECwpEL4aedwk",
    "outputId": "92aab655-edf4-4053-e57a-668eb553280b"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids '''\n",
    "set_routes_Netherlands = {r for r in routes_cleaned_Netherlands['route_id']}\n",
    "len(set_routes_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 359289,
     "status": "ok",
     "timestamp": 1616073864155,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "UtA2_DM6efhd",
    "outputId": "d6647f66-73fa-4ef5-8e1d-5bca49639d17"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of stations in the stops_cleaned_Netherlands dataset'''\n",
    "set_stations_Netherlands = {s for s in stops_cleaned_Netherlands['stop_id']}\n",
    "len(set_stations_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 359272,
     "status": "ok",
     "timestamp": 1616073864158,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "1uvasY6Heg-f",
    "outputId": "b34744e2-c28e-4831-8bc8-b39bba27e827"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of Dutch stations in the stops_cleaned dataset'''\n",
    "len(dutch_stops_Netherlands_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oat0T2iAeibZ"
   },
   "source": [
    "# **Preparation for the L-space representation of the Dutch railway system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 359257,
     "status": "ok",
     "timestamp": 1616073864161,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "TbNSQeBaesjc",
    "outputId": "8706c065-b681-4c4d-d1b6-1b4e8fdf22a7"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the trips dataset and a selection of the routes dataset on route_id'''\n",
    "trips_routes_Netherlands = pd.merge(trips_cleaned_Netherlands[['route_id','service_id','trip_id', 'trip_headsign']], routes_cleaned_Netherlands[['route_id', 'route_short_name', 'route_long_name']], on='route_id')\n",
    "trips_routes_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 359322,
     "status": "ok",
     "timestamp": 1616073864242,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "U804DAlvevK5",
    "outputId": "dcb6a4f3-2c3b-4cf9-de53-bbcb25029a8f"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stop_times_cleaned_Netherlands dataset with a selection of the stops_cleaned_Netherlands dataset'''\n",
    "stop_times_stops_Netherlands = pd.merge(stop_times_cleaned_Netherlands[['trip_id','arrival_time', 'departure_time','stop_name','stop_sequence']], stops_cleaned_Netherlands[['stop_name', 'stop_lat', 'stop_lon']], on='stop_name')\n",
    "stop_times_stops_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 359306,
     "status": "ok",
     "timestamp": 1616073864244,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "d1ZZxUR4ew1k",
    "outputId": "4bab9047-5400-4030-b636-a9c88d1457fc"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stop_times_stops_Netherlands dataset with the trips_routes_Netherlands dataset.'''\n",
    "trips_routes_stop_times_stops_Netherlands = pd.merge(trips_routes_Netherlands, stop_times_stops_Netherlands, on='trip_id')\n",
    "trips_routes_stop_times_stops_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 359915,
     "status": "ok",
     "timestamp": 1616073864868,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Fafh2Wcfeynm",
    "outputId": "e08b5ba2-0e82-4236-e020-06ee992219b3"
   },
   "outputs": [],
   "source": [
    "'''To create a route_sequence dataset that gives for each trip_id that belongs to a route the sequence of stations served'''\n",
    "route_sequence_Netherlands = trips_routes_stop_times_stops_Netherlands.groupby(['route_id','route_long_name','trip_headsign','trip_id','stop_sequence'], as_index=False)[['stop_name', 'stop_lat', 'stop_lon']].last()\n",
    "route_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5dsj88KZoVN"
   },
   "source": [
    "'''To calculate the hash and the hash_inverse values for the stop sequence of each trip_id'''\n",
    "\n",
    "#To copy the trips_routes_Netherlands df\n",
    "trips_hash_Netherlands = trips_routes_Netherlands.copy()\n",
    "\n",
    "#To create a column called hash that contains NaN values\n",
    "trips_hash_Netherlands['hash'] = np.nan\n",
    "\n",
    "#To create a column called hash_inverse that contains NaN values\n",
    "trips_hash_Netherlands['hash_inverse'] = np.nan\n",
    "\n",
    "#For each trip_id in trips_routes_Netherlands, the stop_sequence that gets calculated is the subset of the stop_times dataset for that trip_id. \n",
    "#The tuple that results from the stop_name column of this subset dataset contains all the stop_names that get served by this trip_id. \n",
    "\n",
    "#The hash value of the tuple of the stop_name column is calculated and is placed in the hash column of the trip_id in the trips_routes dataset\n",
    "#The inverse_hash value of the tuple of the stop_name column is calculated as well.\n",
    "\n",
    "for trip_Netherlands in trips_routes_Netherlands['trip_id'].unique():\n",
    "    stop_sequence_Netherlands = stop_times_cleaned_Netherlands[stop_times_cleaned_Netherlands['trip_id'] == trip_Netherlands].sort_values(by = 'stop_sequence')\n",
    "    trips_hash_Netherlands.loc[trips_hash_Netherlands['trip_id'] == trip_Netherlands, 'hash'] = hash(tuple(stop_sequence_Netherlands['stop_name']))\n",
    "    trips_hash_Netherlands.loc[trips_hash_Netherlands['trip_id'] == trip_Netherlands, 'hash_inverse'] = hash(tuple(list(stop_sequence_Netherlands['stop_name'])[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 359917,
     "status": "ok",
     "timestamp": 1616073864874,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "rKoJWvg6e3M3"
   },
   "outputs": [],
   "source": [
    "#trips_hash_Netherlands.to_csv(r'/Users/pol/Desktop/CSV_export/trips_hash_Netherlands.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 360849,
     "status": "ok",
     "timestamp": 1616073865818,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "BBwsA35KZhML",
    "outputId": "ea20d413-5325-4293-c192-93074189fe95"
   },
   "outputs": [],
   "source": [
    "datalink = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main\"\n",
    "trips_hash_Netherlands = pd.read_csv(datalink + \"/hash_cleaning/trips_hash_Netherlands.csv\", sep=\",\")\n",
    "trips_hash_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "executionInfo": {
     "elapsed": 405230,
     "status": "ok",
     "timestamp": 1616073910211,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "lFw1YlkyfH6a",
    "outputId": "579ca062-8ca4-4227-9151-ead642ccc16f"
   },
   "outputs": [],
   "source": [
    "''' To groupby the trip_id and to order the stop_sequence in an ascending order (the stop_sequences of some\n",
    "routes are initially in descending order while other stop_sequences are in ascending order) '''\n",
    "\n",
    "trips_stop_sequence_ascending_Netherlands = stop_times_stops_Netherlands.groupby(['trip_id'], as_index=False).apply(lambda x: x.sort_values('stop_sequence'))\n",
    "trips_stop_sequence_ascending_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 407003,
     "status": "ok",
     "timestamp": 1616073911997,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "0hfmQ0J6fPPV",
    "outputId": "da655042-77b7-4919-ffd1-a2681eec946e"
   },
   "outputs": [],
   "source": [
    "''' To put the stop_names of a stop sequence of a trip_id in a list '''\n",
    "trips_stop_sequence_Netherlands = trips_stop_sequence_ascending_Netherlands.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "trips_stop_sequence_Netherlands.rename(columns={'stop_name':'stop_sequence'}, inplace=True)\n",
    "trips_stop_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 407002,
     "status": "ok",
     "timestamp": 1616073912009,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "NL34w0R6fTB-",
    "outputId": "41af0da0-dee3-458c-ec18-bef6b7264a92"
   },
   "outputs": [],
   "source": [
    "''' To add the list of stop_sequence of stations to the trips_hash_Netherlands df by joining on trip_id'''\n",
    "# To add the stop_sequence of stations to the trips_hash_France df by joining on trip_id\n",
    "trips_hash_stop_sequence_Netherlands = pd.merge(trips_hash_Netherlands, trips_stop_sequence_Netherlands, on='trip_id', how='left')\n",
    "\n",
    "# To put the columns in a more logical order\n",
    "trips_hash_stop_sequence_Netherlands = trips_hash_stop_sequence_Netherlands[['route_id', 'route_long_name','service_id','trip_headsign','trip_id','hash', 'hash_inverse','stop_sequence']]\n",
    "trips_hash_stop_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 406996,
     "status": "ok",
     "timestamp": 1616073912016,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "k9aO7ojufY8V",
    "outputId": "5209f6d8-1660-4951-e686-b87e68bcd0f7"
   },
   "outputs": [],
   "source": [
    "''' To count the number of dates for each service_id '''\n",
    "service_id_df_Netherlands = calendar_dates_Netherlands.groupby(['service_id'])[['service_id']].count().rename(columns={'service_id':'count_service_id'}).reset_index()\n",
    "service_id_df_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 407000,
     "status": "ok",
     "timestamp": 1616073912033,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "u8z2j6SxfZbT",
    "outputId": "c38e586e-f2c6-4aed-de83-d75b90b787ac"
   },
   "outputs": [],
   "source": [
    "''' To regroup the days per service_id in a set '''\n",
    "service_id_dates_Netherlands = calendar_dates_Netherlands.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "service_id_dates_Netherlands.rename(columns={'date':'dates'}, inplace=True)\n",
    "service_id_dates_Netherlands = service_id_dates_Netherlands.merge(service_id_df_Netherlands, on='service_id', how='left')\n",
    "service_id_dates_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 407244,
     "status": "ok",
     "timestamp": 1616073912289,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "xmOiY_jffbMp",
    "outputId": "dddab327-9058-47f8-c4b2-da8891afebdb"
   },
   "outputs": [],
   "source": [
    "''' To put the different trip_ids in a list after joining on (route_id, route_long_name, hash and service_id) '''\n",
    "route_hash_freq_Netherlands = trips_hash_stop_sequence_Netherlands.groupby(['route_id','route_long_name','hash', 'hash_inverse', 'service_id'])['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 407234,
     "status": "ok",
     "timestamp": 1616073912292,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "ouSygWbZfd6M",
    "outputId": "68612098-9853-441f-fa2a-47d7cf2f540e"
   },
   "outputs": [],
   "source": [
    "''' To add the sequence of stops to the route_hash_freq dataset '''\n",
    "route_hash_freq_Netherlands = pd.merge(route_hash_freq_Netherlands, trips_hash_stop_sequence_Netherlands[['route_id','hash', 'hash_inverse', 'service_id','stop_sequence']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "route_hash_freq_Netherlands = route_hash_freq_Netherlands.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 407224,
     "status": "ok",
     "timestamp": 1616073912296,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "7aSQ6CZVfh14",
    "outputId": "3e75c77e-6d19-4918-f9d2-010eb97da82b"
   },
   "outputs": [],
   "source": [
    "''' To calculate the number of trip_ids in the list of trip_ids and to add it as a new column '''\n",
    "number_trip_ids_Netherlands = []\n",
    "for list_trip_ids_Netherlands in route_hash_freq_Netherlands['trip_id']:\n",
    "    count_Netherlands = len(list_trip_ids_Netherlands)\n",
    "    number_trip_ids_Netherlands.append(count_Netherlands)\n",
    "route_hash_freq_Netherlands['number_trip_ids'] = number_trip_ids_Netherlands\n",
    "\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "executionInfo": {
     "elapsed": 407214,
     "status": "ok",
     "timestamp": 1616073912298,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "j66hcLzAfkMB",
    "outputId": "776daf0b-08c6-46bc-edae-64a8bcb05b61"
   },
   "outputs": [],
   "source": [
    "''' To merge the route_hash_freq_Netherlands df with the service_id_dates to get the sets of corresponding dates '''\n",
    "route_hash_service_freq_Netherlands = pd.merge(route_hash_freq_Netherlands, service_id_dates_Netherlands, on='service_id', how='left')\n",
    "route_hash_service_freq_Netherlands_copy = route_hash_service_freq_Netherlands.copy()\n",
    "route_hash_service_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "executionInfo": {
     "elapsed": 410960,
     "status": "ok",
     "timestamp": 1616073916057,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "qZnfLSQ5fsdZ",
    "outputId": "beddeb56-02ec-4f25-a426-d928e7304ab1"
   },
   "outputs": [],
   "source": [
    "'''Groups the service_id together for each route_id and hash combination'''\n",
    "for index, combi_route_id_hash in route_hash_service_freq_Netherlands_copy.groupby(['route_id','hash'], as_index = False)['service_id'].last().iterrows():\n",
    "    set_service_id = set(route_hash_service_freq_Netherlands_copy.loc[(route_hash_service_freq_Netherlands_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_Netherlands_copy['hash'] == combi_route_id_hash['hash'])]['service_id'])\n",
    "    route_hash_service_freq_Netherlands_copy.loc[(route_hash_service_freq_Netherlands_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_Netherlands_copy['hash'] == combi_route_id_hash['hash']),['service_id']] = set_service_id\n",
    "route_hash_service_freq_Netherlands_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 410957,
     "status": "ok",
     "timestamp": 1616073916067,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "qbh_G6m4fx49",
    "outputId": "8eb407b7-4dd9-4775-956e-de1aca81c78a"
   },
   "outputs": [],
   "source": [
    "'''Get the distinct stop sequences for all routes to create the possible track combinations later on'''\n",
    "distinct_stop_sequences_Netherlands = route_hash_service_freq_Netherlands_copy.drop_duplicates(subset = [\"route_id\", 'hash'])[['route_id','hash','stop_sequence', 'service_id']]\n",
    "distinct_stop_sequences_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTMnP489f7zV"
   },
   "source": [
    "##Functions for the route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 410949,
     "status": "ok",
     "timestamp": 1616073916072,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "phYn4GZgf1Hn",
    "outputId": "b2657c54-9a0a-4bd4-fca5-fd69da5787e7"
   },
   "outputs": [],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stop_sequences(stop_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stop_sequences_df[stop_sequences_df['route_id'] == route_id].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 410944,
     "status": "ok",
     "timestamp": 1616073916079,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "6FJKfWAZgA5M",
    "outputId": "c58e485f-2383-453b-8619-e842ef68ffa7"
   },
   "outputs": [],
   "source": [
    "'''Finds the routes that can be either extended from behind or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the tree indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stop_sequence']))))].copy()\n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_after = possible_extentions_after[possible_extentions_after['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_behind = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stop_sequence']))))].copy()        \n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_behind = possible_extentions_behind[possible_extentions_behind['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_behind, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                if possible_extentions_behind.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                if route_id not in index_of_complete_sequences:\n",
    "                    index_of_complete_sequences[route_id] = []\n",
    "                index_of_complete_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411136,
     "status": "ok",
     "timestamp": 1616073916283,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Fc5LeNFqgC86",
    "outputId": "f148a063-6d23-4415-ebac-fb8bfdd8dcc0"
   },
   "outputs": [],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route'''\n",
    "\n",
    "def possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stop_sequence', 'service_id']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stop_sequence','service_id']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = route_creation_extensions_route_id[route_creation_extensions_route_id['stop_sequence'].apply(lambda x: any(item for item in [route_part['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(route_part['stop_sequence']))))].copy()\n",
    "                    #take only those extentions that have a common service_id with the route_part\n",
    "                    possible_extentions = possible_extentions[possible_extentions['service_id'].apply(lambda x: any(item for item in route_part['service_id'] if item in x))].copy()                \n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not (possible_extentions.empty):\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stop_sequence'] + possible_extention['stop_sequence'][1:]\n",
    "                            common_service_id = possible_extention['service_id'] & route_part['service_id']\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_service_id]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411123,
     "status": "ok",
     "timestamp": 1616073916286,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "S3mFGPd_gG1F",
    "outputId": "b39378e3-0094-4268-de01-651a4bf078bd"
   },
   "outputs": [],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stop_sequence', 'service_id']].copy()\n",
    "        copy_complete_sequences_df['hash'] = copy_complete_sequences_df['hash'].apply(lambda x: [x])\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411113,
     "status": "ok",
     "timestamp": 1616073916288,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "rGOYu0QagHqW",
    "outputId": "7d0abe46-4bd8-452d-8fed-3269146f2a94"
   },
   "outputs": [],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        if route_id in route_creation['route_id'].unique():\n",
    "            #get a set of the hashes that were employed to create the routes for that route_id\n",
    "            used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "            #get a tuple of all the route sequences for that route_id\n",
    "            used_sequences = tuple(route_creation[route_creation['route_id'] == route_id]['stop_sequence'])\n",
    "            copy_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)[['route_id','hash','stop_sequence', 'service_id']]\n",
    "            copy_sequences_route_id['hash'] = copy_sequences_route_id['hash'].apply(lambda x: [x]) \n",
    "            #adds the hashes that were not employed in any route creations for that route_id\n",
    "            for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "                #first element of the list because there is always only one element\n",
    "                if trip['hash'][0] not in used_sequences_hash:\n",
    "                    #checks that the sequence is not a sublist of any existing sequences\n",
    "                    is_subsequence = False\n",
    "                    for sequence in used_sequences:\n",
    "                        if set(trip['stop_sequence']).issubset(sequence):\n",
    "                            is_subsequence = True\n",
    "                    if not is_subsequence:\n",
    "                        route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411105,
     "status": "ok",
     "timestamp": 1616073916292,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "xZ6P8cvzgJfx",
    "outputId": "da906b5e-a5c1-45a5-8a5b-7389397856fc"
   },
   "outputs": [],
   "source": [
    "'''Calculates the frequency of the constructed routes just made in the route_creation dataframe'''\n",
    "    \n",
    "def calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation):\n",
    "    '''calculates the frequencies of route_construction_third'''\n",
    "    #put the default value of the frequency to 0\n",
    "    route_creation['frequency'] = 0\n",
    "    for index_sequence, sequence in route_creation[['route_id','hash','service_id']].iterrows():\n",
    "        #initialize the varibles\n",
    "        sequence_frequency = 0\n",
    "        set_common_service_id = sequence['service_id']\n",
    "        if set_common_service_id:\n",
    "            #select the number_of_trips_per_hash only for the considered route_id\n",
    "            number_of_trips_per_hash_route_id = number_of_trips_per_hash[number_of_trips_per_hash['route_id'] == sequence['route_id']]\n",
    "            #only select the trips with the hash value contained in the sequence and with the same route_id\n",
    "            containing_hash = number_of_trips_per_hash_route_id[number_of_trips_per_hash_route_id['hash'].apply(lambda x: any(item for item in sequence['hash'] if x == item))]\n",
    "            #loop over each service_id that were common during the trip\n",
    "            for service_id in set_common_service_id:\n",
    "                service_id_number_days = service_id_count_dates[service_id_count_dates['service_id'] == service_id].iloc[0]['count_service_id']\n",
    "                #adds the minimum number of trips per day multiplied by the number of days in the service_id\n",
    "                sequence_frequency += containing_hash[containing_hash['service_id'] == service_id]['number_trip_ids'].min() * service_id_number_days\n",
    "            #adds the frequency in of the new route sequence\n",
    "            route_creation.loc[index_sequence, 'frequency'] = sequence_frequency\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 411103,
     "status": "ok",
     "timestamp": 1616073916294,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "14IOuVrvgLiL"
   },
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #create a column called hash and hash_invese that contains NaN values\n",
    "    route_creation_hash['hash'] = np.nan\n",
    "    route_creation_hash['hash_inverse'] = np.nan\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    for index, route_sequence in route_creation_hash.iterrows():\n",
    "        route_creation_hash.loc[index, 'hash'] = hash(tuple(route_sequence['stop_sequence']))\n",
    "        route_creation_hash.loc[index, 'hash_inverse'] = hash(tuple(list(route_sequence['stop_sequence'])[::-1]))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411093,
     "status": "ok",
     "timestamp": 1616073916295,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "VmdGOOgmgNd4",
    "outputId": "9e57d2d5-1d52-4df5-e2e5-38aa8e2a9a82"
   },
   "outputs": [],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stop_sequences(route_creation_hash):\n",
    "    '''regroups the stop_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_hash.copy().drop(['frequency'], axis = 1)\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    route_hash_freq_combined_first_merge = route_hash_freq_combined_first_merge.drop(['hash_inverse'], axis = 1)\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stop_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stop_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_second_part  = route_hash_freq_second_part.drop(['hash_inverse'], axis = 1)\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id'])\n",
    "    route_hash_freq_combined = route_hash_freq_combined.reset_index(drop = True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411086,
     "status": "ok",
     "timestamp": 1616073916300,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "lZAmA1H1gPOZ",
    "outputId": "06389806-4262-4385-d08d-ab254960d169"
   },
   "outputs": [],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #selects the sequences that are not the most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] != sequences_max_freq_merged['max_frequency']].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    route_id_creation =  route_hash_freq_combined['route_id'].max() + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    sequences_max_freq_merged = sequences_max_freq_merged.sort_values(by=['route_id'],ignore_index=True)\n",
    "    #keep only the column route_id and stop_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency', 'service_id'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411122,
     "status": "ok",
     "timestamp": 1616073916350,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "RxchbLZsXeg7",
    "outputId": "f970902f-7f93-46fe-ebab-c78b3f07122a"
   },
   "outputs": [],
   "source": [
    "''' To keep only the routes that have at least one dutch station in their route_sequence'''\n",
    "\n",
    "def keep_dutch_routes(final_routes):\n",
    "    non_dutch_routes = set()\n",
    "    for index_route, route in final_routes.iterrows():\n",
    "        is_in_Netherlands = False\n",
    "        for stop in route['stop_sequence']:\n",
    "            if stop in set(dutch_stops_Netherlands_series):\n",
    "                is_in_Netherlands = True\n",
    "                break\n",
    "        if not is_in_Netherlands:\n",
    "            route_id = route['route_id']\n",
    "            non_dutch_routes.add(route_id)\n",
    "    dutch_routes = final_routes.loc[~final_routes['route_id'].isin(non_dutch_routes)]    \n",
    "\n",
    "    return dutch_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 411121,
     "status": "ok",
     "timestamp": 1616073916362,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "q_PZYW41gROC",
    "outputId": "f15d7600-33b6-4670-8851-dfa51df7f20d"
   },
   "outputs": [],
   "source": [
    "'''Makes a set that can be used for building the edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx L-space graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stop_sequence_values = final_routes.apply(lambda x: pd.Series(x['stop_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stop_sequence_values.name = 'stop_sequence'\n",
    "    final_routes_stops = final_routes.drop('stop_sequence', axis=1).join(stop_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stop_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges['route_id'] = df_for_edges['route_id'].astype(np.int64)\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 411118,
     "status": "ok",
     "timestamp": 1616073916363,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "b64HwYNRp4tb"
   },
   "outputs": [],
   "source": [
    "def full_route_creation(stop_sequences_df, number_of_trips_per_hash, service_id_count_dates):\n",
    "    '''return a df that can be used to make a Networkx L-space (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stop_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stop_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stop_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single = calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation_third)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single)\n",
    "    route_hash_freq_combined = regroup_same_stop_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    dutch_routes = keep_dutch_routes(final_routes)\n",
    "    df_for_edges = create_df_for_Networkx(dutch_routes)\n",
    "    \n",
    "    return dutch_routes, df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "executionInfo": {
     "elapsed": 433836,
     "status": "ok",
     "timestamp": 1616073939092,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "s_iGBJQmqRBt",
    "outputId": "5ee08c2c-1914-43aa-8500-f778d142f61e"
   },
   "outputs": [],
   "source": [
    "dutch_routes_Netherlands, df_for_edges_Netherlands = full_route_creation(distinct_stop_sequences_Netherlands, route_hash_service_freq_Netherlands.copy(), service_id_df_Netherlands)\n",
    "dutch_routes_Netherlands\n",
    "df_for_edges_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 433833,
     "status": "ok",
     "timestamp": 1616073939094,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "tiEdPPtqgbXk"
   },
   "outputs": [],
   "source": [
    "df_for_edges_Netherlands.to_csv(r'/Users/pol/Desktop/CSV_export/df_for_edges_Netherlands.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cleaning_GTFS_Netherlands.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
