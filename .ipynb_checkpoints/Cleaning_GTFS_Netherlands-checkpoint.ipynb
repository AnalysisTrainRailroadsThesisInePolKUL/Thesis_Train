{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdWaEGPXXxUD",
    "outputId": "15157023-565e-4cb6-acea-977c8ba2ebf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (from geopy) (1.50)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4wVjHDm2CRQc"
   },
   "outputs": [],
   "source": [
    "'''To import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBJzOAhgX4ss"
   },
   "source": [
    "# Import of the Dutch railway datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fV4GmqXeZBs8"
   },
   "outputs": [],
   "source": [
    "'''To register the GitHub link with the Dutch data as a variable.'''\n",
    "datalink = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main/gtfs_train_Netherlands_0315/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DhWzAtwEXzMg"
   },
   "outputs": [],
   "source": [
    "'''Import all the GTFS data'''\n",
    "\n",
    "#To import the agency dataset that contains limited information about the Dutch NS railway agency.\n",
    "agency_Netherlands = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "#To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the Dutch NS railway stations.\n",
    "stops_Netherlands = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "#To import the feed_info dataset that contains limited information about the Dutch NS railway feed.\n",
    "feed_info_Netherlands = pd.read_csv(datalink + \"feed_info.txt\", sep=\",\")\n",
    "#To import the transfers dataset that gives the minimum transfer time to switch routes at each Belgian railway station.\n",
    "transfers_not_cleaned_Netherlands = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "#To import the routes dataset that provides the id, the name and the type of vehicle used for all Dutch NS railway routes.\n",
    "routes_Netherlands = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "#To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the Dutch NS railway route.\n",
    "#The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "trips_Netherlands = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "#To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "calendar_dates_Netherlands = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "_741TlIWZZoX",
    "outputId": "35554dd5-8458-4e86-dba5-bf85b2e8b5ce"
   },
   "outputs": [],
   "source": [
    "'''Import stop_times that is is of multiple csv files'''\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_range = [*range(2, 19)]\n",
    "stop_times_Netherlands = pd.read_csv(datalink + \"stop_times-1.csv\", sep=\",\")\n",
    "for index in stop_times_range:\n",
    "    stop_times_Netherlands = pd.concat([stop_times_Netherlands, pd.read_csv(datalink + \"stop_times-\" + str(index)+ \".csv\", sep=\",\")])\n",
    "stop_times_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "dx5QvhqXcHw-",
    "outputId": "427ae09d-e93e-4a8e-bc1c-fd8dc2c4a8b9"
   },
   "outputs": [],
   "source": [
    "'''Import shapes that is is of multiple csv files'''\n",
    "# ???\n",
    "shapes_range = [*range(2, 4)]\n",
    "shapes_Netherlands = pd.read_csv(datalink + \"shapes-1.csv\", sep=\",\")\n",
    "for index in shapes_range:\n",
    "    shapes_Netherlands = pd.concat([shapes_Netherlands, pd.read_csv(datalink + \"shapes-\" + str(index)+ \".csv\", sep=\",\")])\n",
    "shapes_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4y5ul9hdC1E"
   },
   "source": [
    "# Cleaning of the Dutch railway data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiQ0GHsPdF9J"
   },
   "outputs": [],
   "source": [
    "# To define a definition to remove the accents from a string\n",
    "def remove_accents(text):\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z716A6eWePgC"
   },
   "outputs": [],
   "source": [
    "''' To clean the routes_Netherlands df.'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Netherlands = routes_Netherlands[routes_Netherlands['route_type'] == 2]\n",
    "\n",
    "# To remove the accents from the route_long_name and to change to uppercase\n",
    "routes_cleaned_Netherlands.loc[:,'route_long_name'] = routes_cleaned_Netherlands.loc[:,'route_long_name'].apply(remove_accents)\n",
    "routes_cleaned_Netherlands.loc[:,'route_long_name'] = routes_cleaned_Netherlands.loc[:,'route_long_name'].str.upper()\n",
    "routes_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paayjLIceRbW"
   },
   "outputs": [],
   "source": [
    "''' To clean the trips_Netherlands df.'''\n",
    "# To remove the routes that are not train routes\n",
    "no_route_id_train_routes = routes_Netherlands.loc[routes_Netherlands['route_type'] != 2, 'route_id']\n",
    "trips_cleaned_Netherlands = trips_Netherlands[(~trips_Netherlands['route_id'].isin(no_route_id_train_routes))]\n",
    "\n",
    "# To remove the accents from the trip_headsign and to change to uppercase\n",
    "trips_cleaned_Netherlands.loc[:,'trip_headsign'] = trips_cleaned_Netherlands.loc[:,'trip_headsign'].apply(remove_accents)\n",
    "trips_cleaned_Netherlands.loc[:,'trip_headsign'] = trips_cleaned_Netherlands.loc[:,'trip_headsign'].str.upper()\n",
    "trips_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjdjoSPieVGn"
   },
   "outputs": [],
   "source": [
    "''' To clean the stop_times_Netherlands df.'''\n",
    "# To remove the stop_times trip_ids that are not trip_ids that belong to train routes and \n",
    "# to change the data type of the stop_id column\n",
    "no_trip_id_train_routes = trips_Netherlands.loc[trips_Netherlands['route_id'].isin(no_route_id_train_routes), 'trip_id']\n",
    "stop_times_cleaned_Netherlands = stop_times_Netherlands[(~stop_times_Netherlands['trip_id'].isin(no_trip_id_train_routes))]\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_id'] = stop_times_cleaned_Netherlands.stop_id.apply(str)\n",
    "\n",
    "# To take a subset of the stops_Netherlands df and to remove the accents from the stop_name and to change the stop_name to uppercase\n",
    "stop_id_name_Netherlands = stops_Netherlands[['stop_id', 'stop_name']]\n",
    "stop_id_name_Netherlands.loc[:,'stop_name'] = stop_id_name_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stop_id_name_Netherlands.loc[:,'stop_name'] = stop_id_name_Netherlands.loc[:,'stop_name'].str.upper()\n",
    "\n",
    "# To add the stop_name attribute of the stop_times_stop_id_name_Netherlands df to the stop_times_cleaned_Netherlands df and\n",
    "# to remove the stop_id attribute\n",
    "stop_times_cleaned_Netherlands = pd.merge(stop_times_cleaned_Netherlands, stop_id_name_Netherlands, on='stop_id', how='left')\n",
    "stop_times_cleaned_Netherlands.drop('stop_id', axis=1, inplace=True)\n",
    "stop_times_cleaned_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPyNzczxeWEX"
   },
   "outputs": [],
   "source": [
    "''' To clean the stops_Netherlands df.  (1) ''' \n",
    "# To take all unique stop_names that appear in the stop_times df\n",
    "unique_stop_names_stop_times_Netherlands = stop_times_cleaned_Netherlands['stop_name'].drop_duplicates()\n",
    "\n",
    "# To select all rows of the stops_Netherlands df that contain a stop_name that is in unique_stop_names_stop_times_Netherlands\n",
    "stops_initial_Netherlands = stops_Netherlands.copy()\n",
    "stops_initial_Netherlands['stop_name'] = stops_Netherlands['stop_name'].apply(remove_accents)\n",
    "stops_initial_Netherlands['stop_name'] = stops_initial_Netherlands['stop_name'].str.upper()\n",
    "stops_initial_Netherlands = stops_initial_Netherlands[stops_initial_Netherlands['stop_name'].isin(unique_stop_names_stop_times_Netherlands)]\n",
    "\n",
    "# To take from the stops_initial_Netherlands df all stop_ids that contain a 'stoparea:' to get the correct stop coordinates\n",
    "stops_cleaned_Netherlands = stops_initial_Netherlands[stops_initial_Netherlands['stop_id'].str.contains('stoparea:')]\n",
    "stops_cleaned_Netherlands = stops_cleaned_Netherlands.drop_duplicates()\n",
    "\n",
    "# To verify that there is an equal number of unique stop_names in the unique_stop_names_stop_times_Netherlands series and the stops_cleaned_Netherlands df\n",
    "stop_names_stops_cleaned_Netherlands = stops_cleaned_Netherlands[['stop_name']].drop_duplicates()\n",
    "len(unique_stop_names_stop_times_Netherlands)\n",
    "len(stop_names_stops_cleaned_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq4qH9eIeYFF"
   },
   "outputs": [],
   "source": [
    "''' To clean the stops_Netherlands df.  (2) ''' \n",
    "# To initialize the Nominatim API to get the location from the input string \n",
    "geolocator = Nominatim(user_agent=\"application\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "# To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "country_list = []\n",
    "for index, row in stops_cleaned_Netherlands.iterrows():\n",
    "    latitude = row['stop_lat']\n",
    "    longitude = row['stop_lon']\n",
    "    # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "    location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "    # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "    address = location.raw['address']\n",
    "    country = address.get('country', '')\n",
    "    country_list.append(country)\n",
    "\n",
    "# To add the values of country_list as a new attribute country     \n",
    "stops_cleaned_Netherlands.loc[:,'country'] = country_list\n",
    "stops_cleaned_Netherlands\n",
    "\n",
    "# To calculate the total number of Belgian stations in the stops_cleaned dataset\n",
    "dutch_stops_Netherlands = stops_cleaned_Netherlands[stops_cleaned_Netherlands['country'] == 'Netherlands']\n",
    "dutch_stops_Netherlands_series = stops_cleaned_Netherlands.loc[stops_cleaned_Netherlands['country'] == 'Netherlands', 'stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbAxq5n7ec32"
   },
   "source": [
    "# Exploratory data analysis with the Dutch railway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECwpEL4aedwk"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids '''\n",
    "set_routes_Netherlands = {r for r in routes_cleaned_Netherlands['route_id']}\n",
    "len(set_routes_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtA2_DM6efhd"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of stations in the stops_cleaned_Netherlands dataset'''\n",
    "set_stations_Netherlands = {s for s in stops_cleaned_Netherlands['stop_id']}\n",
    "len(set_stations_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uvasY6Heg-f"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of Dutch stations in the stops_cleaned dataset'''\n",
    "set_dutch_stations = {s for s in dutch_stops_Netherlands['stop_id']}\n",
    "len(set_dutch_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oat0T2iAeibZ"
   },
   "source": [
    "# **Preparation for the L-space representation of the Dutch railway system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbNSQeBaesjc"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the trips dataset and a selection of the routes dataset on route_id'''\n",
    "trips_routes_Netherlands = pd.merge(trips_cleaned_Netherlands[['route_id','service_id','trip_id', 'trip_headsign']], routes_cleaned_Netherlands[['route_id', 'route_short_name', 'route_long_name']], on='route_id')\n",
    "trips_routes_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U804DAlvevK5"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stop_times_cleaned_Netherlands dataset with a selection of the stops_cleaned_Netherlands dataset'''\n",
    "stop_times_stops_Netherlands = pd.merge(stop_times_cleaned_Netherlands[['trip_id','arrival_time', 'departure_time','stop_name','stop_sequence']], stops_cleaned_Netherlands[['stop_name', 'stop_lat', 'stop_lon']], on='stop_name')\n",
    "stop_times_stops_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1ZZxUR4ew1k"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stop_times_stops_Netherlands dataset with the trips_routes_Netherlands dataset.'''\n",
    "trips_routes_stop_times_stops_Netherlands = pd.merge(trips_routes_Netherlands, stop_times_stops_Netherlands, on='trip_id')\n",
    "trips_routes_stop_times_stops_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fafh2Wcfeynm"
   },
   "outputs": [],
   "source": [
    "'''To create a route_sequence dataset that gives for each trip_id that belongs to a route the sequence of stations served'''\n",
    "route_sequence_Netherlands = trips_routes_stop_times_stops_Netherlands.groupby(['route_id','route_long_name','trip_headsign','trip_id','stop_sequence'], as_index=False)[['stop_name', 'stop_lat', 'stop_lon']].last()\n",
    "route_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LneXrlKe0ox"
   },
   "outputs": [],
   "source": [
    "'''To calculate the hash and the hash_inverse values for the stop sequence of each trip_id'''\n",
    "\n",
    "#To copy the trips_routes_Netherlands df\n",
    "trips_hash_Netherlands = trips_routes_Netherlands.copy()\n",
    "\n",
    "#To create a column called hash that contains NaN values\n",
    "trips_hash_Netherlands['hash'] = np.nan\n",
    "\n",
    "#To create a column called hash_inverse that contains NaN values\n",
    "trips_hash_Netherlands['hash_inverse'] = np.nan\n",
    "\n",
    "#For each trip_id in trips_routes_Netherlands, the stop_sequence that gets calculated is the subset of the stop_times dataset for that trip_id. \n",
    "#The tuple that results from the stop_name column of this subset dataset contains all the stop_names that get served by this trip_id. \n",
    "\n",
    "#The hash value of the tuple of the stop_name column is calculated and is placed in the hash column of the trip_id in the trips_routes dataset\n",
    "#The inverse_hash value of the tuple of the stop_name column is calculated as well.\n",
    "\n",
    "for trip_Netherlands in trips_routes_Netherlands['trip_id'].unique():\n",
    "    stop_sequence_Netherlands = stop_times_cleaned_Netherlands[stop_times_cleaned_Netherlands['trip_id'] == trip_Netherlands].sort_values(by = 'stop_sequence')\n",
    "    trips_hash_Netherlands.loc[trips_hash_Netherlands['trip_id'] == trip_Netherlands, 'hash'] = hash(tuple(stop_sequence_Netherlands['stop_name']))\n",
    "    trips_hash_Netherlands.loc[trips_hash_Netherlands['trip_id'] == trip_Netherlands, 'hash_inverse'] = hash(tuple(list(stop_sequence_Netherlands['stop_name'])[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKoJWvg6e3M3"
   },
   "outputs": [],
   "source": [
    "trips_hash_Netherlands.to_csv(r'/Users/pol/Desktop/CSV_export/trips_hash_Netherlands.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW8xFq57fAac"
   },
   "outputs": [],
   "source": [
    "#trips_hash_Netherlands = pd.read_csv(datalink + \"trips_hash_Netherlands.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFw1YlkyfH6a"
   },
   "outputs": [],
   "source": [
    "''' To groupby the trip_id and to order the stop_sequence in an ascending order (the stop_sequences of some\n",
    "routes are initially in descending order while other stop_sequences are in ascending order) '''\n",
    "\n",
    "trips_stop_sequence_ascending_Netherlands = stop_times_stops_Netherlands.groupby(['trip_id'], as_index=False).apply(lambda x: x.sort_values('stop_sequence'))\n",
    "trips_stop_sequence_ascending_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hfmQ0J6fPPV"
   },
   "outputs": [],
   "source": [
    "''' To put the stop_names of a stop sequence of a trip_id in a list '''\n",
    "trips_stop_sequence_Netherlands = trips_stop_sequence_ascending_Netherlands.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "trips_stop_sequence_Netherlands.rename(columns={'stop_name':'stop_sequence'}, inplace=True)\n",
    "trips_stop_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL34w0R6fTB-"
   },
   "outputs": [],
   "source": [
    "''' To add the list of stop_sequence of stations to the trips_hash_Netherlands df by joining on trip_id'''\n",
    "# To add the stop_sequence of stations to the trips_hash_France df by joining on trip_id\n",
    "trips_hash_stop_sequence_Netherlands = pd.merge(trips_hash_Netherlands, trips_stop_sequence_Netherlands, on='trip_id', how='left')\n",
    "\n",
    "# To put the columns in a more logical order\n",
    "trips_hash_stop_sequence_Netherlands = trips_hash_stop_sequence_Netherlands[['route_id', 'route_long_name','service_id','trip_headsign','trip_id','hash', 'hash_inverse','stop_sequence']]\n",
    "trips_hash_stop_sequence_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9aO7ojufY8V"
   },
   "outputs": [],
   "source": [
    "''' To count the number of dates for each service_id '''\n",
    "service_id_df_Netherlands = calendar_dates_Netherlands.groupby(['service_id'])[['service_id']].count().rename(columns={'service_id':'count_service_id'}).reset_index()\n",
    "service_id_df_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8z2j6SxfZbT"
   },
   "outputs": [],
   "source": [
    "''' To regroup the days per service_id in a set '''\n",
    "service_id_dates_Netherlands = calendar_dates_Netherlands.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "service_id_dates_Netherlands.rename(columns={'date':'dates'}, inplace=True)\n",
    "service_id_dates_Netherlands = service_id_dates_Netherlands.merge(service_id_df_Netherlands, on='service_id', how='left')\n",
    "service_id_dates_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmOiY_jffbMp"
   },
   "outputs": [],
   "source": [
    "''' To put the different trip_ids in a list after joining on (route_id, route_long_name, hash and service_id) '''\n",
    "route_hash_freq_Netherlands = trips_hash_stop_sequence_Netherlands.groupby(['route_id','route_long_name','hash', 'hash_inverse', 'service_id'])['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouSygWbZfd6M"
   },
   "outputs": [],
   "source": [
    "''' To add the sequence of stops to the route_hash_freq dataset '''\n",
    "route_hash_freq_Netherlands = pd.merge(route_hash_freq_Netherlands, trips_hash_stop_sequence_Netherlands[['route_id','hash', 'hash_inverse', 'service_id','stop_sequence']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "route_hash_freq_Netherlands = route_hash_freq_Netherlands.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aSQ6CZVfh14"
   },
   "outputs": [],
   "source": [
    "''' To calculate the number of trip_ids in the list of trip_ids and to add it as a new column '''\n",
    "number_trip_ids_Netherlands = []\n",
    "for list_trip_ids_Netherlands in route_hash_freq_Netherlands['trip_id']:\n",
    "    count_Netherlands = len(list_trip_ids_Netherlands)\n",
    "    number_trip_ids_Netherlands.append(count_Netherlands)\n",
    "route_hash_freq_Netherlands['number_trip_ids'] = number_trip_ids_Netherlands\n",
    "\n",
    "route_hash_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j66hcLzAfkMB"
   },
   "outputs": [],
   "source": [
    "''' To merge the route_hash_freq_Netherlands df with the service_id_dates to get the sets of corresponding dates '''\n",
    "route_hash_service_freq_Netherlands = pd.merge(route_hash_freq_Netherlands, service_id_dates_Netherlands, on='service_id', how='left')\n",
    "route_hash_service_freq_Netherlands_copy = route_hash_service_freq_Netherlands.copy()\n",
    "route_hash_service_freq_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZnfLSQ5fsdZ"
   },
   "outputs": [],
   "source": [
    "'''Groups the service_id together for each route_id and hash combination'''\n",
    "for index, combi_route_id_hash in route_hash_service_freq_Netherlands_copy.groupby(['route_id','hash'], as_index = False)['service_id'].last().iterrows():\n",
    "    set_service_id = set(route_hash_service_freq_Netherlands_copy.loc[(route_hash_service_freq_Netherlands_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_Netherlands_copy['hash'] == combi_route_id_hash['hash'])]['service_id'])\n",
    "    route_hash_service_freq_Netherlands_copy.loc[(route_hash_service_freq_Netherlands_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_Netherlands_copy['hash'] == combi_route_id_hash['hash']),['service_id']] = set_service_id\n",
    "route_hash_service_freq_Netherlands_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbh_G6m4fx49"
   },
   "outputs": [],
   "source": [
    "'''Get the distinct stop sequences for all routes to create the possible track combinations later on'''\n",
    "distinct_stop_sequences_Netherlands = route_hash_service_freq_Netherlands_copy.drop_duplicates(subset = [\"route_id\", 'hash'])[['route_id','hash','stop_sequence', 'service_id']]\n",
    "distinct_stop_sequences_Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTMnP489f7zV"
   },
   "source": [
    "##Functions for the route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phYn4GZgf1Hn"
   },
   "outputs": [],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stop_sequences(stop_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stop_sequences_df[stop_sequences_df['route_id'] == route_id].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FJKfWAZgA5M"
   },
   "outputs": [],
   "source": [
    "'''Finds the routes that can be either extended from behind or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the tree indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stop_sequence']))))].copy()\n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_after = possible_extentions_after[possible_extentions_after['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_behind = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stop_sequence']))))].copy()        \n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_behind = possible_extentions_behind[possible_extentions_behind['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_behind, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                if possible_extentions_behind.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                if route_id not in index_of_complete_sequences:\n",
    "                    index_of_complete_sequences[route_id] = []\n",
    "                index_of_complete_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc5LeNFqgC86"
   },
   "outputs": [],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route'''\n",
    "\n",
    "def possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stop_sequence', 'service_id']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stop_sequence','service_id']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = route_creation_extensions_route_id[route_creation_extensions_route_id['stop_sequence'].apply(lambda x: any(item for item in [route_part['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(route_part['stop_sequence']))))].copy()\n",
    "                    #take only those extentions that have a common service_id with the route_part\n",
    "                    possible_extentions = possible_extentions[possible_extentions['service_id'].apply(lambda x: any(item for item in route_part['service_id'] if item in x))].copy()                \n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not (possible_extentions.empty):\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stop_sequence'] + possible_extention['stop_sequence'][1:]\n",
    "                            common_service_id = possible_extention['service_id'] & route_part['service_id']\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_service_id]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3mFGPd_gG1F"
   },
   "outputs": [],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stop_sequence', 'service_id']].copy()\n",
    "        copy_complete_sequences_df['hash'] = copy_complete_sequences_df['hash'].apply(lambda x: [x])\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGOYu0QagHqW"
   },
   "outputs": [],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        if route_id in route_creation['route_id'].unique():\n",
    "            #get a set of the hashes that were employed to create the routes for that route_id\n",
    "            used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "            #get a tuple of all the route sequences for that route_id\n",
    "            used_sequences = tuple(route_creation[route_creation['route_id'] == route_id]['stop_sequence'])\n",
    "            copy_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)[['route_id','hash','stop_sequence', 'service_id']]\n",
    "            copy_sequences_route_id['hash'] = copy_sequences_route_id['hash'].apply(lambda x: [x]) \n",
    "            #adds the hashes that were not employed in any route creations for that route_id\n",
    "            for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "                #first element of the list because there is always only one element\n",
    "                if trip['hash'][0] not in used_sequences_hash:\n",
    "                    #checks that the sequence is not a sublist of any existing sequences\n",
    "                    is_subsequence = False\n",
    "                    for sequence in used_sequences:\n",
    "                        if set(trip['stop_sequence']).issubset(sequence):\n",
    "                            is_subsequence = True\n",
    "                    if not is_subsequence:\n",
    "                        route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ6P8cvzgJfx"
   },
   "outputs": [],
   "source": [
    "'''Calculates the frequency of the constructed routes just made in the route_creation dataframe'''\n",
    "    \n",
    "def calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation):\n",
    "    '''calculates the frequencies of route_construction_third'''\n",
    "    #put the default value of the frequency to 0\n",
    "    route_creation['frequency'] = 0\n",
    "    for index_sequence, sequence in route_creation[['route_id','hash','service_id']].iterrows():\n",
    "        #initialize the varibles\n",
    "        sequence_frequency = 0\n",
    "        set_common_service_id = sequence['service_id']\n",
    "        if set_common_service_id:\n",
    "            #select the number_of_trips_per_hash only for the considered route_id\n",
    "            number_of_trips_per_hash_route_id = number_of_trips_per_hash[number_of_trips_per_hash['route_id'] == sequence['route_id']]\n",
    "            #only select the trips with the hash value contained in the sequence and with the same route_id\n",
    "            containing_hash = number_of_trips_per_hash_route_id[number_of_trips_per_hash_route_id['hash'].apply(lambda x: any(item for item in sequence['hash'] if x == item))]\n",
    "            #loop over each service_id that were common during the trip\n",
    "            for service_id in set_common_service_id:\n",
    "                service_id_number_days = service_id_count_dates[service_id_count_dates['service_id'] == service_id].iloc[0]['count_service_id']\n",
    "                #adds the minimum number of trips per day multiplied by the number of days in the service_id\n",
    "                sequence_frequency += containing_hash[containing_hash['service_id'] == service_id]['number_trip_ids'].min() * service_id_number_days\n",
    "            #adds the frequency in of the new route sequence\n",
    "            route_creation.loc[index_sequence, 'frequency'] = sequence_frequency\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14IOuVrvgLiL"
   },
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #create a column called hash and hash_invese that contains NaN values\n",
    "    route_creation_hash['hash'] = np.nan\n",
    "    route_creation_hash['hash_inverse'] = np.nan\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    for index, route_sequence in route_creation_hash.iterrows():\n",
    "        route_creation_hash.loc[index, 'hash'] = hash(tuple(route_sequence['stop_sequence']))\n",
    "        route_creation_hash.loc[index, 'hash_inverse'] = hash(tuple(list(route_sequence['stop_sequence'])[::-1]))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmdGOOgmgNd4"
   },
   "outputs": [],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stop_sequences(route_creation_hash):\n",
    "    '''regroups the stop_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_hash.copy().drop(['frequency'], axis = 1)\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    route_hash_freq_combined_first_merge = route_hash_freq_combined_first_merge.drop(['hash_inverse'], axis = 1)\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stop_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stop_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_second_part  = route_hash_freq_second_part.drop(['hash_inverse'], axis = 1)\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id'])\n",
    "    route_hash_freq_combined = route_hash_freq_combined.reset_index(drop = True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZAmA1H1gPOZ"
   },
   "outputs": [],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #selects the sequences that are not the most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] != sequences_max_freq_merged['max_frequency']].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    route_id_creation =  route_hash_freq_combined['route_id'].max() + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    sequences_max_freq_merged = sequences_max_freq_merged.sort_values(by=['route_id'],ignore_index=True)\n",
    "    #keep only the column route_id and stop_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency', 'service_id'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_PZYW41gROC"
   },
   "outputs": [],
   "source": [
    "'''Makes a set that can be used for building the edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx L-space graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stop_sequence_values = final_routes.apply(lambda x: pd.Series(x['stop_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stop_sequence_values.name = 'stop_sequence'\n",
    "    final_routes_stops = final_routes.drop('stop_sequence', axis=1).join(stop_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stop_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges['route_id'] = df_for_edges['route_id'].astype(np.int64)\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-LnHWPFgS4a"
   },
   "outputs": [],
   "source": [
    "def full_route_creation(stop_sequences_df, number_of_trips_per_hash, service_id_count_dates):\n",
    "    '''return a df that can be used to make a Networkx L-space (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stop_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stop_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stop_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single = calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation_third)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single)\n",
    "    route_hash_freq_combined = regroup_same_stop_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    df_for_edges = create_df_for_Networkx(final_routes)\n",
    "    \n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkqEuc6pgXC_"
   },
   "outputs": [],
   "source": [
    "df_for_edges_Netherlands = full_route_creation(distinct_stop_sequences_Netherlands, route_hash_service_freq_Netherlands.copy(), service_id_df_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiEdPPtqgbXk"
   },
   "outputs": [],
   "source": [
    "df_for_edges_Netherlands.to_csv(r'/Users/pol/Desktop/CSV_export/df_for_edges_Netherlands.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cleaning_GTFS_Netherlands.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
