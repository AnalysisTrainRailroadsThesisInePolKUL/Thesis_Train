{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3715,
     "status": "ok",
     "timestamp": 1616073301854,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "3XGVUGZ0XmCA",
    "outputId": "c84801fa-7158-4e3c-9113-17ab49492f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /Users/pol/opt/anaconda3/lib/python3.8/site-packages (from geopy) (1.50)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1616073302273,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "vOrgc5gwCHwg"
   },
   "outputs": [],
   "source": [
    "'''To import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "ox.plot_graph(ox.graph_from_place('Modena, Italy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNRvl3loPGbq"
   },
   "source": [
    "# Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3974,
     "status": "ok",
     "timestamp": 1616073302279,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "1a-6rwJUPDnz"
   },
   "outputs": [],
   "source": [
    "'''To display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3957,
     "status": "ok",
     "timestamp": 1616073302282,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "gElfdgSAPLwx",
    "outputId": "25576e59-9d91-426f-9b94-b101fae1a170"
   },
   "outputs": [],
   "source": [
    "'''To ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4A0-Q0JPPLp"
   },
   "source": [
    "# **Belgian railway system**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8C62khoPUDH"
   },
   "source": [
    "# Import of the Belgian railway datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3947,
     "status": "ok",
     "timestamp": 1616073302290,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "VAgYpRrtPSrh",
    "outputId": "6c63bc5d-99a1-4268-9d5c-c81bffd4ec0a"
   },
   "outputs": [],
   "source": [
    "'''To register the GitHub link with the Belgian data as a variable.'''\n",
    "datalink = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main/gtfs_train_Belgium_1503/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11244,
     "status": "ok",
     "timestamp": 1616073309605,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "CDA0iMqBPMcZ",
    "outputId": "2cb49de8-c934-4f3b-ca1e-df2a7349309b"
   },
   "outputs": [],
   "source": [
    "'''Import all the GTFS data'''\n",
    "\n",
    "#To import the agency dataset that contains limited information about Belgian NMBS/SNCB railway agency\n",
    "agency = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "#To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the Belgian railway stations.\n",
    "stops = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "#To import the translations dataset that provides the French-, Dutch-, German- and English-language translations of the Belgian railway stations.\n",
    "translations = pd.read_csv(datalink + \"translations.txt\", sep=\",\")\n",
    "#To import the transfers dataset that gives the minimum transfer time to switch routes at each Belgian railway station.\n",
    "transfers = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "#To import the routes dataset that provides the id, the name and the type of vehicle used for all Belgian railway routes.\n",
    "routes = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "#To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the Belgian railway route.\n",
    "#The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "trips = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times = pd.read_csv(datalink + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar = pd.read_csv(datalink + \"calendar.txt\", sep=\",\")\n",
    "#To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "calendar_dates = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n",
    "#???\n",
    "stop_time_overrides = pd.read_csv(datalink + \"stop_time_overrides.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-p1EbMuTIbe"
   },
   "source": [
    "# Cleaning of the Belgian railway data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 11326,
     "status": "ok",
     "timestamp": 1616073309712,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "f5YvYhs_S8X8",
    "outputId": "6babe122-513c-4c7c-e138-a28021e3e2b6"
   },
   "source": [
    "''' To clean the stops df.  (1) ''' \n",
    "#####To eliminate the stop_ids in the stops dataset that contain an underscore or that start with a character 'S'. \n",
    "stops_cleaned = stops[(~stops['stop_id'].str.contains('_')) & (~stops['stop_id'].str.contains('S'))]\n",
    "\n",
    "#####To modify the object datatype of the stop_id column to the numpy int64 datatype\n",
    "stops_cleaned.loc[:,'stop_id'] = stops_cleaned.loc[:,'stop_id'].astype(np.int64)\n",
    "\n",
    "##### To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned.loc[:,'stop_name'] = stops_cleaned.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned.loc[:,'stop_name'] = stops_cleaned.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "executionInfo": {
     "elapsed": 316405,
     "status": "ok",
     "timestamp": 1616073614806,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "9WIa4kXSTLZp",
    "outputId": "c2c49afa-5b6e-46c5-f867-34dab5ec3cb2"
   },
   "source": [
    "''' To clean the stops df.  (2) ''' \n",
    "##### To initialize the Nominatim API to get the location from the input string \n",
    "geolocator = Nominatim(user_agent=\"application\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "##### To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "country_list = []\n",
    "for index, row in stops_cleaned.iterrows():\n",
    "    latitude = row['stop_lat']\n",
    "    longitude = row['stop_lon']\n",
    "    # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "    location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "    # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "    address = location.raw['address']\n",
    "    country = address.get('country', '')\n",
    "    country_list.append(country)\n",
    "\n",
    "##### To add the values of country_list as a new attribute country \n",
    "stops_cleaned.loc[:,'country'] = country_list\n",
    "stops_cleaned\n",
    "\n",
    "##### To calculate the total number of Belgian stations in the stops_cleaned dataset\n",
    "belgian_stops_Belgium = stops_cleaned[stops_cleaned['country'] == 'Belgium']\n",
    "belgian_stops_Belgium_series = stops_cleaned.loc[stops_cleaned['country'] == 'Belgium', 'stop_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stops_cleaned.to_csv(r'/Users/pol/Desktop/CSV_export/stops_cleaned_Belgium.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "belgian_stops_Belgium_series.to_csv(r'/Users/pol/Desktop/CSV_export/belgian_stops_Belgium_series.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports the cleaned version of the stops with their country'''\n",
    "stops_cleaned = pd.read_csv(\"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main/stops_cleaned/stops_cleaned_Belgium.csv\", sep=\",\")\n",
    "belgian_stops_Belgium_series = pd.read_csv(\"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main/country_stops_series/stops_Belgium_series.csv\", sep=\",\")['stop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 316395,
     "status": "ok",
     "timestamp": 1616073614812,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "pgfh3IyXTRir",
    "outputId": "d8ac488b-22ec-4982-d3d0-99ec42712e7c"
   },
   "outputs": [],
   "source": [
    "'''To clean the trips df'''\n",
    "#To merge a selection of the trips dataset and a selection of the routes dataset on route_id\n",
    "trip_route_short_name = pd.merge(trips[['route_id','service_id','trip_id', 'trip_headsign']], routes[['route_id', 'route_short_name', 'route_long_name']], on='route_id')\n",
    "\n",
    "#To select the trips that belong to the routes that have a route_short_name that begins with an 'S' or is equal to 'IC', 'L' or 'P.'''\n",
    "allowed_route_type = {'IC', 'L', 'P', 'ICT', 'IZY'}\n",
    "filtered_trips = trip_route_short_name[(trip_route_short_name['route_short_name'].isin(allowed_route_type)) | (trip_route_short_name['route_short_name'].str.startswith('S'))]\n",
    "filtered_trips = filtered_trips.drop(columns=['route_short_name'])\n",
    "\n",
    "# To remove the accents from the route_long_name and to change to uppercase\n",
    "filtered_trips['route_long_name'] = filtered_trips['route_long_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "filtered_trips['route_long_name'] = filtered_trips['route_long_name'].str.upper()\n",
    "\n",
    "# To remove the accents from the trip_headsign and to change to uppercase\n",
    "filtered_trips['trip_headsign'] = filtered_trips['trip_headsign'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "filtered_trips['trip_headsign'] = filtered_trips['trip_headsign'].str.upper()\n",
    "filtered_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Filters the dats from the selected begin to the end date'''\n",
    "#here we used 4 months\n",
    "begin_date = 20210314\n",
    "end_date = 20210713\n",
    "filtered_calendar_dates = calendar_dates.copy()\n",
    "filtered_calendar_dates = filtered_calendar_dates.drop(filtered_calendar_dates[(filtered_calendar_dates['date'] > end_date) |(filtered_calendar_dates['date'] < begin_date)].index)\n",
    "filtered_calendar_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a75vbEp6TZzB"
   },
   "source": [
    "# Exploratory data analysis with the Belgian railway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 316383,
     "status": "ok",
     "timestamp": 1616073614815,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "8o8h6CF9TUYJ",
    "outputId": "7aa9e7f1-df4b-48c6-f108-90cec6dcc783"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids before removing the routes with a route_short_name that does not begin with an S and is not 'IC', 'L', or 'P'.'''\n",
    "initial_set_routes = {r for r in routes['route_id']}\n",
    "len(initial_set_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 316373,
     "status": "ok",
     "timestamp": 1616073614821,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "vf1yj1GeTg5b",
    "outputId": "6b5e0ce4-58cf-4de5-d065-7eb13619c24f"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids after removing the routes with a route_short_name that does not begin with an S and is not 'IC', 'L', or 'P'.'''\n",
    "set_routes = {r for r in filtered_trips['route_id']}\n",
    "len(set_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 316362,
     "status": "ok",
     "timestamp": 1616073614825,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "AEyJhjDsTk5K",
    "outputId": "d57f5219-b0e4-46db-ac50-b3a1990f5bdc"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of stations in the stops_cleaned dataset'''\n",
    "set_stations = {s for s in stops_cleaned['stop_id']}\n",
    "len(set_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 316482,
     "status": "ok",
     "timestamp": 1616073614960,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "fNKatxI5TknG",
    "outputId": "aeb920cf-1016-4e09-8cf8-d7a09197dcdf"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of Belgian stations in the stops_cleaned dataset'''\n",
    "len(belgian_stops_Belgium_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBVJ0VlkTjkY"
   },
   "source": [
    "# **Preparation for the L-space representation of the Belgian railway system**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 316464,
     "status": "ok",
     "timestamp": 1616073614963,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "XZrkXd07T7B8",
    "outputId": "f27b34a1-6a05-4efa-ba57-b19595e72dcb"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stops_cleaned dataset with a selection of the stop_times dataset'''\n",
    "stops_cleaned_stop_times_merge = pd.merge(stop_times[['trip_id','arrival_time', 'departure_time','stop_id','stop_sequence']], stops_cleaned[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on='stop_id')\n",
    "stops_cleaned_stop_times_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "executionInfo": {
     "elapsed": 316767,
     "status": "ok",
     "timestamp": 1616073615282,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "8vyuB5xcUIuS",
    "outputId": "1c3cbd49-37a8-452a-f0fc-a810fe73240a"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stops_cleaned_stop_times_merge dataset with the filtered_trips dataset. And sort the values.'''\n",
    "stops_cleaned_stop_times_trips_merge = pd.merge(filtered_trips, stops_cleaned_stop_times_merge, on='trip_id')\n",
    "stops_cleaned_stop_times_trips_merge = stops_cleaned_stop_times_trips_merge.sort_values(by=['route_id', 'trip_id', 'stop_sequence'])\n",
    "stops_cleaned_stop_times_trips_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creates a dataframe with the departure time form the first stop sequence and with the one from last stop sequence for each trip_id'''\n",
    "departure_time_first = stops_cleaned_stop_times_trips_merge.reset_index().loc[stops_cleaned_stop_times_trips_merge.reset_index().groupby(['trip_id'])['stop_sequence'].idxmin()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "departure_time_first = departure_time_first.rename(columns = {'departure_time': 'departure_time_first'})\n",
    "departure_time_last = stops_cleaned_stop_times_trips_merge.reset_index().loc[stops_cleaned_stop_times_trips_merge.reset_index().groupby(['trip_id'])['stop_sequence'].idxmax()][['route_id', 'trip_id', 'departure_time']].copy()\n",
    "departure_time_last = departure_time_last.rename(columns = {'departure_time': 'departure_time_last'})\n",
    "departure_times = departure_time_first.merge(departure_time_last[['trip_id', 'departure_time_last']], on='trip_id')\n",
    "departure_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 359561,
     "status": "ok",
     "timestamp": 1616073658127,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "IzcdyhZ-Uocb",
    "outputId": "ba430045-fc21-4b9f-cc40-5b791c0a3f53"
   },
   "outputs": [],
   "source": [
    "''' To groupby the trip_id and to order the stop_sequence in an ascending order\n",
    "Otherwise, different hash values could correspond to a same stop_sequence (since the stop_sequences of some\n",
    "routes are initially in descending order while other stop_sequences are in ascending order)'''\n",
    "\n",
    "trip_stop_sequence_ascending = stops_cleaned_stop_times_merge.groupby(['trip_id'], as_index=False).apply(lambda x: x.sort_values('stop_sequence'))\n",
    "trip_stop_sequence_ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 361107,
     "status": "ok",
     "timestamp": 1616073659693,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "or7hl-08UvOI",
    "outputId": "f52b2c6b-7da1-43b0-f275-8356a0c1c4b7"
   },
   "outputs": [],
   "source": [
    "'''To put the stop_names per trip_id in a list'''\n",
    "trip_stop_sequence = trip_stop_sequence_ascending.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "trip_stop_sequence.rename(columns={'stop_name':'stop_sequence'}, inplace=True)\n",
    "trip_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To calculate the hash value for the stop sequence of each trip_id\n",
    "and also the hash value of the stop sequence in the opposite direction'''\n",
    "\n",
    "#To copy the filtered_trips dataset\n",
    "trips_hash = trip_stop_sequence.copy()\n",
    "\n",
    "#calculates the hash of the stop sequence in both order (ascending and descending)\n",
    "trips_hash['hash'] = trips_hash['stop_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "trips_hash['hash_inverse'] = trips_hash['stop_sequence'].apply(lambda x: hash(tuple(x[::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "executionInfo": {
     "elapsed": 361108,
     "status": "ok",
     "timestamp": 1616073659711,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "NQ3Ed9oyU4Zd",
    "outputId": "79fa5dee-b697-441d-948a-e9ce29ad0ddd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' To add the list of stop_sequence of stations to the trips_hash df by joining on trip_id'''\n",
    "# To add the stop_sequence of stations to the filtered_trips dataset by joining on trip_id\n",
    "trips_hash_stop_sequence = pd.merge(filtered_trips, trips_hash, on='trip_id', how='left')\n",
    "\n",
    "# To put the columns in a more logical order\n",
    "trips_hash_stop_sequence = trips_hash_stop_sequence[['route_id', 'route_long_name','service_id','trip_headsign','trip_id','hash', 'hash_inverse','stop_sequence']]\n",
    "trips_hash_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merges the trips_hash_stop_sequence with the departure_times'''\n",
    "trips_hash_stop_sequence_departure = trips_hash_stop_sequence.merge(departure_times[['trip_id','departure_time_first','departure_time_last']], on='trip_id')\n",
    "trips_hash_stop_sequence_departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 361091,
     "status": "ok",
     "timestamp": 1616073659713,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "VWJUHseDU7CF",
    "outputId": "40957a0c-bed6-4fc9-d76f-1b49c495d754"
   },
   "outputs": [],
   "source": [
    "'''To count the number of dates for each service_id'''\n",
    "service_id_df = filtered_calendar_dates.groupby(['service_id'])[['service_id']].count().rename(columns={'service_id':'count_service_id'}).reset_index()\n",
    "service_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 361633,
     "status": "ok",
     "timestamp": 1616073660272,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "IO8UX0dOU9tT",
    "outputId": "f4dfe630-20ab-4e39-e9e2-60b9b28692ef"
   },
   "outputs": [],
   "source": [
    "'''Regroups the days per service id in a set and count them'''\n",
    "service_id_dates = filtered_calendar_dates.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "service_id_dates.rename(columns={'date':'dates'}, inplace=True)\n",
    "service_id_dates = service_id_dates.merge(service_id_df, on='service_id', how='left')\n",
    "service_id_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To merge the trips_hash_stop_sequence df with the service_id_dates to get the sets of corresponding dates'''\n",
    "stops_cleaned_stop_times_trips_merge_dates = pd.merge(stops_cleaned_stop_times_trips_merge, service_id_dates, on='service_id', how='inner')\n",
    "stops_cleaned_stop_times_trips_merge_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 361624,
     "status": "ok",
     "timestamp": 1616073660279,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "pzEgQhPxU_Br",
    "outputId": "c7d3479b-d140-44ab-fe94-a23f0cf628b4"
   },
   "outputs": [],
   "source": [
    "'''To put the different trip_ids in a list and add the departure_time first and last lists'''\n",
    "common_columns = ['route_id','route_long_name','hash', 'hash_inverse', 'service_id']\n",
    "route_hash_freq_dep = trips_hash_stop_sequence_departure.groupby(common_columns)['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq_dep_first = trips_hash_stop_sequence_departure.groupby(common_columns)['departure_time_first'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq_dep_last = trips_hash_stop_sequence_departure.groupby(common_columns)['departure_time_last'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq_dep = route_hash_freq_dep.merge(route_hash_freq_dep_first, on= common_columns)\n",
    "route_hash_freq_dep = route_hash_freq_dep.merge(route_hash_freq_dep_last, on= common_columns)\n",
    "route_hash_freq_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "executionInfo": {
     "elapsed": 361611,
     "status": "ok",
     "timestamp": 1616073660282,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "tqjGMDceVG_0",
    "outputId": "c5238978-0eab-420b-aef8-c6e3b5818512"
   },
   "outputs": [],
   "source": [
    "'''To add the sequence of stops to the route_hash_freq_dep dataset'''\n",
    "route_hash_freq_seq = pd.merge(route_hash_freq_dep, trips_hash_stop_sequence[['route_id','hash', 'hash_inverse', 'service_id','stop_sequence']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "route_hash_freq_seq = route_hash_freq_seq.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "\n",
    "route_hash_freq_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 361808,
     "status": "ok",
     "timestamp": 1616073660502,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Sd8tQ1ULVNTp",
    "outputId": "61b124fb-d857-4bcd-d4d6-1f5d3c2b324d"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of trip ids in the list of trip_ids and to add it as a new column'''\n",
    "route_hash_freq = route_hash_freq_seq.copy()\n",
    "number_trip_ids = []\n",
    "for list_trip_ids in route_hash_freq['trip_id']:\n",
    "    count = len(list_trip_ids)\n",
    "    number_trip_ids.append(count)\n",
    "route_hash_freq['number_trip_ids'] = number_trip_ids\n",
    "\n",
    "route_hash_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 361809,
     "status": "ok",
     "timestamp": 1616073660519,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "yPVkKKp2VOHz",
    "outputId": "f1dcf893-1bf9-4454-84dc-eb65bd5450f0"
   },
   "outputs": [],
   "source": [
    "'''To merge the route_hash_freq df with the service_id_dates to get the sets of corresponding dates'''\n",
    "route_hash_service_freq = pd.merge(route_hash_freq, service_id_dates, on='service_id', how='inner')\n",
    "route_hash_service_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYLL4jeLV3qC"
   },
   "source": [
    "## Functions for the route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366280,
     "status": "ok",
     "timestamp": 1616073665054,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "3m5eKSeHV6ul",
    "outputId": "3fbd40b5-1793-4124-f375-71d35eb2ea5f"
   },
   "outputs": [],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stop_sequences(stop_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stop_sequences_df[stop_sequences_df['route_id'] == route_id].copy()\n",
    "\n",
    "def take_leftovers_list_c_from_intersection_AAndB(list_a, list_b, list_c):\n",
    "    '''take the indexes of the intersection of list a with list b and retain the elments of list c with that index'''\n",
    "    ind_dict = dict((k,i) for i,k in enumerate(list_a))\n",
    "    return [list_c[ind_dict[x]] for x in (set(list_a).intersection(list_b))]\n",
    "\n",
    "def get_extentions (after_or_behind, route_sequences_route_id, trip):\n",
    "    '''returns the extentions for the trip (behind or after)'''\n",
    "    if after_or_behind == 'after':\n",
    "        #checks the extentions possible for the trip that can follow after its last stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stop_sequence']))))].copy()\n",
    "    elif after_or_behind == 'behind':\n",
    "        #checks the extentions possible for the trip that can follow before its first stop\n",
    "        possible_extentions = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stop_sequence']))))].copy()        \n",
    "    #checks that those extentions have a common date as the trip\n",
    "    possible_extentions = possible_extentions[possible_extentions['dates'].apply(lambda x: any(item for item in trip['dates'] if item in x))].copy()   \n",
    "    if not possible_extentions.empty: \n",
    "        if after_or_behind == 'after':\n",
    "            #checks that those extentions have a matching time schedule as the trip\n",
    "            possible_extentions = possible_extentions[possible_extentions['departure_time_first'].apply(lambda x: any(item for item in trip['departure_time_last'] if item in x))].copy()\n",
    "        elif after_or_behind == 'behind':\n",
    "            #checks that those extentions have a matching time schedule as the trip\n",
    "            possible_extentions = possible_extentions[possible_extentions['departure_time_last'].apply(lambda x: any(item for item in trip['departure_time_first'] if item in x))].copy()\n",
    "    return possible_extentions      \n",
    "\n",
    "def calculate_frequency (sequences_df):\n",
    "    '''calculate the frequency based on the length of the dates and departure_time and put the hash in as a column of list'''\n",
    "    sequences_df['number_dates'] = sequences_df['dates'].apply(lambda x: len(x))\n",
    "    sequences_df['number_times'] = sequences_df['departure_time_last'].apply(lambda x: len(x))\n",
    "    sequences_df['frequency'] = sequences_df['number_dates']* sequences_df['number_times'] \n",
    "    sequences_df = sequences_df.drop(['dates', 'departure_time_last', 'number_dates', 'number_times'], axis=1)\n",
    "    sequences_df['hash'] = sequences_df['hash'].apply(lambda x: [x])\n",
    "    return sequences_df.copy()\n",
    "        \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "def calculate_time_difference(time_df, later_time, earlier_time, column_name):\n",
    "    '''calculates the time difference between later time and earlier time and put it in time_df[column_name]'''\n",
    "    #transform 24:00:00 into 00:00:00\n",
    "    time_df['departure_time'] = time_df['departure_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >= 24 else x)\n",
    "    time_df['arrival_time'] = time_df['arrival_time'].apply(lambda x: str(int(x[:2])-24) + x[2:] if int(x[:2]) >=  24 else x)\n",
    "    #calculate the waiting_time\n",
    "    time_df[column_name] = time_df[['arrival_time','departure_time']].apply(lambda x: int((datetime.strptime(x[later_time], FMT) - datetime.strptime(x[earlier_time], FMT)).total_seconds()/60), axis=1)\n",
    "    #if one day as past, take it into consideration\n",
    "    time_df[column_name] = time_df[column_name].apply(lambda x: day_in_seconds/60 + x if x < 0 else x)\n",
    "    return time_df            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366260,
     "status": "ok",
     "timestamp": 1616073665055,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "J338sBEQWBWA",
    "outputId": "0f4cdff2-e7ea-4cbc-b179-e00d700ac9be"
   },
   "outputs": [],
   "source": [
    "'''Finds the routes that can be either extended from behind or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the tree indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = get_extentions('after', route_sequences_route_id, trip)\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_behind = get_extentions('behind', route_sequences_route_id, trip)\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_behind, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                if possible_extentions_behind.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                if route_id not in index_of_complete_sequences:\n",
    "                    index_of_complete_sequences[route_id] = []\n",
    "                index_of_complete_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366247,
     "status": "ok",
     "timestamp": 1616073665064,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "qgJ_I8_tWJny",
    "outputId": "1ee1f1a5-7e91-466e-9e1c-e49ad8533ada"
   },
   "outputs": [],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route and calculates their frequency'''\n",
    "\n",
    "def possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "            #set default frequency to NaN\n",
    "            routes_with_route_id['frequency'] = np.nan\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stop_sequence', 'dates', 'departure_time_last','frequency']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stop_sequence', 'dates', 'departure_time_first', 'departure_time_last','frequency']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route \n",
    "                    #and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = get_extentions('after', route_creation_extensions_route_id, route_part)\n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not possible_extentions.empty:\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stop_sequence'] + possible_extention['stop_sequence'][1:]\n",
    "                            common_dates = possible_extention['dates'] & route_part['dates']\n",
    "                            new_departure_time_last = take_leftovers_list_c_from_intersection_AAndB(list(possible_extentions['departure_time_first'])[0], list(route_part['departure_time_last']), list(possible_extentions['departure_time_last'])[0])\n",
    "                            new_frequency = len(new_departure_time_last) * len(common_dates)\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_dates, new_departure_time_last, new_frequency]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    if 'departure_time_last' in route_creation.columns:\n",
    "        route_creation = route_creation.drop(['dates', 'departure_time_last'], axis=1)\n",
    "    route_creation = route_creation.reindex(columns=['route_id','hash','stop_sequence', 'frequency'])\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366231,
     "status": "ok",
     "timestamp": 1616073665067,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "gHsAnUyRWLmk",
    "outputId": "f164d847-ecfb-4456-e465-bd8c390f27de"
   },
   "outputs": [],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stop_sequence', 'dates', 'departure_time_last']].copy()\n",
    "        copy_complete_sequences_df = calculate_frequency(copy_complete_sequences_df)\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366215,
     "status": "ok",
     "timestamp": 1616073665070,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "VSlsV8ExWNrd",
    "outputId": "f4f7d18a-6e70-4cee-d4fb-4756aedeaa04"
   },
   "outputs": [],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        if route_id in route_creation['route_id'].unique():\n",
    "            #get a set of the hashes that were employed to create the routes for that route_id\n",
    "            used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "            #get a tuple of all the route sequences for that route_id\n",
    "            used_sequences = tuple(route_creation[route_creation['route_id'] == route_id]['stop_sequence'])\n",
    "            copy_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)[['route_id','hash','stop_sequence', 'dates', 'departure_time_last']]\n",
    "            copy_sequences_route_id = calculate_frequency(copy_sequences_route_id)\n",
    "            #adds the hashes that were not employed in any route creations for that route_id\n",
    "            for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "                #first element of the list because there is always only one element\n",
    "                if trip['hash'][0] not in used_sequences_hash:\n",
    "                    #checks that the sequence is not a sublist of any existing sequences\n",
    "                    is_subsequence = False\n",
    "                    for sequence in used_sequences:\n",
    "                        if set(trip['stop_sequence']).issubset(sequence):\n",
    "                            is_subsequence = True\n",
    "                    if not is_subsequence:\n",
    "                        route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creates a column in the df that calculates the travel time between the first and last stop (waiting time included)\n",
    "and another column with the waiting time (calculated with a weighted average based on the frequency)'''\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "FMT = '%H:%M:%S'\n",
    "day_in_seconds = timedelta(days=1).total_seconds()\n",
    "\n",
    "def give_begin_end_time(route_creation_frequency_single, trips_hash_stop_sequence, stops_cleaned_stop_times_trips_merge_dates):\n",
    "    #create a copy to not change the input DataFrame\n",
    "    route_creation_frequency_single = route_creation_frequency_single.copy()\n",
    "    #makes a column with the a representative begin time and end time of the route\n",
    "    route_creation_frequency_single['travel_time'] = np.nan\n",
    "    for index_sequence, sequence in route_creation_frequency_single.iterrows():\n",
    "        constructed_route = pd.DataFrame()\n",
    "        for index_hash, hash_value in enumerate(sequence['hash']):\n",
    "            index_plus_one = index_hash + 1\n",
    "            #take all the trips with that hash\n",
    "            next_representative_trips = trips_hash_stop_sequence[(trips_hash_stop_sequence['hash'] == hash_value) & (trips_hash_stop_sequence['route_id'] == sequence['route_id'])].copy()['trip_id']\n",
    "            #take all the stop sequences and their time that belongs \n",
    "            full_times = stops_cleaned_stop_times_trips_merge_dates[stops_cleaned_stop_times_trips_merge_dates['trip_id'].isin(next_representative_trips)].copy()\n",
    "            #select) only the last stop sequences of full_times for each trip_id\n",
    "            new_index_max_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmax()\n",
    "            max_per_trip_id = full_times.reset_index().loc[new_index_max_per_trip_id]\n",
    "            #select only the first stop sequences of full_times for each trip_id            \n",
    "            new_index_min_per_trip_id = full_times.reset_index().groupby(['route_id', 'trip_id'])['stop_sequence'].idxmin()            \n",
    "            min_per_trip_id = full_times.reset_index().loc[new_index_min_per_trip_id]\n",
    "            #merge max_per_trip_id and min_per_trip_id\n",
    "            merged = min_per_trip_id[['trip_id', 'dates', 'departure_time']].merge(max_per_trip_id[['trip_id', 'arrival_time', 'departure_time']], on='trip_id')\n",
    "            #take all the stop sequences except the first one, and the last one if it is not the last sequence of the route\n",
    "            if index_hash == len(sequence['hash']) - 1:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(pd.concat([new_index_min_per_trip_id,new_index_max_per_trip_id]))\n",
    "            else:\n",
    "                rest_per_trip_id = full_times.reset_index().drop(new_index_min_per_trip_id)            \n",
    "            if not rest_per_trip_id.empty:\n",
    "                rest_per_trip_id = calculate_time_difference(rest_per_trip_id, 'departure_time', 'arrival_time', 'waiting_time')\n",
    "                #calculate the total waiting_time\n",
    "                rest_per_trip_id_grouped = rest_per_trip_id.groupby(['trip_id'], as_index=False)['waiting_time'].sum()\n",
    "                merged_waiting_time = merged.merge(rest_per_trip_id_grouped, on='trip_id')\n",
    "            #in case there are only two stops in for the hash\n",
    "            else:\n",
    "                merged_waiting_time = merged.copy()\n",
    "                merged_waiting_time['waiting_time'] = 0\n",
    "            #rename the columns     \n",
    "            merged_waiting_time = merged_waiting_time.rename(columns = {'trip_id': 'trip_id_' + str(index_plus_one),'departure_time_x':'departure_time_'+ str(index_plus_one), 'arrival_time':'arrival_time_'+ str(index_plus_one),\n",
    "                                          'departure_time_y':'departure_time_'+ str(index_plus_one + 1), 'waiting_time': 'waiting_time_' + str(index_plus_one)})\n",
    "            if index_hash == 0:\n",
    "                constructed_route = merged_waiting_time\n",
    "            elif index_hash > 0:\n",
    "                constructed_route = constructed_route.merge(merged_waiting_time, how='inner', on=['departure_time_' + str(index_plus_one)])\n",
    "                #take the intersection of the dates => only get the common dates and retain those rows with common dates\n",
    "                constructed_route['dates'] = [a & b for a,b in zip(constructed_route['dates_x'], constructed_route['dates_y'])]\n",
    "                constructed_route = constructed_route[constructed_route['dates'].map(lambda d: len(d)) > 0]\n",
    "                constructed_route = constructed_route.drop(['dates_x','dates_y'], axis=1)        \n",
    "        #make a list of all the columns of waiting_times\n",
    "        list_column_waiting_time = []\n",
    "        for i in range(1, index_plus_one + 1):\n",
    "            list_column_waiting_time.append('waiting_time_' + str(i))\n",
    "        #sum all the waiting times together for each route itinerary\n",
    "        constructed_route['waiting_time'] = constructed_route[list_column_waiting_time].astype(int).sum(1)\n",
    "        \n",
    "        #sometimes it is impossible to find trips that follow each other\n",
    "        if not constructed_route.empty:\n",
    "            #when the loop is finished, take the last arrival time, that will be used to calculate the travel time\n",
    "            time_constructed_route = constructed_route[['departure_time_1', 'arrival_time_' + str(index_plus_one), 'waiting_time', 'dates']]\n",
    "            time_constructed_route = time_constructed_route.rename(columns = {'departure_time_1':'departure_time', 'arrival_time_' + str(index_plus_one):'arrival_time'})\n",
    "            time_constructed_route = calculate_time_difference(time_constructed_route, 'arrival_time', 'departure_time', 'time_diff_min')\n",
    "            #add here a new column count dates that is the sum of the common dates\n",
    "            time_constructed_route['count_dates'] = time_constructed_route['dates'].apply(lambda x: len(x))\n",
    "            sum_count_dates = time_constructed_route['count_dates'].sum()\n",
    "            #take the first most frequent one\n",
    "            #create the weighted sum\n",
    "            time_constructed_route['WS_travel_time'] = (time_constructed_route['time_diff_min'] * time_constructed_route['count_dates'])/sum_count_dates\n",
    "            time_constructed_route['WS_waiting_time'] = (time_constructed_route['waiting_time'] * time_constructed_route['count_dates'])/sum_count_dates    \n",
    "            weighted_sum_tt = time_constructed_route['WS_travel_time'].sum()\n",
    "            weighted_sum_wt = time_constructed_route['WS_waiting_time'].sum()\n",
    "            #Add this to the first dataframe\n",
    "            route_creation_frequency_single.loc[index_sequence,'travel_time'] = weighted_sum_tt\n",
    "            route_creation_frequency_single.loc[index_sequence,'waiting_time'] = weighted_sum_wt\n",
    "        #if there is no trips that follow each other with the hash from the array\n",
    "        else:\n",
    "            route_creation_frequency_single = route_creation_frequency_single.drop(index_sequence)\n",
    "            \n",
    "    return route_creation_frequency_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 366203,
     "status": "ok",
     "timestamp": 1616073665077,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "JA7ZKz_IWSHC"
   },
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    route_creation_hash['hash'] = route_creation_hash['stop_sequence'].apply(lambda x: hash(tuple(x)))\n",
    "    route_creation_hash['hash_inverse'] = route_creation_hash['stop_sequence'].apply(lambda x: hash(tuple(x[::-1])))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366190,
     "status": "ok",
     "timestamp": 1616073665079,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Y_do0tL_WT4h",
    "outputId": "c8d91f90-3f6b-44c5-a5d0-3f52f77961ed"
   },
   "outputs": [],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stop_sequences(route_creation_hash):\n",
    "    '''regroups the stop_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_hash.copy().drop(['frequency'], axis = 1)\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stop_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stop_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id'])\n",
    "    route_hash_freq_combined = route_hash_freq_combined.reset_index(drop = True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366176,
     "status": "ok",
     "timestamp": 1616073665081,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "fDwTS3rYWWJ8",
    "outputId": "382e1dd7-3704-49e4-ef6a-7f43b07cb24d"
   },
   "outputs": [],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #drop the routes with the same hash as others\n",
    "    route_hash_freq_treshold['max_hash'] = route_hash_freq_treshold[['hash', 'hash_inverse']].max(axis=1)\n",
    "    route_hash_freq_treshold = route_hash_freq_treshold.drop_duplicates(subset='max_hash')\n",
    "    route_hash_freq_treshold  = route_hash_freq_treshold.drop(['hash_inverse', 'max_hash'], axis = 1)\n",
    "    #selects the sequences that are not the first most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == sequences_max_freq_merged['max_frequency']].drop_duplicates(subset='route_id').index\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[~sequences_max_freq_merged.index.isin(sequences_max_freq_index)].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    route_id_creation =  route_hash_freq_combined['route_id'].max() + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    sequences_max_freq_merged = sequences_max_freq_merged.sort_values(by=['route_id'],ignore_index=True)\n",
    "    #keep only the column route_id and stop_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(sequences_max_freq_merged[sequences_max_freq_merged['frequency'] == 0].index)\n",
    "    final_routes = final_routes.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366161,
     "status": "ok",
     "timestamp": 1616073665082,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "2zmTEWeipgM0",
    "outputId": "a8111a08-9db9-42e4-dcde-d26e2481ab03"
   },
   "outputs": [],
   "source": [
    "''' To keep only the routes that have at least one belgian station in their route_sequence'''\n",
    "\n",
    "def keep_belgian_routes(final_routes):\n",
    "    non_belgian_routes = set()\n",
    "    for index_route, route in final_routes.iterrows():\n",
    "        is_in_Belgium = False\n",
    "        for stop in route['stop_sequence']:\n",
    "            if stop in set(belgian_stops_Belgium_series):\n",
    "                is_in_Belgium = True\n",
    "                break\n",
    "        if not is_in_Belgium:\n",
    "            route_id = route['route_id']\n",
    "            non_belgian_routes.add(route_id)\n",
    "    belgian_routes = final_routes.loc[~final_routes['route_id'].isin(non_belgian_routes)]\n",
    "    \n",
    "    return belgian_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculates the distances of the trip, by taking the distance between each stop of the stop_sequence'''\n",
    "\n",
    "def calculate_distance_from_lat_long(name_first, name_second, stop_df):\n",
    "        lon_first, lat_first = math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_first].iloc[0]['stop_lat'])\n",
    "        lon_second, lat_second = math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lon']), math.radians(stop_df[stop_df['stop_name'] == name_second].iloc[0]['stop_lat'])\n",
    "        # The radius of the earth\n",
    "        R = 6373.0 \n",
    "        # To calculate the change in coordinates\n",
    "        dlon = lon_second - lon_first\n",
    "        dlat = lat_second - lat_first\n",
    "        # To use the Haversine formula to get the distance in kilometers between the starting_station and the ending_station\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat_first) * math.cos(lat_second) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        # To calculate the distance\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "def calculate_distance(stop_sequence, stop_df):\n",
    "    distance = 0\n",
    "    for index_stop ,stop in enumerate(stop_sequence):\n",
    "        index_plus_one = index_stop + 1\n",
    "        if index_plus_one <= len(stop_sequence) - 1:\n",
    "            distance += calculate_distance_from_lat_long(stop, stop_sequence[index_plus_one], stop_df)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 366147,
     "status": "ok",
     "timestamp": 1616073665085,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "Ws2UdF5lWZrI",
    "outputId": "ed310c09-e305-4ec7-82b5-d8acfce11eec"
   },
   "outputs": [],
   "source": [
    "'''Makes a df that can be used for building the nodes and edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx L-space graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stop_sequence_values = final_routes.apply(lambda x: pd.Series(x['stop_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stop_sequence_values.name = 'stop_sequence'\n",
    "    final_routes_stops = final_routes.drop('stop_sequence', axis=1).join(stop_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stop_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges['route_id'] = df_for_edges['route_id'].astype(np.int64)\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivW6-O7ypY7P"
   },
   "source": [
    "# To apply the route creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the df to get always the right order of rows \n",
    "route_hash_service_freq_sorted = route_hash_service_freq.sort_values(by=['route_id','hash']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 366361,
     "status": "ok",
     "timestamp": 1616073665305,
     "user": {
      "displayName": "Ine Winters",
      "photoUrl": "",
      "userId": "11267554535649979995"
     },
     "user_tz": -60
    },
    "id": "VIpvkpAAk7Co"
   },
   "outputs": [],
   "source": [
    "'''Applies all the functions from 1 get_extention_indexes to 11 create_df_for_Networkx'''\n",
    "\n",
    "def full_route_creation(stop_sequences_df, number_of_trips_per_hash, service_id_count_dates, trips_hash_stop_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned):\n",
    "    '''return a df that can be used to make a Networkx L-space (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stop_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stop_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stop_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single_travel_time = give_begin_end_time(route_creation_third, trips_hash_stop_sequence, stops_cleaned_stop_times_trips_merge_dates)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single_travel_time)\n",
    "    route_hash_freq_combined = regroup_same_stop_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    belgian_routes = keep_belgian_routes(final_routes)\n",
    "    belgian_routes['distance'] = belgian_routes['stop_sequence'].apply(lambda x: calculate_distance(x, stops_cleaned))\n",
    "    df_for_edges = create_df_for_Networkx(belgian_routes)\n",
    "    \n",
    "    return route_creation_frequency_single_travel_time, route_creation_frequency_single_travel_time ,belgian_routes, df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_creation_frequency_single_travel_time, route_creation_frequency_single_travel_time, belgian_routes_Belgium, df_for_edges_Belgium = full_route_creation(route_hash_service_freq, route_hash_service_freq_sorted, service_id_df, trips_hash_stop_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned)\n",
    "route_creation_frequency_single_travel_time\n",
    "belgian_routes_Belgium\n",
    "df_for_edges_Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_edges_Belgium.to_csv(r'/Users/pol/Desktop/CSV_export/df_for_edges_Belgium.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belgian_routes_Belgium.to_csv(r'/Users/pol/Desktop/CSV_export/belgian_routes_Belgium.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_hash_service_freq, route_hash_service_freq_sorted, service_id_df, trips_hash_stop_sequence, stops_cleaned_stop_times_trips_merge_dates, stops_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cleaning_GTFS_Belgium.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
