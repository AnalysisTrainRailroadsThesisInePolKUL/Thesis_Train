{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.1.0-py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geographiclib<2,>=1.49\n",
      "  Downloading geographiclib-1.50-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vOrgc5gwCHwg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To import the required packages.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNRvl3loPGbq"
   },
   "source": [
    "# Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1a-6rwJUPDnz"
   },
   "outputs": [],
   "source": [
    "'''To display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "gElfdgSAPLwx",
    "outputId": "f16ebc15-3a35-4a21-b361-ce53a08b649c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To ensure that the output results of extensive output results are not truncated.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4A0-Q0JPPLp"
   },
   "source": [
    "# **Belgian railway system**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8C62khoPUDH"
   },
   "source": [
    "# Import of the Belgian railway datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "VAgYpRrtPSrh",
    "outputId": "397aac53-095e-4949-ff17-509034762ddc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To register the GitHub link with the Belgian data as a variable.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To register the GitHub link with the Belgian data as a variable.'''\n",
    "datalink = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main/gtfs_train_Belgium_1503/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "CDA0iMqBPMcZ",
    "outputId": "b32fcaf4-106d-498b-a031-5bbc13468e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import all the GTFS data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import all the GTFS data'''\n",
    "\n",
    "#To import the agency dataset that contains limited information about Belgian NMBS/SNCB railway agency\n",
    "agency = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "#To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the Belgian railway stations.\n",
    "stops = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "#To import the translations dataset that provides the French-, Dutch-, German- and English-language translations of the Belgian railway stations.\n",
    "translations = pd.read_csv(datalink + \"translations.txt\", sep=\",\")\n",
    "#To import the transfers dataset that gives the minimum transfer time to switch routes at each Belgian railway station.\n",
    "transfers = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "#To import the routes dataset that provides the id, the name and the type of vehicle used for all Belgian railway routes.\n",
    "routes = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "#To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the Belgian railway route.\n",
    "#The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "trips = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times = pd.read_csv(datalink + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar = pd.read_csv(datalink + \"calendar.txt\", sep=\",\")\n",
    "#To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "calendar_dates = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n",
    "#???\n",
    "stop_time_overrides = pd.read_csv(datalink + \"stop_time_overrides.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-p1EbMuTIbe"
   },
   "source": [
    "# Cleaning of the Belgian railway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "f5YvYhs_S8X8",
    "outputId": "cf6d1dff-9889-492a-b6f0-74e768118ba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To clean the stops df.  (1) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "''' To clean the stops df.  (1) ''' \n",
    "#To eliminate the stop_ids in the stops dataset that contain an underscore or that start with a character 'S'. \n",
    "stops_cleaned = stops[(~stops['stop_id'].str.contains('_')) & (~stops['stop_id'].str.contains('S'))]\n",
    "\n",
    "#To modify the object datatype of the stop_id column to the numpy int64 datatype\n",
    "stops_cleaned.loc[:,'stop_id'] = stops_cleaned.loc[:,'stop_id'].astype(np.int64)\n",
    "\n",
    "# To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned.loc[:,'stop_name'] = stops_cleaned.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned.loc[:,'stop_name'] = stops_cleaned.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "9WIa4kXSTLZp",
    "outputId": "a166a4ea-0ecc-414f-8334-ab9eabbd82c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To clean the stops df.  (2) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Nominatim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-284ec7246d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m''' To clean the stops df.  (2) '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# To initialize the Nominatim API to get the location from the input string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgeolocator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRateLimiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delay_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Nominatim' is not defined"
     ]
    }
   ],
   "source": [
    "''' To clean the stops df.  (2) ''' \n",
    "# To initialize the Nominatim API to get the location from the input string \n",
    "geolocator = Nominatim(user_agent=\"application\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "# To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "country_list = []\n",
    "for index, row in stops_cleaned.iterrows():\n",
    "    latitude = row['stop_lat']\n",
    "    longitude = row['stop_lon']\n",
    "    # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "    location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "    # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "    address = location.raw['address']\n",
    "    country = address.get('country', '')\n",
    "    country_list.append(country)\n",
    "\n",
    "# To add the values of country_list as a new attribute country \n",
    "stops_cleaned.loc[:,'country'] = country_list\n",
    "stops_cleaned\n",
    "\n",
    "# To calculate the total number of Belgian stations in the stops_cleaned dataset\n",
    "belgian_stops_Belgium = stops_cleaned[stops_cleaned['country'] == 'Belgium']\n",
    "belgian_stops_Belgium_series = stops_cleaned.loc[stops_cleaned['country'] == 'Belgium', 'stop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "pgfh3IyXTRir",
    "outputId": "739920aa-32c7-4a65-89b8-52fe226fe8e4"
   },
   "outputs": [],
   "source": [
    "''' To clean the trips df'''\n",
    "#To merge a selection of the trips dataset and a selection of the routes dataset on route_id\n",
    "trip_route_short_name = pd.merge(trips[['route_id','service_id','trip_id', 'trip_headsign']], routes[['route_id', 'route_short_name', 'route_long_name']], on='route_id')\n",
    "\n",
    "#To select the trips that belong to the routes that have a route_short_name that begins with an 'S' or is equal to 'IC', 'L' or 'P.'''\n",
    "allowed_route_type = {'IC', 'L', 'P'}\n",
    "filtered_trips = trip_route_short_name[(trip_route_short_name['route_short_name'].isin(allowed_route_type)) | (trip_route_short_name['route_short_name'].str.startswith('S'))]\n",
    "filtered_trips = filtered_trips.drop(columns=['route_short_name'])\n",
    "\n",
    "# To remove the accents from the route_long_name and to change to uppercase\n",
    "filtered_trips['route_long_name'] = filtered_trips['route_long_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "filtered_trips['route_long_name'] = filtered_trips['route_long_name'].str.upper()\n",
    "\n",
    "# To remove the accents from the trip_headsign and to change to uppercase\n",
    "filtered_trips['trip_headsign'] = filtered_trips['trip_headsign'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "filtered_trips['trip_headsign'] = filtered_trips['trip_headsign'].str.upper()\n",
    "filtered_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a75vbEp6TZzB"
   },
   "source": [
    "# Exploratory data analysis with the Belgian railway data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "8o8h6CF9TUYJ",
    "outputId": "37a2ee30-2b21-44b1-d144-2a12f300aa35"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids before removing the routes with a route_short_name that does not begin with an S and is not 'IC', 'L', or 'P'.'''\n",
    "initial_set_routes = {r for r in routes['route_id']}\n",
    "len(initial_set_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "vf1yj1GeTg5b",
    "outputId": "4f7800ef-f50c-425c-d68b-60874648df6e"
   },
   "outputs": [],
   "source": [
    "'''To calculate the number of unique route_ids after removing the routes with a route_short_name that does not begin with an S and is not 'IC', 'L', or 'P'.'''\n",
    "set_routes = {r for r in filtered_trips['route_id']}\n",
    "len(set_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "AEyJhjDsTk5K",
    "outputId": "26ff974f-36a6-470f-da98-e59704140ffe"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of stations in the stops_cleaned dataset'''\n",
    "set_stations = {s for s in stops_cleaned['stop_id']}\n",
    "len(set_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "fNKatxI5TknG",
    "outputId": "87466110-defe-43b0-98d7-147ac3ff484f"
   },
   "outputs": [],
   "source": [
    "'''To calculate the total number of Belgian stations in the stops_cleaned dataset'''\n",
    "set_belgian_stations = {s for s in belgian_stops_Belgium['stop_id']}\n",
    "len(set_belgian_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBVJ0VlkTjkY"
   },
   "source": [
    "# **Preparation for the L-space representation of the Belgian railway system**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "XZrkXd07T7B8",
    "outputId": "962037c3-6654-4161-a525-32ac59980ae6"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stops_cleaned dataset with a selection of the stop_times dataset'''\n",
    "stops_cleaned_stop_times_merge = pd.merge(stop_times[['trip_id','arrival_time', 'departure_time','stop_id','stop_sequence']], stops_cleaned[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on='stop_id')\n",
    "stops_cleaned_stop_times_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "8vyuB5xcUIuS",
    "outputId": "4172d83d-2836-4d1e-9982-ee94ce2dcd29"
   },
   "outputs": [],
   "source": [
    "'''To merge a selection of the stops_cleaned_stop_times_merge dataset with the filtered_trips dataset.'''\n",
    "stops_cleaned_stop_times_trips_merge = pd.merge(filtered_trips, stops_cleaned_stop_times_merge, on='trip_id', how = 'left')\n",
    "stops_cleaned_stop_times_trips_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "maL8ongPULzr",
    "outputId": "22faf6e6-b519-40fe-e20d-7f021c81fb98"
   },
   "outputs": [],
   "source": [
    "'''To create a route_sequence dataset that gives for each trip_id that belongs to a route the sequence of stations served'''\n",
    "route_sequence = stops_cleaned_stop_times_trips_merge.groupby(['route_id','route_long_name','trip_headsign','trip_id','stop_sequence'], as_index=False)[['stop_name', 'stop_lat', 'stop_lon']].last()\n",
    "route_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1B-EzURUWP6"
   },
   "outputs": [],
   "source": [
    "'''To calculate the hash value for the stop sequence of each trip_id'''\n",
    "\n",
    "#To copy the filtered_trips dataset\n",
    "trips_hash = filtered_trips.copy()\n",
    "\n",
    "#To create a column called hash that contains NaN values\n",
    "trips_hash['hash'] = np.nan\n",
    "\n",
    "#For each trip_id in filtered_trips, the stop_sequence that gets calculated is the subset of the stop_time dataset for that trip_id. \n",
    "#The tuple that results from the stop_id column of this subset dataset contains all the stop_ids that get served by this trip_id. \n",
    "#The hash value of the tuple of the stop_id column is calculated and is placed in the hash column of the trip_id in the filtered_trips dataset\n",
    "\n",
    "trips_hash['hash_inverse'] = np.nan\n",
    "for trip in filtered_trips['trip_id'].unique():\n",
    "    stop_sequence = stop_times[stop_times['trip_id'] == trip].sort_values(by = 'stop_sequence')\n",
    "    trips_hash.loc[trips_hash['trip_id'] == trip, 'hash'] = hash(tuple(stop_sequence['stop_id']))\n",
    "    trips_hash.loc[trips_hash['trip_id'] == trip, 'hash_inverse'] = hash(tuple(list(stop_sequence['stop_id'])[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPE20yD7UbEi"
   },
   "outputs": [],
   "source": [
    "trips_hash.to_csv(r'/Users/pol/Desktop/CSV_export/trips_hash_Belgium.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zoa4XAgUUjnn"
   },
   "outputs": [],
   "source": [
    "#trips_hash = pd.read_csv(datalink + \"trips_hash.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzcdyhZ-Uocb"
   },
   "outputs": [],
   "source": [
    "# To groupby the trip_id and to order the stop_sequence in an ascending order\n",
    "# Otherwise, different hash values could correspond to a same stop_sequence (since the stop_sequences of some\n",
    "# routes are initially in descending order while other stop_sequences are in ascending order)\n",
    "\n",
    "trip_stop_sequence_sorted = stops_cleaned_stop_times_merge.groupby(['trip_id'], as_index=False).apply(lambda x: x.sort_values('stop_sequence'))\n",
    "trip_stop_sequence_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or7hl-08UvOI"
   },
   "outputs": [],
   "source": [
    "# To put the stop_names per trip_id in a list\n",
    "trip_stop_sequence = trip_stop_sequence_sorted.groupby('trip_id')['stop_name'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "trip_stop_sequence.rename(columns={'stop_name':'stop_sequence'}, inplace=True)\n",
    "trip_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ3Ed9oyU4Zd"
   },
   "outputs": [],
   "source": [
    "# To add the stop_sequence of stations to the trips_hash dataset in the trips_hash_stop_sequence dataset by joining on trip_id\n",
    "trips_hash_stop_sequence = pd.merge(trips_hash, trip_stop_sequence, on='trip_id', how='left')\n",
    "\n",
    "# To put the columns in a more logical order\n",
    "trips_hash_stop_sequence = trips_hash_stop_sequence[['route_id', 'route_long_name','service_id','trip_headsign','trip_id','hash', 'hash_inverse','stop_sequence']]\n",
    "trips_hash_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWJUHseDU7CF"
   },
   "outputs": [],
   "source": [
    "# To count the number of dates for each service_id\n",
    "service_id_df = calendar_dates.groupby(['service_id'])[['service_id']].count().rename(columns={'service_id':'count_service_id'}).reset_index()\n",
    "service_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO8UX0dOU9tT"
   },
   "outputs": [],
   "source": [
    "#regroup the days per service id in a set\n",
    "service_id_dates = calendar_dates.groupby('service_id')['date'].apply(lambda group_series: set(group_series.tolist())).reset_index()\n",
    "service_id_dates.rename(columns={'date':'dates'}, inplace=True)\n",
    "service_id_dates = service_id_dates.merge(service_id_df, on='service_id', how='left')\n",
    "service_id_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzEgQhPxU_Br"
   },
   "outputs": [],
   "source": [
    "# To put the different trip_ids in a list after joining on (route_id, route_long_name, hash and service_id)\n",
    "route_hash_freq = trips_hash_stop_sequence.groupby(['route_id','route_long_name','hash', 'hash_inverse', 'service_id'])['trip_id'].apply(lambda group_series: group_series.tolist()).reset_index()\n",
    "route_hash_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqjGMDceVG_0"
   },
   "outputs": [],
   "source": [
    "# To add the sequence of stops to the route_hash_freq dataset\n",
    "route_hash_freq = pd.merge(route_hash_freq, trips_hash_stop_sequence[['route_id','hash', 'hash_inverse', 'service_id','stop_sequence']], on=['route_id', 'hash', 'hash_inverse', 'service_id'], how='left')\n",
    "route_hash_freq = route_hash_freq.drop_duplicates( subset = ['route_id', 'hash', 'service_id'], keep = 'first')\n",
    "\n",
    "route_hash_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd8tQ1ULVNTp"
   },
   "outputs": [],
   "source": [
    "# To calculate the number of trip ids in the list of trip_ids and to add it as a new column\n",
    "number_trip_ids = []\n",
    "for list_trip_ids in route_hash_freq['trip_id']:\n",
    "    count = len(list_trip_ids)\n",
    "    number_trip_ids.append(count)\n",
    "route_hash_freq['number_trip_ids'] = number_trip_ids\n",
    "\n",
    "route_hash_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPVkKKp2VOHz"
   },
   "outputs": [],
   "source": [
    "# To merge the route_hash_freq df with the service_id_dates to get the sets of corresponding dates\n",
    "route_hash_service_freq = pd.merge(route_hash_freq, service_id_dates, on='service_id', how='left')\n",
    "route_hash_service_freq_copy = route_hash_service_freq.copy()\n",
    "route_hash_service_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tc3T3QQbVQRb"
   },
   "outputs": [],
   "source": [
    "'''Groups the service_id together for each route_id and hash combination'''\n",
    "for index, combi_route_id_hash in route_hash_service_freq_copy.groupby(['route_id','hash'], as_index = False)['service_id'].last().iterrows():\n",
    "    set_service_id = set(route_hash_service_freq.loc[(route_hash_service_freq_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_copy['hash'] == combi_route_id_hash['hash'])]['service_id'])\n",
    "    route_hash_service_freq_copy.loc[(route_hash_service_freq_copy['route_id'] == combi_route_id_hash['route_id']) & (route_hash_service_freq_copy['hash'] == combi_route_id_hash['hash']),['service_id']] = set_service_id\n",
    "route_hash_service_freq_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPI7sKAUVeAn"
   },
   "outputs": [],
   "source": [
    "'''Get the distinct stop sequences for creating the possible roads combinations later on'''\n",
    "distinct_stop_sequences = route_hash_service_freq_copy.drop_duplicates(subset = [\"route_id\", 'hash'])[['route_id','hash','stop_sequence', 'service_id']]\n",
    "distinct_stop_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYLL4jeLV3qC"
   },
   "source": [
    "##Functions for the route creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m5eKSeHV6ul"
   },
   "outputs": [],
   "source": [
    "'''Some functions to better factorise the functions in the coming cells'''\n",
    "\n",
    "def select_stop_sequences(stop_sequences_df, route_id):\n",
    "    '''retruns the stop sequences with the selected route_id'''\n",
    "    return stop_sequences_df[stop_sequences_df['route_id'] == route_id].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J338sBEQWBWA"
   },
   "outputs": [],
   "source": [
    "'''Finds the routes that can be either extended from behind or from after and those which are complete sequences'''\n",
    "\n",
    "def get_extention_indexes(stop_sequences_df):\n",
    "    '''returns the tree indexes: index_of_extendable, index_of_begin_sequences, index_of_complete_sequences'''\n",
    "    #intiate the dictionnaries, that will be used to retrieve different rows later on\n",
    "    index_of_extendable = {}\n",
    "    index_of_begin_sequences = {}\n",
    "    index_of_complete_sequences = {}\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        #select the route with the route_id selected by the loop iteration\n",
    "        route_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "        for index_trip, trip in route_sequences_route_id.iterrows():\n",
    "            #checks the extentions possible for the trip that can follow after its last stop\n",
    "            possible_extentions_after = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(trip['stop_sequence']))))].copy()\n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_after = possible_extentions_after[possible_extentions_after['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #checks the extentions possible for the trip that can follow before its first stop\n",
    "            possible_extentions_behind = route_sequences_route_id[route_sequences_route_id['stop_sequence'].apply(lambda x: any(item for item in [trip['stop_sequence'][0]] if (item == x[-1]) and not(set(x[:-1]) & set(trip['stop_sequence']))))].copy()        \n",
    "            #checks that those extentions have a common service_id as the trip\n",
    "            possible_extentions_behind = possible_extentions_behind[possible_extentions_behind['service_id'].apply(lambda x: any(item for item in trip['service_id'] if item in x))].copy()\n",
    "            #put all the sequences that can be extended either from the beginning either from the end together\n",
    "            possible_extentions = possible_extentions_after.append(possible_extentions_behind, ignore_index = True)\n",
    "            if not possible_extentions.empty:\n",
    "                if route_id not in index_of_extendable:\n",
    "                    index_of_extendable[route_id] = []\n",
    "                index_of_extendable[route_id].append(index_trip)\n",
    "                if possible_extentions_behind.empty:\n",
    "                    if route_id not in index_of_begin_sequences:\n",
    "                        index_of_begin_sequences[route_id] = []\n",
    "                    index_of_begin_sequences[route_id].append(index_trip)\n",
    "            elif possible_extentions.empty:\n",
    "                if route_id not in index_of_complete_sequences:\n",
    "                    index_of_complete_sequences[route_id] = []\n",
    "                index_of_complete_sequences[route_id].append(index_trip)\n",
    "                \n",
    "    return index_of_extendable, index_of_begin_sequences, index_of_complete_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgJ_I8_tWJny"
   },
   "outputs": [],
   "source": [
    "'''Creates all the sequences of routes possible to reconstruct the real route'''\n",
    "\n",
    "def possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences):\n",
    "    '''returns the first part of the route_creation, two others need to be added'''\n",
    "    import copy\n",
    "    #create an empty df for the process of route creation\n",
    "    route_creation  = pd.DataFrame()\n",
    "    for route_id in index_of_extendable:\n",
    "        #checks if some parts are begin sequences, if not, then we can't build routes with multiple sequences\n",
    "        if route_id in index_of_begin_sequences:\n",
    "            #create a copy of the df with only the route considered in the loop iteration\n",
    "            routes_with_route_id = select_stop_sequences(stop_sequences_df, route_id)\n",
    "            #create a df where only the routes that have an end stop as their first element of the sequence\n",
    "            route_creation_route_id = routes_with_route_id.loc[index_of_begin_sequences[route_id]][['route_id', 'hash', 'stop_sequence', 'service_id']]\n",
    "            #create a df with the exentable sequences for that route_id\n",
    "            route_creation_extensions_route_id = routes_with_route_id.loc[index_of_extendable[route_id]][['route_id', 'hash', 'stop_sequence','service_id']]    \n",
    "            #make the hash column as a column of lists\n",
    "            route_creation_route_id['hash'] = route_creation_route_id['hash'].apply(lambda x: [x])\n",
    "            route_creation_route_id = route_creation_route_id.reset_index(drop=True)\n",
    "            #to stop the while loop when all the routes are complete in the df for the route_id of the loop iteration\n",
    "            complete_routes = 0\n",
    "            while complete_routes < len(route_creation_route_id.index):\n",
    "                #use a deepcopy to not impact the iterrows of the main loop\n",
    "                route_creation_deep_copy = copy.deepcopy(route_creation_route_id)\n",
    "                for index_original, route_part in route_creation_deep_copy.iterrows():\n",
    "                    #create a dataframe of the possible extentions for each route_part\n",
    "                    #select an extention only if the extention is the next part of the route and also that no other station are repeated in the sequence if this extention is added(otherwise it might cause an infinite loop)\n",
    "                    possible_extentions = route_creation_extensions_route_id[route_creation_extensions_route_id['stop_sequence'].apply(lambda x: any(item for item in [route_part['stop_sequence'][-1]] if (item == x[0]) and not(set(x[1:]) & set(route_part['stop_sequence']))))].copy()\n",
    "                    #take only those extentions that have a common service_id with the route_part\n",
    "                    possible_extentions = possible_extentions[possible_extentions['service_id'].apply(lambda x: any(item for item in route_part['service_id'] if item in x))].copy()                \n",
    "                    #checks whether any extention fullfilling the criterias has been found\n",
    "                    if not (possible_extentions.empty):\n",
    "                        #if so, extend it with every single possibilities\n",
    "                        for index_extention, possible_extention in possible_extentions.iterrows():\n",
    "                            #must create a deepcopy, otherwise the orignal hash list will change as well (mutable)\n",
    "                            updated_hash = copy.deepcopy(route_part['hash'])\n",
    "                            updated_hash.append(possible_extention['hash'])\n",
    "                            updated_route_sequence = route_part['stop_sequence'] + possible_extention['stop_sequence'][1:]\n",
    "                            common_service_id = possible_extention['service_id'] & route_part['service_id']\n",
    "                            route_creation_route_id.loc[max(route_creation_route_id.index)+1] = [route_id, updated_hash, updated_route_sequence, common_service_id]\n",
    "                        #then delete the route with the index (see loop here above)\n",
    "                        route_creation_route_id = route_creation_route_id.drop(index = index_original)            \n",
    "                    #the route can't be extended anymore\n",
    "                    else:\n",
    "                        complete_routes += 1\n",
    "            #adds all the possible routes created with the trips of the route_id of the main loop\n",
    "            route_creation = route_creation.append(route_creation_route_id, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHsAnUyRWLmk"
   },
   "outputs": [],
   "source": [
    "'''Adds the full sequences to the route_creation dataframe'''\n",
    "\n",
    "def add_full_sequences(stop_sequences_df, route_creation, index_of_complete_sequences):\n",
    "    '''returns the second part of the route_creation, one other needs to be added'''\n",
    "    for route_id in index_of_complete_sequences:\n",
    "        #findes all the complete sequences for that route_id\n",
    "        copy_complete_sequences_df = stop_sequences_df.loc[index_of_complete_sequences[route_id]][['route_id','hash','stop_sequence', 'service_id']].copy()\n",
    "        copy_complete_sequences_df['hash'] = copy_complete_sequences_df['hash'].apply(lambda x: [x])\n",
    "        #adds each of them in the route_creation dataframe\n",
    "        for index_complete_sequence, complete_sequence in copy_complete_sequences_df.iterrows():\n",
    "            route_creation = route_creation.append(complete_sequence, ignore_index = True)\n",
    "    route_creation = route_creation.sort_values(by=['route_id'], ignore_index = True)\n",
    "    return route_creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSlsV8ExWNrd"
   },
   "outputs": [],
   "source": [
    "'''Adds the sequences that were not yet added in the route_creation dataframe'''\n",
    "\n",
    "def add_unused_sequences(stop_sequences_df, route_creation):\n",
    "    '''returns the third part of the route_creation'''\n",
    "    for route_id in stop_sequences_df['route_id'].unique():\n",
    "        if route_id in route_creation['route_id'].unique():\n",
    "            #get a set of the hashes that were employed to create the routes for that route_id\n",
    "            used_sequences_hash = set(route_creation[route_creation['route_id'] == route_id].apply(lambda x: pd.Series(x['hash']),axis=1).stack().reset_index(level=1, drop=True))\n",
    "            #get a tuple of all the route sequences for that route_id\n",
    "            used_sequences = tuple(route_creation[route_creation['route_id'] == route_id]['stop_sequence'])\n",
    "            copy_sequences_route_id = select_stop_sequences(stop_sequences_df, route_id)[['route_id','hash','stop_sequence', 'service_id']]\n",
    "            copy_sequences_route_id['hash'] = copy_sequences_route_id['hash'].apply(lambda x: [x]) \n",
    "            #adds the hashes that were not employed in any route creations for that route_id\n",
    "            for index_trip, trip in copy_sequences_route_id.iterrows():\n",
    "                #first element of the list because there is always only one element\n",
    "                if trip['hash'][0] not in used_sequences_hash:\n",
    "                    #checks that the sequence is not a sublist of any existing sequences\n",
    "                    is_subsequence = False\n",
    "                    for sequence in used_sequences:\n",
    "                        if set(trip['stop_sequence']).issubset(sequence):\n",
    "                            is_subsequence = True\n",
    "                    if not is_subsequence:\n",
    "                        route_creation = route_creation.append(trip, ignore_index = True)\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrH1dojZWPzC"
   },
   "outputs": [],
   "source": [
    "'''Calculates the frequency of the constructed routes just made in the route_creation dataframe'''\n",
    "    \n",
    "def calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation):\n",
    "    '''calculates the frequencies of route_construction_third'''\n",
    "    #put the default value of the frequency to 0\n",
    "    route_creation['frequency'] = 0\n",
    "    for index_sequence, sequence in route_creation[['route_id','hash','service_id']].iterrows():\n",
    "        #initialize the varibles\n",
    "        sequence_frequency = 0\n",
    "        set_common_service_id = sequence['service_id']\n",
    "        if set_common_service_id:\n",
    "            #select the number_of_trips_per_hash only for the considered route_id\n",
    "            number_of_trips_per_hash_route_id = number_of_trips_per_hash[number_of_trips_per_hash['route_id'] == sequence['route_id']]\n",
    "            #only select the trips with the hash value contained in the sequence and with the same route_id\n",
    "            containing_hash = number_of_trips_per_hash_route_id[number_of_trips_per_hash_route_id['hash'].apply(lambda x: any(item for item in sequence['hash'] if x == item))]\n",
    "            #loop over each service_id that were common during the trip\n",
    "            for service_id in set_common_service_id:\n",
    "                service_id_number_days = service_id_count_dates[service_id_count_dates['service_id'] == service_id].iloc[0]['count_service_id']\n",
    "                #adds the minimum number of trips per day multiplied by the number of days in the service_id\n",
    "                sequence_frequency += containing_hash[containing_hash['service_id'] == service_id]['number_trip_ids'].min() * service_id_number_days\n",
    "            #adds the frequency in of the new route sequence\n",
    "            route_creation.loc[index_sequence, 'frequency'] = sequence_frequency\n",
    "    return route_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA7ZKz_IWSHC"
   },
   "outputs": [],
   "source": [
    "def calculate_hash_route_creation(route_creation): \n",
    "    '''calculates the hash and the hash inverse of the route_creation'''\n",
    "    #copy the route_creation dataFrame\n",
    "    route_creation_hash = route_creation.copy()\n",
    "    #create a column called hash and hash_invese that contains NaN values\n",
    "    route_creation_hash['hash'] = np.nan\n",
    "    route_creation_hash['hash_inverse'] = np.nan\n",
    "    #calculate the hash and the hash inverse using the lists in stop_sequence\n",
    "    for index, route_sequence in route_creation_hash.iterrows():\n",
    "        route_creation_hash.loc[index, 'hash'] = hash(tuple(route_sequence['stop_sequence']))\n",
    "        route_creation_hash.loc[index, 'hash_inverse'] = hash(tuple(list(route_sequence['stop_sequence'])[::-1]))\n",
    "    return route_creation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_do0tL_WT4h"
   },
   "outputs": [],
   "source": [
    "'''Regroup the routes that are the same (even though they are in the opposite direction)'''\n",
    "\n",
    "def regroup_same_stop_sequences(route_creation_hash):\n",
    "    '''regroups the stop_sequences that are the same'''\n",
    "    \n",
    "    route_creation_max_hash = route_creation_hash.copy()\n",
    "    route_creation_max_hash['max_hash'] = route_creation_max_hash[['hash', 'hash_inverse']].max(axis=1)\n",
    "    #create a df that sums the frequence of the trips going from opposite directions\n",
    "    route_creation_max_hash_freq = route_creation_max_hash.groupby(['route_id','max_hash'], as_index = False)[['frequency']].sum()\n",
    "    #renames the max_hash column into hash so it the dataframe can be merged with route_hash_without_freq\n",
    "    route_creation_max_hash_freq = route_creation_max_hash_freq.rename(columns = {'max_hash':'hash'})\n",
    "    #drops the column freq_sequence_route because the one that is of interest is in route_creation_max_hash_freq\n",
    "    route_hash_without_freq = route_creation_hash.copy().drop(['frequency'], axis = 1)\n",
    "    route_hash_without_freq = route_hash_without_freq.drop_duplicates(subset=['route_id', 'hash'])\n",
    "    route_hash_freq_combined_first_merge = pd.merge(route_creation_max_hash_freq, route_hash_without_freq, on=['route_id', 'hash'], how='left')\n",
    "    route_hash_freq_combined_first_merge = route_hash_freq_combined_first_merge.drop(['hash_inverse'], axis = 1)\n",
    "    #selects the part of the dataset that doesn't have NaN (because for the NaN, their hash_value that was max was the one in hash_inverse and it didn't exist in the other df), so we can concatenate it with the part that had NaN later\n",
    "    route_hash_freq_first_part = route_hash_freq_combined_first_merge[pd.notnull(route_hash_freq_combined_first_merge['stop_sequence'])]\n",
    "    #selects one part the part of the dataset that does have NaN, so we can concatenate it with the part that has no NaN later on.\n",
    "    #but first, we will need to fill those NaN values (done in the code lines behind this one)\n",
    "    route_hash_freq_second_part = route_hash_freq_combined_first_merge[pd.isnull(route_hash_freq_combined_first_merge['stop_sequence'])][['route_id', 'hash', 'frequency']]\n",
    "    #renames the hash column into hash_inverse so it the dataframe can be merged with route_hash_without_freq (because it didn't work with 'hash' on the first merge)\n",
    "    route_hash_freq_second_part = route_hash_freq_second_part.rename(columns = {'hash':'hash_inverse'})\n",
    "    route_hash_freq_second_part = pd.merge(route_hash_freq_second_part, route_hash_without_freq, on=['route_id', 'hash_inverse'], how='left')\n",
    "    #the hash that is of interest in the final df will be hash and not hash_inverse\n",
    "    route_hash_freq_second_part  = route_hash_freq_second_part.drop(['hash_inverse'], axis = 1)\n",
    "    route_hash_freq_combined_not_sorted = pd.concat([route_hash_freq_first_part, route_hash_freq_second_part])\n",
    "    route_hash_freq_combined = route_hash_freq_combined_not_sorted.sort_values(by = ['route_id'])\n",
    "    route_hash_freq_combined = route_hash_freq_combined.reset_index(drop = True)\n",
    "    return route_hash_freq_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDwTS3rYWWJ8"
   },
   "outputs": [],
   "source": [
    "'''Deletes the routes that do not represent 10% of the total route frequency and creates new route, if some of them are different'''\n",
    "\n",
    "def apply_treshold_route_creation(route_hash_freq_combined): \n",
    "    #calculates the total frequency per route_id\n",
    "    frequency_each_route = route_hash_freq_combined.groupby(['route_id'], as_index = False)['frequency'].sum()\n",
    "    frequency_treshold = frequency_each_route.copy()\n",
    "    #calculates the treshold (here 10%)\n",
    "    frequency_treshold['frequency'] = frequency_treshold['frequency']/10\n",
    "    frequency_treshold.rename(columns = {'frequency':'frequency_treshold'}, inplace = True)\n",
    "    route_hash_freq_treshold = route_hash_freq_combined.merge(frequency_treshold, on='route_id', how = 'left')\n",
    "    #find the sequences that are not more than 10% of the route frequency and delete them\n",
    "    index_names = route_hash_freq_treshold[route_hash_freq_treshold['frequency'] < route_hash_freq_treshold['frequency_treshold']].index\n",
    "    route_hash_freq_treshold.drop(index_names, inplace = True)\n",
    "    #selects the sequences that are not the most frequent per route_id\n",
    "    sequences_max_freq = route_hash_freq_treshold.groupby(['route_id'],as_index = False)['frequency'].max()\n",
    "    sequences_max_freq.rename(columns = {'frequency':'max_frequency'}, inplace = True)\n",
    "    sequences_max_freq_merged = route_hash_freq_treshold.merge(sequences_max_freq, on='route_id', how='left')\n",
    "    sequences_non_max_freq_index = sequences_max_freq_merged[sequences_max_freq_merged['frequency'] != sequences_max_freq_merged['max_frequency']].index\n",
    "    #those selected sequences get a new route_id that starts from routes['route_id'].max() + 1 and increments by one for each new route\n",
    "    route_id_creation =  route_hash_freq_combined['route_id'].max() + 1\n",
    "    new_route_id_column = list(range(route_id_creation, route_id_creation + len(sequences_non_max_freq_index)))    \n",
    "    sequences_max_freq_merged.loc[sequences_non_max_freq_index, 'route_id'] = new_route_id_column\n",
    "    sequences_max_freq_merged = sequences_max_freq_merged.sort_values(by=['route_id'],ignore_index=True)\n",
    "    #keep only the column route_id and stop_sequence\n",
    "    final_routes = sequences_max_freq_merged.drop(columns=['hash', 'frequency', 'frequency_treshold', 'max_frequency', 'service_id'])\n",
    "    return final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws2UdF5lWZrI"
   },
   "outputs": [],
   "source": [
    "'''Makes a set that can be used for building the edges of the graph using Networkx package'''\n",
    "\n",
    "def create_df_for_Networkx(final_routes):\n",
    "    '''return df_for_edges a df that can be used to build a Networkx L-space graph'''\n",
    "    #takes the list stop sequence and make it a new column for each stop\n",
    "    stop_sequence_values = final_routes.apply(lambda x: pd.Series(x['stop_sequence']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    stop_sequence_values.name = 'stop_sequence'\n",
    "    final_routes_stops = final_routes.drop('stop_sequence', axis=1).join(stop_sequence_values)\n",
    "    final_routes_stops = final_routes_stops.reset_index(drop=True)\n",
    "    #Creates a shifted instance of the df to use it for the final result\n",
    "    final_routes_stops_shifted = final_routes_stops.shift()\n",
    "    #Check if which of the rows are followed by a row with the same trip_id\n",
    "    final_routes_stops_shifted['match'] = final_routes_stops_shifted['route_id'].eq(final_routes_stops['route_id'])\n",
    "    #Drop the rows for which this condition is not satisfied\n",
    "    final_routes_stops_shifted.drop(final_routes_stops_shifted[final_routes_stops_shifted['match'] == False].index, inplace = True)\n",
    "    final_routes_stops_shifted.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_1\",\n",
    "      \"stop_name\": \"stop_name_1\"}, inplace=True)\n",
    "    #joins the df with its shifted version sothat each sequence of two stations is represented in the table as a row\n",
    "    df_for_edges = final_routes_stops_shifted.join(final_routes_stops[['stop_sequence']], lsuffix='_caller', rsuffix='_other', how='left')\n",
    "    df_for_edges.rename(columns=\n",
    "      {\"stop_sequence\": \"stop_name_2\",\n",
    "      \"stop_name\": \"stop_name_2\"}, inplace=True)\n",
    "\n",
    "    df_for_edges['route_id'] = df_for_edges['route_id'].astype(np.int64)\n",
    "    df_for_edges = df_for_edges.drop_duplicates()\n",
    "    df_for_edges = df_for_edges[['route_id','stop_name_1', 'stop_name_2']]\n",
    "    df_for_edges = df_for_edges.reset_index(drop=True)\n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SLrbtwwWagw"
   },
   "outputs": [],
   "source": [
    "def full_route_creation(stop_sequences_df, number_of_trips_per_hash, service_id_count_dates):\n",
    "    '''return a df that can be used to make a Networkx L-space (with treshold applied of 10%)'''\n",
    "    index_of_extendable, index_of_begin_sequences, index_of_complete_sequences = get_extention_indexes(stop_sequences_df)\n",
    "    route_creation_first = possible_sequences_construction(stop_sequences_df, index_of_extendable, index_of_begin_sequences, index_of_complete_sequences)\n",
    "    route_creation_second = add_full_sequences(stop_sequences_df, route_creation_first, index_of_complete_sequences)\n",
    "    route_creation_third = add_unused_sequences(stop_sequences_df, route_creation_second)\n",
    "    route_creation_frequency_single = calculate_frequenty_new_sequences(number_of_trips_per_hash, service_id_count_dates, route_creation_third)\n",
    "    route_creation_hash = calculate_hash_route_creation(route_creation_frequency_single)\n",
    "    route_hash_freq_combined = regroup_same_stop_sequences(route_creation_hash)\n",
    "    final_routes = apply_treshold_route_creation(route_hash_freq_combined)\n",
    "    df_for_edges = create_df_for_Networkx(final_routes)\n",
    "    \n",
    "    return df_for_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpYNiwp-WeYb"
   },
   "source": [
    "##Apply the route creation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Gn8fslrWhcR"
   },
   "outputs": [],
   "source": [
    "df_for_edges_Belgium = full_route_creation(distinct_stop_sequences, route_hash_service_freq.copy(), service_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP15z-DKWmYt"
   },
   "outputs": [],
   "source": [
    "df_for_edges_Belgium.to_csv(r'/Users/pol/Desktop/CSV_export/df_for_edges_Belgium.csv', index = False, header=True, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cleaning_GTFS_Belgium.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
