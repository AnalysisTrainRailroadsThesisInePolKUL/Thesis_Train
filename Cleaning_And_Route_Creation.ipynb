{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import the required packages.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Display all output results of a Jupyter cell.'''\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ensure that the output results of extensive output results are not truncated.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ensure that the output results of extensive output results are not truncated.'''\n",
    "#pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Change the width of the Notebook to see the output on the screen'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Change the width of the Notebook to see the output on the screen'''\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Register the GitHub link or the file relative location'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Register the GitHub link or the file relative location'''\n",
    "#the Github link\n",
    "#repository_loc = \"https://raw.githubusercontent.com/polkuleuven/Thesis_Train/main\"\n",
    "#the local link\n",
    "repository_loc = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get the other folder locations'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the other folder locations'''\n",
    "belgian_GTFS_loc = repository_loc + '/gtfs_train_Belgium_1503/'\n",
    "dutch_GTFS_loc = repository_loc + '/gtfs_train_Netherlands_1503/'\n",
    "swiss_GTFS_loc = repository_loc + '/gtfs_train_Switzerland_1503/'\n",
    "\n",
    "stops_series_loc = repository_loc + '/country_stops_series/'\n",
    "stops_cleaned_loc = repository_loc + '/stops_cleaned/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import all the DataFrames that are common for the three train networks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import all the DataFrames that are common for the three train networks'''\n",
    "\n",
    "def common_imports(datalink):\n",
    "    #To import the agency dataset that contains limited information about the railway agency.\n",
    "    agency = pd.read_csv(datalink + \"agency.txt\", sep=\",\")\n",
    "    #To import the calendar_dates dataset that gives for each service_id all the exact dates when that service_id is valid.\n",
    "    calendar_dates = pd.read_csv(datalink + \"calendar_dates.txt\", sep=\",\")\n",
    "    #To import the routes dataset that provides the id, the name and the type of vehicle used for all railway routes.\n",
    "    routes = pd.read_csv(datalink + \"routes.txt\", sep=\",\")\n",
    "    #To import the stops dataset that contains information about the ids, the names and the geographical coordinates of the railway stations.\n",
    "    stops = pd.read_csv(datalink + \"stops.txt\", sep=\",\")\n",
    "    #To import the transfers dataset that gives the minimum transfer time to switch routes at each railway station.\n",
    "    transfers = pd.read_csv(datalink + \"transfers.txt\", sep=\",\")\n",
    "    #To import the trips dataset that gives for all routes an overview of the trips and the headsigns of these trips belonging to the railway route.\n",
    "    #The service_id is an indication of all the dates this trip is valid (consultable in the calendar_dates dataset).\n",
    "    trips = pd.read_csv(datalink + \"trips.txt\", sep=\",\")\n",
    "    return agency, calendar_dates, routes, stops, transfers, trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Belgium, calendar_dates_Belgium, routes_Belgium, stops_Belgium, transfers_Belgium, trips_Belgium = common_imports(belgian_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the translations dataset that provides the French-, Dutch-, German- and English-language translations of the Belgian railway stations.\n",
    "translations_Belgium = pd.read_csv(belgian_GTFS_loc + \"translations.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Belgium = pd.read_csv(belgian_GTFS_loc + \"calendar.txt\", sep=\",\")\n",
    "#To import the stop_time_overrides dataset \n",
    "stop_time_overrides_Belgium = pd.read_csv(belgian_GTFS_loc + \"stop_time_overrides.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Netherlands, calendar_dates_Netherlands, routes_Netherlands, stops_Netherlands, transfers_not_cleaned_Netherlands, trips_Netherlands = common_imports(dutch_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Dutch NS railway feed.\n",
    "feed_info_Netherlands = pd.read_csv(dutch_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_range = [*range(2, 19)]\n",
    "stop_times_Netherlands = pd.read_csv(dutch_GTFS_loc + \"stop_times-1.csv\", sep=\",\")\n",
    "for index in stop_times_range:\n",
    "    stop_times_Netherlands = pd.concat([stop_times_Netherlands, pd.read_csv(dutch_GTFS_loc + \"stop_times-\" + str(index)+ \".csv\", sep=\",\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply common_import()'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply common_import()'''\n",
    "agency_Switzerland, calendar_dates_Switzerland, routes_Switzerland, stops_Switzerland, transfers_not_cleaned_Switzerland, trips_Switzerland = common_imports(swiss_GTFS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Import other DataFrames'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "'''Import other DataFrames'''\n",
    "#To import the feed_info dataset that contains limited information about the Swiss SBB railway feed.\n",
    "feed_info_Switzerland = pd.read_csv(swiss_GTFS_loc + \"feed_info.txt\", sep=\",\")\n",
    "#To import the stop_times dataset that gives for all trips an overview of the ids of the stations served and the sequence in which these stations are served. \n",
    "#In addition, for all the trips the arrival and departure times at the stations served are given.\n",
    "stop_times_Switzerland = pd.read_csv(swiss_GTFS_loc + \"stop_times.txt\", sep=\",\")\n",
    "#To import the calendar dataset that gives the first and last date of all data observations.\n",
    "calendar_Switzerland = pd.read_csv(swiss_GTFS_loc + \"calendar.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the railway data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the calendar_dates DataFrame'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the calendar_dates DataFrame'''\n",
    "\n",
    "def clean_calendar_dates(calendar_dates):\n",
    "    #To filter the dates from the selected begin to the end date\n",
    "    begin_date = 20210314\n",
    "    end_date = 20210713\n",
    "    calendar_dates_cleaned = calendar_dates.copy()\n",
    "    calendar_dates_cleaned = calendar_dates_cleaned.drop(calendar_dates_cleaned[(calendar_dates_cleaned['date'] > end_date) | (calendar_dates_cleaned['date'] < begin_date)].index)\n",
    "    return calendar_dates_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Add the country to the stops DataFrame and returns the country filtered DataFrame of stops and the serie of those stops'''\n",
    "\n",
    "def country_information(stops, country_name, stops_cleaned_loc, stops_series_loc):\n",
    "    #To initialize the Nominatim API to get the location from the input string \n",
    "    geolocator = Nominatim(user_agent=\"application\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=0.2)\n",
    "\n",
    "    #To get the location with the geolocator.reverse() function and to extract the country from the location instance\n",
    "    country_list = []\n",
    "    for index, row in stops.iterrows():\n",
    "        latitude = row['stop_lat']\n",
    "        longitude = row['stop_lon']\n",
    "        # To assign the latitude and longitude into a geolocator.reverse() method\n",
    "        location = reverse((latitude, longitude), language='en', exactly_one=True)\n",
    "        # To get the country from the given list and parsed into a dictionary with raw function()\n",
    "        address = location.raw['address']\n",
    "        country = address.get('country', '')\n",
    "        country_list.append(country)\n",
    "\n",
    "    #To add the values of country_list as a new attribute country \n",
    "    stops.loc[:,'country'] = country_list\n",
    "\n",
    "    #To calculate the total number of Belgian stations in the stops dataset\n",
    "    country_stops = stops[stops['country'] == country_name]\n",
    "    country_stops_series = stops.loc[stops['country'] == country_name, 'stop_name']\n",
    "    \n",
    "    stops.to_csv(f'{stops_cleaned_loc}stops_cleaned_{country_name}.csv')\n",
    "    country_stops_series.to_csv(f'{stops_series_loc}stops_{country_name}_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remove the accents from a string'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Remove the accents from a string'''\n",
    "\n",
    "def remove_accents(text):\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acutal cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Belgium df'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Belgium df'''\n",
    "allowed_route_type = {'IC', 'L', 'P', 'ICT', 'IZY'}\n",
    "routes_cleaned_Belgium = routes_Belgium[(routes_Belgium['route_short_name'].isin(allowed_route_type)) | (routes_Belgium['route_short_name'].str.startswith('S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Belgium = clean_calendar_dates(calendar_dates_Belgium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops_Belgium df.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops_Belgium df.''' \n",
    "#To eliminate the stop_ids in the stops dataset that contain an underscore or that start with a character 'S'. \n",
    "stops_cleaned_Belgium = stops_Belgium[(~stops_Belgium['stop_id'].str.contains('_')) & (~stops_Belgium['stop_id'].str.contains('S'))]\n",
    "\n",
    "#To modify the object datatype of the stop_id column to the NumPy int64 datatype\n",
    "stops_cleaned_Belgium.loc[:,'stop_id'] = stops_cleaned_Belgium.loc[:,'stop_id'].astype(np.int64)\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Belgium.loc[:,'stop_name'] = stops_cleaned_Belgium.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Belgium'\n",
    "#country_information(stops_cleaned_Belgium, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Belgium = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Netherlands DataFrame'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>agency_id</th>\n",
       "      <th>route_short_name</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>route_desc</th>\n",
       "      <th>route_type</th>\n",
       "      <th>route_color</th>\n",
       "      <th>route_text_color</th>\n",
       "      <th>route_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>145</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>17803</td>\n",
       "      <td>IFF:NS</td>\n",
       "      <td>Sprinter</td>\n",
       "      <td>Nachtnettrein Utrecht Centraal &lt;-&gt; Rotterdam C...</td>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_id agency_id route_short_name  \\\n",
       "count       145       145              145   \n",
       "unique      145        11               15   \n",
       "top       17803    IFF:NS         Sprinter   \n",
       "freq          1        87               47   \n",
       "\n",
       "                                          route_long_name route_desc  \\\n",
       "count                                                 145        145   \n",
       "unique                                                144          1   \n",
       "top     Nachtnettrein Utrecht Centraal <-> Rotterdam C...        nan   \n",
       "freq                                                    2        145   \n",
       "\n",
       "       route_type route_color route_text_color route_url  \n",
       "count         145         145              145       145  \n",
       "unique          1           1                1         1  \n",
       "top             2         nan              nan       nan  \n",
       "freq          145         145              145       145  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Netherlands DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Netherlands = routes_Netherlands[routes_Netherlands['route_type'] == 2]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands.astype(str)\n",
    "routes_cleaned_Netherlands.describe(include=['object'])\n",
    "\n",
    "#To change the route_id object datatype to a NumPy int64 datatype\n",
    "routes_cleaned_Netherlands.loc[:,'route_id'] = routes_cleaned_Netherlands.loc[:,'route_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Netherlands = clean_calendar_dates(calendar_dates_Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops DataFrame'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pol/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "'''Clean the stops DataFrame'''\n",
    "#To take from the stops_initial_Netherlands df all stop_ids that contain a 'stoparea:' to get the correct stop coordinates\n",
    "stops_cleaned_Netherlands = stops_Netherlands[stops_Netherlands['stop_id'].str.contains('stoparea:')]\n",
    "\n",
    "#To remove the accents from the accented characters and to convert the remaining characters to uppercase characters\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stops_cleaned_Netherlands.loc[:,'stop_name'] = stops_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Netherlands'\n",
    "#country_information(stops_cleaned_Netherlands, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Netherlands = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stop_times df'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stop_times df'''\n",
    "stop_times_cleaned_Netherlands = stop_times_Netherlands.copy()\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_id'] = stop_times_cleaned_Netherlands.stop_id.apply(str)\n",
    "stop_times_cleaned_Netherlands = pd.merge(stop_times_cleaned_Netherlands, stops_Netherlands[['stop_id', 'stop_name']], on='stop_id')\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].apply(remove_accents)\n",
    "stop_times_cleaned_Netherlands.loc[:,'stop_name'] = stop_times_cleaned_Netherlands.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the routes_Switzerland DataFrame'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the routes_Switzerland DataFrame'''\n",
    "#To keep the train routes\n",
    "routes_cleaned_Switzerland = routes_Switzerland[routes_Switzerland['route_type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply clean_calendar_dates()'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply clean_calendar_dates()'''\n",
    "calendar_dates_cleaned_Switzerland = clean_calendar_dates(calendar_dates_Switzerland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stop_times_Switzerland DataFrame'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stop_times_Switzerland DataFrame'''\n",
    "# To remove the superfluous characters of the stop_id (platform codes)\n",
    "stop_times_cleaned_Switzerland = stop_times_Switzerland.copy()\n",
    "stop_times_cleaned_Switzerland_column = stop_times_cleaned_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland_column\n",
    "\n",
    "# To make the stop_ids numerical \n",
    "stop_times_cleaned_Switzerland.loc[:,'stop_id'] = stop_times_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean the stops_Switzerland DataFrame'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the stops_Switzerland DataFrame'''\n",
    "#To remove the superfluous characters (platform codes)\n",
    "stops_cleaned_Switzerland_column = stops_Switzerland['stop_id'].str.split(':').str[0]\n",
    "stops_cleaned_Switzerland = stops_Switzerland.copy()\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland_column\n",
    "\n",
    "#To make the stop_ids numerical and to remove the duplicate stop_ids\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "stops_cleaned_Switzerland.loc[:,'stop_id'] = stops_cleaned_Switzerland.loc[:,'stop_id'].astype(np.int64)\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland.drop_duplicates()\n",
    "\n",
    "#To remove the accents from the stop_name and to change to uppercase\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "stops_cleaned_Switzerland.loc[:,'stop_name'] = stops_cleaned_Switzerland.loc[:,'stop_name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply country_information() and take the DataFrames from the files'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply country_information() and take the DataFrames from the files'''\n",
    "country_name = 'Switzerland'\n",
    "#country_information(stops_cleaned_Switzerland, country_name, stops_cleaned_loc, stops_series_loc)\n",
    "stops_cleaned_Switzerland = pd.read_csv(f\"{stops_cleaned_loc}stops_cleaned_{country_name}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merge the DataFrames'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Merge the DataFrames'''\n",
    "\n",
    "def merge_df(stop_times, stops, routes, trips, calendar_dates, on_stop):\n",
    "    list_columns = ['stop_name', 'stop_lat', 'stop_lon', 'country']\n",
    "    if on_stop == 'stop_id':\n",
    "        list_columns.append('stop_id')\n",
    "    #To merge the stop_times df with the stops df on stop_id\n",
    "    stop_times_stops = pd.merge(stop_times, stops[list_columns], on= on_stop)\n",
    "\n",
    "    #To merge the trips df with the routes df on route_id\n",
    "    routes_trips = pd.merge(routes[['route_id']], trips, on='route_id')\n",
    "\n",
    "    #To merge the stop_times_stops df with the trips_routes df on trip_id\n",
    "    uncleaned_train_information = pd.merge(routes_trips, stop_times_stops, on='trip_id')\n",
    "\n",
    "    #To take only the service_ids present in both the routes_trips_stop_times_stops df and the calendar_dates df into account\n",
    "    calendar_dates_unique = calendar_dates['service_id'].unique()\n",
    "    train_information = uncleaned_train_information[(uncleaned_train_information['service_id'].isin(calendar_dates_unique))]\n",
    "    \n",
    "    return train_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select all required fields'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Belgium = agency_Belgium[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Belgium = routes_cleaned_Belgium[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Belgium = trips_Belgium[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Belgium = calendar_dates_cleaned_Belgium[['service_id', 'date']]\n",
    "stops_cleaned_Belgium = stops_cleaned_Belgium[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Belgium = stop_times_Belgium[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "train_information_Belgium = merge_df(stop_times_cleaned_Belgium, stops_cleaned_Belgium, routes_cleaned_Belgium, trips_cleaned_Belgium, calendar_dates_cleaned_Belgium, 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To select all required fields'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Netherlands = agency_Netherlands[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Netherlands = routes_cleaned_Netherlands[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Netherlands = trips_Netherlands[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Netherlands = calendar_dates_cleaned_Netherlands[['service_id', 'date']]\n",
    "stops_cleaned_Netherlands = stops_cleaned_Netherlands[['stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Netherlands = stop_times_cleaned_Netherlands[['trip_id', 'stop_name', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "train_information_Netherlands = merge_df(stop_times_cleaned_Netherlands, stops_cleaned_Netherlands, routes_cleaned_Netherlands, trips_cleaned_Netherlands, calendar_dates_cleaned_Netherlands, 'stop_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To select all required fields'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Select all required fields'''\n",
    "agency_cleaned_Switzerland = agency_Switzerland[['agency_id', 'agency_name', 'agency_url', 'agency_timezone']]\n",
    "routes_cleaned_Switzerland = routes_cleaned_Switzerland[['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
    "trips_cleaned_Switzerland = trips_Switzerland[['trip_id', 'route_id', 'service_id', 'trip_headsign']]\n",
    "calendar_dates_cleaned_Switzerland = calendar_dates_cleaned_Switzerland[['service_id', 'date']]\n",
    "stops_cleaned_Switzerland = stops_cleaned_Switzerland[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'country']]\n",
    "stop_times_cleaned_Switzerland = stop_times_cleaned_Switzerland[['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apply merge_df()'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Apply merge_df()'''\n",
    "train_information_Switzerland = merge_df(stop_times_cleaned_Switzerland, stops_cleaned_Switzerland, routes_cleaned_Switzerland, trips_cleaned_Switzerland, calendar_dates_cleaned_Switzerland, 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
